{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "360c2169",
   "metadata": {
    "papermill": {
     "duration": 0.021613,
     "end_time": "2022-07-02T11:31:43.387891",
     "exception": false,
     "start_time": "2022-07-02T11:31:43.366278",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Notes\n",
    "- LGBM Reg v17\n",
    "\n",
    "# To do list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d70f99e",
   "metadata": {
    "_cell_guid": "5ae9011f-40e6-497b-a6be-61b748ce1d2d",
    "_kg_hide-input": true,
    "_uuid": "2c45ee48-0594-488c-8a45-26a2e5510d83",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-07-02T11:31:43.434181Z",
     "iopub.status.busy": "2022-07-02T11:31:43.432870Z",
     "iopub.status.idle": "2022-07-02T11:32:17.505660Z",
     "shell.execute_reply": "2022-07-02T11:32:17.506227Z",
     "shell.execute_reply.started": "2022-07-02T11:08:33.247148Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 34.097808,
     "end_time": "2022-07-02T11:32:17.506586",
     "exception": false,
     "start_time": "2022-07-02T11:31:43.408778",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/ta-0101/ta-0.10.1-py3-none-any.whl\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from ta==0.10.1) (1.20.3)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from ta==0.10.1) (1.3.5)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->ta==0.10.1) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->ta==0.10.1) (2021.3)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->ta==0.10.1) (1.16.0)\r\n",
      "Installing collected packages: ta\r\n",
      "Successfully installed ta-0.10.1\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import os\n",
    "from os.path import isfile, isdir, join\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from IPython.display import display\n",
    "from zipfile import ZipFile\n",
    "import pickle\n",
    "import unicodedata\n",
    "import pytz\n",
    "from joblib import Parallel, delayed\n",
    "import shutil\n",
    "import difflib\n",
    "import random\n",
    "import math\n",
    "from shutil import copyfile\n",
    "import itertools\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import collections\n",
    "from collections import deque\n",
    "import gc\n",
    "import seaborn as sns\n",
    "import scipy.cluster.hierarchy as spc\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import ndcg_score, accuracy_score\n",
    "\n",
    "import lightgbm as lgbm\n",
    "import optuna\n",
    "from optuna import Trial, visualization\n",
    "\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import jpx_tokyo_market_prediction\n",
    "\n",
    "from utility_script import *\n",
    "\n",
    "!pip install ../input/ta-0101/ta-0.10.1-py3-none-any.whl\n",
    "import ta\n",
    "from ta import add_all_ta_features\n",
    "from ta.utils import dropna\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c87fd70c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-02T11:32:17.559951Z",
     "iopub.status.busy": "2022-07-02T11:32:17.559262Z",
     "iopub.status.idle": "2022-07-02T11:32:17.562497Z",
     "shell.execute_reply": "2022-07-02T11:32:17.561972Z",
     "shell.execute_reply.started": "2022-07-02T11:09:05.604419Z"
    },
    "papermill": {
     "duration": 0.032028,
     "end_time": "2022-07-02T11:32:17.562637",
     "exception": false,
     "start_time": "2022-07-02T11:32:17.530609",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# general params\n",
    "FE = 'orig'\n",
    "MODEL = 'clf3'\n",
    "FE_PATH = '../input/jpx-feature-engineering-v15'\n",
    "MODEL_PATH = '../input/jpx-model-lgbm'\n",
    "JPX_PATH = '../input/jpx-tokyo-stock-exchange-prediction'\n",
    "EXPORT = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84a390dd",
   "metadata": {
    "_cell_guid": "c0f0bfa1-3dea-43a3-b2f1-8a2c268d6339",
    "_uuid": "c3cfb2dc-8437-441f-b2c0-88b3d4a77281",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-07-02T11:32:17.614354Z",
     "iopub.status.busy": "2022-07-02T11:32:17.613680Z",
     "iopub.status.idle": "2022-07-02T11:32:17.618219Z",
     "shell.execute_reply": "2022-07-02T11:32:17.617713Z",
     "shell.execute_reply.started": "2022-07-02T11:09:05.611780Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.033572,
     "end_time": "2022-07-02T11:32:17.618401",
     "exception": false,
     "start_time": "2022-07-02T11:32:17.584829",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "FE Parameters\n",
    "'''\n",
    "SEED = 0\n",
    "LAGS = {'1d':1, '3d':3, '1w':5, '1m':20, '3m':20*3, '6m':20*6, '12m':20*12}\n",
    "MAX_DAYS_LAG = max(list(LAGS.values()))\n",
    "WIN_SIZE = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34749778",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-02T11:32:17.670588Z",
     "iopub.status.busy": "2022-07-02T11:32:17.669894Z",
     "iopub.status.idle": "2022-07-02T11:32:17.672523Z",
     "shell.execute_reply": "2022-07-02T11:32:17.672991Z",
     "shell.execute_reply.started": "2022-07-02T11:09:05.624718Z"
    },
    "papermill": {
     "duration": 0.032287,
     "end_time": "2022-07-02T11:32:17.673191",
     "exception": false,
     "start_time": "2022-07-02T11:32:17.640904",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Model Params\n",
    "'''\n",
    "# basic\n",
    "SEED = 0\n",
    "SEEDS = [1,2,3,4,5]\n",
    "\n",
    "# feature composition\n",
    "DROP_MARKET_FEATS = False\n",
    "\n",
    "# PCA\n",
    "RUN_PCA = True\n",
    "PCA_SPLIT = False\n",
    "\n",
    "# target definition\n",
    "RANK_ASCENDING = False # set this to False if model prediction is same direction of Target\n",
    "TARGET_POW = 0\n",
    "\n",
    "# data split\n",
    "N_FOLD = 5\n",
    "\n",
    "# final features\n",
    "SELECTED_FEATS = 'pc35, pc27, pc12, pc8, pc16, pc6, pc4'.split(', ')\n",
    "\n",
    "# optimization\n",
    "CLUSTER_DEMEAN = True\n",
    "CLUST_N_DAY = 60\n",
    "VOL_PENALTY = True\n",
    "VOL_N_DAY = 60\n",
    "VOL_POW = 1.5789473684210527"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb51b7c",
   "metadata": {
    "papermill": {
     "duration": 0.022901,
     "end_time": "2022-07-02T11:32:17.718475",
     "exception": false,
     "start_time": "2022-07-02T11:32:17.695574",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37bffa8f",
   "metadata": {
    "_cell_guid": "6a3c94c5-9152-4477-9b79-dec0b881fd0e",
    "_uuid": "54a20792-c5bc-4ee2-8162-07364e311d9c",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-07-02T11:32:17.783902Z",
     "iopub.status.busy": "2022-07-02T11:32:17.783177Z",
     "iopub.status.idle": "2022-07-02T11:32:17.786053Z",
     "shell.execute_reply": "2022-07-02T11:32:17.786640Z",
     "shell.execute_reply.started": "2022-07-02T11:09:05.642149Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.04599,
     "end_time": "2022-07-02T11:32:17.786811",
     "exception": false,
     "start_time": "2022-07-02T11:32:17.740821",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class JPXData:\n",
    "    def __init__(self, window_size, df_names):\n",
    "        self.size = 0\n",
    "        self.window_size = window_size\n",
    "        self.df_names = df_names\n",
    "        self.num_df = len(df_names)\n",
    "        self.data = {df_name : pd.DataFrame() for df_name in df_names}\n",
    "        self.row_counts = {df_name : [] for df_name in df_names}\n",
    "        self.dates = []\n",
    "        self.first_date, self.last_date = None, None\n",
    "        self.features = []\n",
    "        self.curr_features = None\n",
    "        self.n_day_hist = 0\n",
    "        self.init_folders()\n",
    "        \n",
    "    def init_folders(self):\n",
    "        shutil.rmtree(path='./features', ignore_errors=True)\n",
    "        os.mkdir('./features')\n",
    "        \n",
    "    def append_data(self):\n",
    "        self.features.append(self.curr_features)\n",
    "        self.n_day_hist += 1\n",
    "        \n",
    "    def archive_data(self):\n",
    "        save_pkl(self.features, f'./features/features_{self.n_day_hist}')\n",
    "        self.clear_hist()\n",
    "        \n",
    "    def clear_hist(self):\n",
    "        self.features = []\n",
    "        \n",
    "    def push_forward(self, new_data, append, last):\n",
    "        # assign names to new data assuming the same as df_names\n",
    "        new_data = dict(zip(self.df_names, new_data))\n",
    "        # case when no enough data\n",
    "        if self.size < self.window_size:\n",
    "            for df_name in self.df_names:\n",
    "                self.data[df_name] = pd.concat([self.data[df_name], new_data[df_name]]).reset_index(drop=True)\n",
    "                self.row_counts[df_name] = self.row_counts[df_name] + [new_data[df_name].shape[0]]\n",
    "            self.dates = self.dates + [new_data[self.df_names[0]].Date.iloc[0]] \n",
    "            self.size += 1\n",
    "        # general case (shift by 1 day)\n",
    "        else:\n",
    "            for df_name in self.df_names:\n",
    "                self.data[df_name] = pd.concat([self.data[df_name].iloc[self.row_counts[df_name][0]:], new_data[df_name]]).reset_index(drop=True)\n",
    "                self.row_counts[df_name] = self.row_counts[df_name][1:] + [new_data[df_name].shape[0]]\n",
    "            self.dates = self.dates[1:] + [new_data[self.df_names[0]].Date.iloc[0]]  \n",
    "        # update date range\n",
    "        self.first_date, self.last_date = self.dates[0], self.dates[-1]\n",
    "        # generate features\n",
    "        if self.size == self.window_size:\n",
    "            self.curr_features = get_features(self.data)\n",
    "            if append==True:\n",
    "                self.append_data()\n",
    "                if (self.n_day_hist%20 == 0 and self.n_day_hist > 0) or last==True:\n",
    "                    self.archive_data()\n",
    "        log(f'Pushed to latest date: {self.last_date}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b168d312",
   "metadata": {
    "_cell_guid": "ae8fcff3-07c3-4ef5-ade3-61d6dc34243b",
    "_uuid": "c4fccf97-02d7-4f36-a5fa-53efcfd2edc6",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-07-02T11:32:17.835056Z",
     "iopub.status.busy": "2022-07-02T11:32:17.834365Z",
     "iopub.status.idle": "2022-07-02T11:32:17.850571Z",
     "shell.execute_reply": "2022-07-02T11:32:17.851131Z",
     "shell.execute_reply.started": "2022-07-02T11:09:05.664163Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.042018,
     "end_time": "2022-07-02T11:32:17.851303",
     "exception": false,
     "start_time": "2022-07-02T11:32:17.809285",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def standard_dist(s, lag):\n",
    "    tail_data = s.tail(LAGS[lag])\n",
    "    return (s.iloc[-1] - tail_data.mean()) / tail_data.std()\n",
    "\n",
    "def ma_pctg_ch(s, lag):\n",
    "    return s.iloc[-1] / s.tail(LAGS[lag]).mean() - 1\n",
    "\n",
    "def sharpe(s, lag):\n",
    "    tail_data = s.tail(LAGS[lag])\n",
    "    std = tail_data.std()\n",
    "    if std > 0:\n",
    "        sharpe_ratio = tail_data.mean() / tail_data.std()\n",
    "    else:\n",
    "        sharpe_ratio = 0\n",
    "    return sharpe_ratio\n",
    "\n",
    "def gen_ta_feats(df, n_day_ma, n_day_scale):\n",
    "    # preprocess\n",
    "    df = df \\\n",
    "        .sort_values(['SecuritiesCode','Date']) \\\n",
    "        .groupby('SecuritiesCode') \\\n",
    "        .tail(n_day_ma + n_day_scale) \\\n",
    "        .loc[:, ['SecuritiesCode','Open','High','Low','Close','Volume']] \\\n",
    "        .reset_index(drop=True)\n",
    "    # gen TA feats\n",
    "    df = ta.add_all_ta_features(df, \"Open\", \"High\", \"Low\", \"Close\", \"Volume\", fillna=False) \\\n",
    "        .drop(['Open','High','Low','Close','Volume'], axis=1)\n",
    "    ta_cols = df.columns.tolist()[1:]\n",
    "    ta_cols = [c for c in ta_cols if c not in ['volatility_atr',\n",
    "                                                 'trend_adx',\n",
    "                                                 'trend_adx_pos',\n",
    "                                                 'trend_adx_neg',\n",
    "                                                 'trend_psar_up',\n",
    "                                                 'trend_psar_down',\n",
    "                                                 'momentum_kama']]\n",
    "    ta_cols = ['volume_obv',\n",
    "                 'volume_cmf',\n",
    "                 'volume_fi',\n",
    "                 'volume_em',\n",
    "                 'volume_sma_em',\n",
    "                 'volume_vpt',\n",
    "                 'volume_mfi',\n",
    "                 'volume_nvi',\n",
    "                 'volatility_bbw',\n",
    "                 'volatility_bbhi',\n",
    "                 'volatility_bbli',\n",
    "                 'volatility_kcw',\n",
    "                 'volatility_kcp',\n",
    "                 'volatility_kchi',\n",
    "                 'volatility_kcli',\n",
    "                 'volatility_ui',\n",
    "                 'trend_macd_signal',\n",
    "                 'trend_macd_diff',\n",
    "                 'trend_vortex_ind_pos',\n",
    "                 'trend_vortex_ind_neg',\n",
    "                 'trend_mass_index',\n",
    "                 'trend_dpo',\n",
    "                 'trend_kst',\n",
    "                 'trend_kst_diff',\n",
    "                 'trend_ichimoku_conv',\n",
    "                 'trend_stc',\n",
    "                 'trend_cci',\n",
    "                 'trend_visual_ichimoku_b',\n",
    "                 'trend_aroon_up',\n",
    "                 'trend_aroon_down',\n",
    "                 'trend_psar_up_indicator',\n",
    "                 'trend_psar_down_indicator',\n",
    "                 'momentum_rsi',\n",
    "                 'momentum_stoch_rsi_d',\n",
    "                 'momentum_uo',\n",
    "                 'momentum_wr',\n",
    "                 'momentum_ao',\n",
    "                 'momentum_roc',\n",
    "                 'momentum_ppo',\n",
    "                 'momentum_ppo_hist',\n",
    "                 'momentum_pvo',\n",
    "                 'momentum_pvo_hist',\n",
    "                 'others_dlr']\n",
    "    df = df[['SecuritiesCode'] + ta_cols]\n",
    "    # scale by mean\n",
    "    mean = df \\\n",
    "        .groupby('SecuritiesCode') \\\n",
    "        .tail(n_day_scale)[ta_cols] \\\n",
    "        .abs() \\\n",
    "        .mean()\n",
    "    for c in ta_cols:\n",
    "        df[c] = df[c] / mean[c]\n",
    "    # take last row as features\n",
    "    df = df.groupby('SecuritiesCode').tail(1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7aafdc46",
   "metadata": {
    "_cell_guid": "4a637337-7541-4b8f-bdff-7aca913a0aef",
    "_uuid": "18f34c4c-eadc-4ab7-bbaf-ee1753588b7f",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-07-02T11:32:17.926172Z",
     "iopub.status.busy": "2022-07-02T11:32:17.920457Z",
     "iopub.status.idle": "2022-07-02T11:32:17.947199Z",
     "shell.execute_reply": "2022-07-02T11:32:17.947933Z",
     "shell.execute_reply.started": "2022-07-02T11:09:05.685539Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.074705,
     "end_time": "2022-07-02T11:32:17.948251",
     "exception": false,
     "start_time": "2022-07-02T11:32:17.873546",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7 µs, sys: 0 ns, total: 7 µs\n",
      "Wall time: 11.2 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if FE=='orig':\n",
    "    def get_features(data):\n",
    "        df_prices, df_sec_prices, df_fins, df_opts, df_trades = tuple(data.values())\n",
    "\n",
    "        # base table\n",
    "        features = df_prices \\\n",
    "            .loc[lambda x: x.Date==x.Date.iloc[-1]] \\\n",
    "            .loc[:, ['RowId','Date','SecuritiesCode']] \\\n",
    "            .drop_duplicates(subset='RowId') \\\n",
    "            .reset_index(drop=True)\n",
    "\n",
    "        '''\n",
    "        Major stock prices features\n",
    "        '''\n",
    "        # precalculate new columns\n",
    "        cols = [c for c in df_prices.columns.tolist()[3:] if c!='ExpectedDividend']\n",
    "        df_prices[cols] = df_prices.groupby('SecuritiesCode')[cols].ffill()\n",
    "        df_prices['ret'] = df_prices.groupby('SecuritiesCode').Close.pct_change()\n",
    "        ret_mkt = df_prices.groupby('Date').ret.mean()\n",
    "        var_mkt = (ret_mkt**2).tail(LAGS['12m']).sum()\n",
    "        df_prices['ret_mkt'] = df_prices.Date.map(ret_mkt)\n",
    "        df_prices['spread'] = df_prices['High'] - df_prices['Low']\n",
    "        df_prices['div_ratio'] = df_prices['ExpectedDividend'].fillna(0) / df_prices['Close']\n",
    "        df_prices['dollar_traded'] = np.log(df_prices.Volume * (df_prices.Open + df_prices.Close)/2 + 1)\n",
    "        df_prices['RS_sqrt_vol'] = np.sqrt(np.log(df_prices['High']/df_prices['Close'])*np.log(df_prices['High']/df_prices['Open']) + np.log(df_prices['Low']/df_prices['Close'])*np.log(df_prices['Low']/df_prices['Open']))\n",
    "        df_prices['num_div'] = df_prices.groupby('SecuritiesCode').ExpectedDividend.apply(lambda s: s.notnull().astype(int).cumsum())\n",
    "        df_prices['first_div'] = ((df_prices.num_div==1) & (df_prices.num_div.shift(1)==0)).astype(int)\n",
    "        # previous day return\n",
    "        features['ret'] = features.SecuritiesCode.map(df_prices.groupby('SecuritiesCode').ret.last())\n",
    "        # Change in Close price\n",
    "        for lag in ['3d','1w']:\n",
    "            features[f'price_ma_pctg_ch_{lag}'] = features.SecuritiesCode.map(df_prices.groupby('SecuritiesCode').Close.apply(lambda s: ma_pctg_ch(s, lag)))\n",
    "        for lag in ['1m','3m','6m','12m']:\n",
    "            features[f'price_standard_dist_{lag}'] = features.SecuritiesCode.map(df_prices.groupby('SecuritiesCode').Close.apply(lambda s: standard_dist(s, lag)))\n",
    "        # Change in volume\n",
    "        for lag in ['3d','1w']:\n",
    "            features[f'volume_ma_pctg_ch_{lag}'] = features.SecuritiesCode.map(df_prices.groupby('SecuritiesCode').Volume.apply(lambda s: ma_pctg_ch(s, lag)))\n",
    "        for lag in ['1m','3m','6m','12m']:\n",
    "            features[f'volume_standard_dist_{lag}'] = features.SecuritiesCode.map(df_prices.groupby('SecuritiesCode').Volume.apply(lambda s: standard_dist(s, lag)))\n",
    "        # daily spread\n",
    "        for lag in ['3d','1w']:\n",
    "            features[f'spread_ma_pctg_ch_{lag}'] = features.SecuritiesCode.map(df_prices.groupby('SecuritiesCode').spread.apply(lambda s: ma_pctg_ch(s, lag)))\n",
    "        for lag in ['1m']:\n",
    "            features[f'spread_standard_dist_{lag}'] = features.SecuritiesCode.map(df_prices.groupby('SecuritiesCode').spread.apply(lambda s: standard_dist(s, lag)))\n",
    "        # volatility\n",
    "        for lag in ['1w','1m','3m','12m']:\n",
    "            features[f'volatility_{lag}'] = features.SecuritiesCode.map(df_prices.groupby('SecuritiesCode').ret.apply(lambda s: s.tail(LAGS[lag]).std())) \n",
    "        # change in volatility\n",
    "        features['volatility_diff'] = features['volatility_1w'] - features['volatility_1m']\n",
    "        # market return and volatility\n",
    "        for lag in ['3d','1w','1m','3m']:\n",
    "            features[f'ret_mkt_{lag}'] = ret_mkt.tail(LAGS[lag]).sum()\n",
    "            features[f'vol_mkt_{lag}'] = ret_mkt.tail(LAGS[lag]).std()\n",
    "        # beta\n",
    "        df_prices['beta'] = df_prices.SecuritiesCode.map(df_prices.groupby('SecuritiesCode').apply(lambda df: (df.set_index('Date').ret * ret_mkt).tail(LAGS['12m']).sum() / var_mkt))\n",
    "        features['beta'] = features.SecuritiesCode.map(df_prices.groupby('SecuritiesCode')['beta'].last())\n",
    "        # excess return\n",
    "        df_prices['exret'] = df_prices['ret'] - df_prices['beta'] * df_prices['ret_mkt']\n",
    "        for lag in ['3d','1w','1m','3m']:\n",
    "            features[f'exret_{lag}'] = features.SecuritiesCode.map(df_prices.groupby('SecuritiesCode')['exret'].apply(lambda s: s.tail(LAGS[lag]).sum()))\n",
    "        # div ratio\n",
    "        features['div_ratio'] = features.SecuritiesCode.map(df_prices.groupby('SecuritiesCode').apply(lambda df: df.ExpectedDividend.fillna(0).iloc[-1] / df.Close.tail(LAGS['1m']).mean()))\n",
    "        # change in dollar value traded\n",
    "        for lag in ['1w','1m']:\n",
    "            features[f'dollar_standard_dist_{lag}'] = features.SecuritiesCode.map(df_prices.groupby('SecuritiesCode').dollar_traded.apply(lambda s: standard_dist(s, lag)))\n",
    "        # RS_sqrt_vol\n",
    "        features['RS_sqrt_vol'] = features.SecuritiesCode.map(df_prices.groupby('SecuritiesCode').RS_sqrt_vol.last())\n",
    "        # sharpe\n",
    "        for lag in ['1m','3m']:\n",
    "            features[f'sharpe_{lag}'] = features.SecuritiesCode.map(df_prices.groupby('SecuritiesCode').ret.apply(lambda s: sharpe(s, lag)))\n",
    "        # days since last dividend\n",
    "        features['days_since_last_div'] = features.SecuritiesCode.map((df_prices.Date.iloc[-1] - df_prices.loc[lambda x: x.ExpectedDividend.notnull()].groupby('SecuritiesCode').Date.last()) / np.timedelta64(1,'D'))\n",
    "        # initiate dividend\n",
    "        features['first_div'] = features.SecuritiesCode.map(df_prices.groupby('SecuritiesCode').first_div.last())\n",
    "        # AdjustmentFactor\n",
    "        features['AdjustmentFactor'] = features.SecuritiesCode.map(df_prices.groupby('SecuritiesCode').AdjustmentFactor.apply(lambda s: (s!=1).astype(int).iloc[-1]))\n",
    "\n",
    "\n",
    "        '''\n",
    "        Secondary stock prices features\n",
    "        '''\n",
    "        # precalculate new columns\n",
    "        df_sec_prices['ret'] = df_sec_prices.groupby('SecuritiesCode').Close.pct_change()\n",
    "        df_sec_prices['dollar_traded'] = np.log(df_sec_prices.Volume * (df_sec_prices.Open + df_sec_prices.Close)/2 + 1)\n",
    "        # cross-sectional return & volatility\n",
    "        for n in [1,3]:\n",
    "            features[f'sec_cross_sect_ret_{n}'] = df_sec_prices.groupby('SecuritiesCode').ret.apply(lambda s: s.tail(n).sum()).mean()\n",
    "            features[f'sec_cross_sect_vol_{n}'] = df_sec_prices.groupby('SecuritiesCode').ret.apply(lambda s: s.tail(n).sum()).std()\n",
    "        # Change in volume\n",
    "        for lag in ['3d','1w']:\n",
    "            features[f'sec_volume_ma_pctg_ch_{lag}'] = df_sec_prices.groupby('SecuritiesCode').Volume.apply(lambda s: ma_pctg_ch(s, lag)).mean()\n",
    "        # volatility\n",
    "        for lag in ['1w','1m','3m']:\n",
    "            features[f'sec_volatility_{lag}'] = df_sec_prices.groupby('SecuritiesCode').ret.apply(lambda s: s.tail(LAGS[lag]).std()).mean()\n",
    "        # change in volatility\n",
    "        features['sec_volatility_diff'] = features['sec_volatility_1w'] - features['sec_volatility_1m']\n",
    "        # change in dollar value traded\n",
    "        for lag in ['1w','1m']:\n",
    "            features[f'sec_dollar_standard_dist_{lag}'] = df_sec_prices.groupby('SecuritiesCode').dollar_traded.apply(lambda s: standard_dist(s, lag)).mean()\n",
    "\n",
    "        '''\n",
    "        Time phase features\n",
    "        '''\n",
    "        # day in week\n",
    "        day_in_week_angle = (features.Date.dt.weekday / 5 * 2 * np.pi).iloc[-1]\n",
    "        features['day_in_week_sin'] = np.sin(day_in_week_angle)\n",
    "        features['day_in_week_cos'] = np.cos(day_in_week_angle)\n",
    "        # day in month\n",
    "        day_in_month_angle = ((features.Date.dt.day - 1) / 31 * 2 * np.pi).iloc[-1]\n",
    "        features['day_in_month_sin'] = np.sin(day_in_month_angle)\n",
    "        features['day_in_month_cos'] = np.cos(day_in_month_angle)\n",
    "        # week in year\n",
    "        week_in_year_angle = ((features.Date.dt.week - 1) / 52 * 2 * np.pi).iloc[-1]\n",
    "        features['week_in_year_sin'] = np.sin(week_in_year_angle)\n",
    "        features['week_in_year_cos'] = np.cos(week_in_year_angle)\n",
    "\n",
    "\n",
    "        '''\n",
    "        Financials features\n",
    "        '''\n",
    "        # convert string to numbers\n",
    "        fin_cols = ['NetSales','OperatingProfit','OrdinaryProfit','Profit','EarningsPerShare','TotalAssets','Equity','EquityToAssetRatio','BookValuePerShare',\n",
    "                    'ForecastNetSales','ForecastOperatingProfit','ForecastOrdinaryProfit','ForecastProfit','ForecastEarningsPerShare']\n",
    "        # clean numeric values\n",
    "        df_fins[fin_cols] = df_fins[fin_cols].replace('－',np.nan).astype(float)\n",
    "        # quarter forward fill\n",
    "        df_fins = df_fins.sort_values(['SecuritiesCode','TypeOfCurrentPeriod','Date']).reset_index(drop=True)\n",
    "        df_fins[fin_cols] = df_fins.groupby(['SecuritiesCode','TypeOfCurrentPeriod'])[fin_cols].ffill()\n",
    "        # overall forward fill\n",
    "        df_fins = df_fins.sort_values(['SecuritiesCode','Date']).reset_index(drop=True)\n",
    "        df_fins[fin_cols] = df_fins.groupby('SecuritiesCode')[fin_cols].ffill()\n",
    "        # drop invalid rows\n",
    "        df_fins = df_fins \\\n",
    "            .loc[lambda x: x.NetSales > 0] \\\n",
    "            .sort_values(['SecuritiesCode','Date']) \\\n",
    "            .reset_index(drop=True)\n",
    "        # define columns\n",
    "        df_fins['Close'] = df_fins.SecuritiesCode.map(df_prices.groupby('SecuritiesCode').Close.last())\n",
    "        df_fins['SalesToEquityRatio'] = df_fins['NetSales'] / df_fins['Equity']\n",
    "        df_fins['BookToMarketRatio'] = df_fins['Equity'] / df_fins['Close']\n",
    "        df_fins['ProfitoMarketRatio'] = df_fins['OperatingProfit'] / df_fins['Close']\n",
    "        df_fins['EarningToPriceRatio'] = df_fins['EarningsPerShare'] / df_fins['Close']\n",
    "        fin_cols_static = ['NetSales','OperatingProfit','OrdinaryProfit','Profit','EarningsPerShare','TotalAssets','Equity','BookValuePerShare',\n",
    "                            'ForecastNetSales','ForecastOperatingProfit','ForecastOrdinaryProfit','ForecastProfit','ForecastEarningsPerShare']\n",
    "        fin_cols_ratio = ['EquityToAssetRatio','SalesToEquityRatio','BookToMarketRatio','ProfitoMarketRatio','EarningToPriceRatio']\n",
    "        fin_cols = fin_cols_static + fin_cols_ratio\n",
    "        # fins feature calculation\n",
    "        df1 = df_fins.sort_values(['SecuritiesCode','TypeOfCurrentPeriod','Date']).reset_index(drop=True).groupby(['SecuritiesCode','TypeOfCurrentPeriod'])[fin_cols].nth(-1)\n",
    "        df2 = df_fins.sort_values(['SecuritiesCode','TypeOfCurrentPeriod','Date']).reset_index(drop=True).groupby(['SecuritiesCode','TypeOfCurrentPeriod'])[fin_cols].nth(-2)\n",
    "        df = df1.merge(df2, how='left', left_index=True, right_index=True)\n",
    "        for c in fin_cols:\n",
    "            if c in fin_cols_static:\n",
    "                df[f'{c}_pctg'] = (df[f'{c}_x'] - df[f'{c}_y']) / df[f'{c}_y'].abs()\n",
    "            elif c in fin_cols_ratio:\n",
    "                df[f'{c}_raw'] = df[f'{c}_x']\n",
    "                df[f'{c}_diff'] = df[f'{c}_x'] - df[f'{c}_y']\n",
    "        df = df.drop([c for c in df if c[-2:] in ['_x','_y']], axis=1).reset_index()\n",
    "        feats_fins = df_fins.sort_values(['SecuritiesCode','Date']).groupby('SecuritiesCode').last()['TypeOfCurrentPeriod'].reset_index()\n",
    "        feats_fins = feats_fins.merge(df, how='left', on=['SecuritiesCode','TypeOfCurrentPeriod']).drop('TypeOfCurrentPeriod', axis=1)\n",
    "        features = features.merge(feats_fins, how='left', on='SecuritiesCode')\n",
    "        # num days since last announcement\n",
    "        features['days_since_last_fin'] = (features.Date - features.SecuritiesCode.map(df_fins.groupby('SecuritiesCode').Date.last())) / np.timedelta64(1,'D')\n",
    "\n",
    "\n",
    "        '''\n",
    "        Post-processing\n",
    "        '''\n",
    "        cols = [c for c in features.columns if c not in ['RowId','Date','SecuritiesCode']]\n",
    "        features[cols] = features[cols].replace(np.inf, np.nan).replace(-np.inf, np.nan)\n",
    "        features[cols] = features[cols].fillna(features[cols].mean())\n",
    "        features[cols] = features[cols].astype(np.float32)\n",
    "\n",
    "        return features\n",
    "\n",
    "    \n",
    "elif FE=='ta':\n",
    "\n",
    "    def get_features(data):\n",
    "        df_prices, df_sec_prices, df_fins, df_opts, df_trades = tuple(data.values())\n",
    "    #     df_prices, df_sec_prices, df_fins, df_opts, df_trades = tuple(data.data.values())\n",
    "\n",
    "        # base table\n",
    "        features = df_prices.loc[lambda x: x.Date==x.Date.iloc[-1]][['RowId','Date','SecuritiesCode']]\n",
    "\n",
    "        # precalculate new columns\n",
    "        cols = [c for c in df_prices.columns.tolist()[3:] if c!='ExpectedDividend']\n",
    "        df_prices[cols] = df_prices.groupby('SecuritiesCode')[cols].ffill()\n",
    "        df_prices['ret'] = df_prices.groupby('SecuritiesCode').Close.pct_change()\n",
    "        ret_mkt = df_prices.groupby('Date').ret.mean()\n",
    "\n",
    "        # market return and volatility\n",
    "        for lag in ['3d','1w','1m','3m']:\n",
    "            features[f'ret_mkt_{lag}'] = ret_mkt.tail(LAGS[lag]).sum()\n",
    "            features[f'vol_mkt_{lag}'] = ret_mkt.tail(LAGS[lag]).std()\n",
    "\n",
    "        # TA feats\n",
    "        ta_feats = gen_ta_feats(df=df_prices, n_day_ma=52, n_day_scale=20)\n",
    "        features = features.merge(ta_feats, how='inner', on='SecuritiesCode')\n",
    "\n",
    "        '''\n",
    "        Post-processing\n",
    "        '''\n",
    "        cols = [c for c in features.columns if c not in ['RowId','Date','SecuritiesCode']]\n",
    "        features[cols] = features[cols].replace(np.inf, np.nan).replace(-np.inf, np.nan)\n",
    "        features[cols] = features[cols].fillna(features[cols].mean())\n",
    "        features[cols] = features[cols].astype(np.float32)\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48718d2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-02T11:32:18.006872Z",
     "iopub.status.busy": "2022-07-02T11:32:18.006178Z",
     "iopub.status.idle": "2022-07-02T11:32:18.009141Z",
     "shell.execute_reply": "2022-07-02T11:32:18.009684Z",
     "shell.execute_reply.started": "2022-07-02T11:09:05.740998Z"
    },
    "papermill": {
     "duration": 0.037686,
     "end_time": "2022-07-02T11:32:18.009856",
     "exception": false,
     "start_time": "2022-07-02T11:32:17.972170",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 0 ns, total: 5 µs\n",
      "Wall time: 9.54 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def update_data(test_start_date):\n",
    "    # load train + sup data\n",
    "    df_prices = pd.concat([pd.read_csv(f'{JPX_PATH}/{folder}/stock_prices.csv', parse_dates=['Date']) for folder in ['train_files','supplemental_files']]).reset_index(drop=True)\n",
    "    df_sec_prices = pd.concat([pd.read_csv(f'{JPX_PATH}/{folder}/secondary_stock_prices.csv', parse_dates=['Date']) for folder in ['train_files','supplemental_files']]).reset_index(drop=True)\n",
    "    df_fins = pd.concat([pd.read_csv(f'{JPX_PATH}/{folder}/financials.csv', parse_dates=['Date']) for folder in ['train_files','supplemental_files']]).reset_index(drop=True)\n",
    "    df_opts = pd.concat([pd.read_csv(f'{JPX_PATH}/{folder}/options.csv', parse_dates=['Date']) for folder in ['train_files','supplemental_files']]).reset_index(drop=True)\n",
    "    df_trades = pd.concat([pd.read_csv(f'{JPX_PATH}/{folder}/trades.csv', parse_dates=['Date']) for folder in ['train_files','supplemental_files']]).reset_index(drop=True)\n",
    "\n",
    "    # identify missing dates\n",
    "    if test_start_date.astype('datetime64[Y]').astype(int) + 1970 == 2021:\n",
    "        fe_end_date = np.datetime64('2021-10-27')\n",
    "#         fe_end_date = np.datetime64('2021-12-01')\n",
    "    else:\n",
    "        fe_end_date = data.last_date\n",
    "    extra_dates = df_prices.Date.drop_duplicates().loc[lambda x: (x > fe_end_date) & (x < test_start_date)].tolist()\n",
    "\n",
    "    # FE for missing dates\n",
    "    for i in range(len(extra_dates)):\n",
    "        data.push_forward([df.loc[lambda x: x.Date==extra_dates[i]] for df in [df_prices, df_sec_prices, df_fins, df_opts, df_trades]], append=False, last=False)\n",
    "    \n",
    "    # release memory\n",
    "    del df_prices, df_sec_prices, df_fins, df_opts, df_trades\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b90b4d",
   "metadata": {
    "papermill": {
     "duration": 0.023517,
     "end_time": "2022-07-02T11:32:18.057022",
     "exception": false,
     "start_time": "2022-07-02T11:32:18.033505",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "848c1bfe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-02T11:32:18.119517Z",
     "iopub.status.busy": "2022-07-02T11:32:18.108743Z",
     "iopub.status.idle": "2022-07-02T11:32:18.122635Z",
     "shell.execute_reply": "2022-07-02T11:32:18.122034Z",
     "shell.execute_reply.started": "2022-07-02T11:09:05.754550Z"
    },
    "papermill": {
     "duration": 0.039841,
     "end_time": "2022-07-02T11:32:18.122792",
     "exception": false,
     "start_time": "2022-07-02T11:32:18.082951",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Function to get sub-columns from table for model fitting\n",
    "'''\n",
    "def get_dataset(df, selected_feats, trn_val):\n",
    "    if trn_val=='val':\n",
    "        df = df.groupby('Date').sample(frac=1.0, random_state=SEED)\n",
    "    df = df.reset_index(drop=True)\n",
    "    grp = df.groupby('Date').size().tolist()\n",
    "    qid = df['Date']\n",
    "    X = df[selected_feats]\n",
    "    y = df['target_train']\n",
    "    target = df['Target']\n",
    "    header = df[id_cols]\n",
    "    return X, y, grp, qid, header, target\n",
    "\n",
    "'''\n",
    "Function to predict scores within groups\n",
    "'''\n",
    "def pred_score(model, X):\n",
    "    if MODEL=='reg':\n",
    "        return pd.Series(model.predict(X))\n",
    "    elif MODEL=='clf':\n",
    "        return pd.Series(model.predict_proba(X)[:,1])\n",
    "    elif MODEL=='clf3':\n",
    "        return pd.Series((model.predict_proba(X) * [-1,0,1]).sum(axis=1))\n",
    "\n",
    "'''\n",
    "Function to transform model output to rank prediction table\n",
    "'''\n",
    "def get_pred_df(header, pred_model, y_true_train, y_true, rank_ascending):\n",
    "    df_pred = pd.concat([header[['RowId','Date','SecuritiesCode']].assign(SecuritiesCode=lambda x: x.SecuritiesCode.astype(int)).reset_index(drop=True),\n",
    "                        pd.Series(pred_model).rename('pred_model').reset_index(drop=True),\n",
    "                        y_true_train.reset_index(drop=True),\n",
    "                        y_true.reset_index(drop=True)\n",
    "                        ], axis=1)\n",
    "    df_pred['Rank'] = df_pred.groupby('Date').pred_model.rank(method='first', ascending=rank_ascending).astype(int) - 1\n",
    "    return df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7098306b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-02T11:32:18.179278Z",
     "iopub.status.busy": "2022-07-02T11:32:18.178590Z",
     "iopub.status.idle": "2022-07-02T11:32:18.180599Z",
     "shell.execute_reply": "2022-07-02T11:32:18.181091Z",
     "shell.execute_reply.started": "2022-07-02T11:09:05.770373Z"
    },
    "papermill": {
     "duration": 0.035103,
     "end_time": "2022-07-02T11:32:18.181271",
     "exception": false,
     "start_time": "2022-07-02T11:32:18.146168",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_stock_clust(ret, date, n_day, stock_list):\n",
    "    # raw correlation table\n",
    "    corr = ret.loc[lambda x: pd.to_datetime(x.Date)<=date].pivot(index='Date', columns='SecuritiesCode', values='ret').tail(n_day).corr()\n",
    "    corr = corr.reindex(index=stock_list, columns=stock_list)\n",
    "    cols = [c for c in corr if corr[c].notnull().sum()==0]\n",
    "    corr = corr.drop(cols, axis=0).drop(cols, axis=1)\n",
    "    stocks = corr.columns.tolist()\n",
    "\n",
    "    # clustering\n",
    "    pdist = spc.distance.pdist(corr.values)\n",
    "    linkage = spc.linkage(pdist, method='complete')\n",
    "    idx = spc.fcluster(linkage, 0.5 * pdist.max(), 'distance')\n",
    "    stock_corr_clust = pd.DataFrame({'SecuritiesCode':stocks, 'clust':idx}).assign(Date=date).sort_values('clust').reset_index(drop=True)\n",
    "    return stock_corr_clust"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc064bc2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-27T14:24:06.821508Z",
     "iopub.status.busy": "2022-06-27T14:24:06.821286Z",
     "iopub.status.idle": "2022-06-27T14:24:14.792169Z",
     "shell.execute_reply": "2022-06-27T14:24:14.791105Z",
     "shell.execute_reply.started": "2022-06-27T14:24:06.821481Z"
    },
    "papermill": {
     "duration": 0.023359,
     "end_time": "2022-07-02T11:32:18.228461",
     "exception": false,
     "start_time": "2022-07-02T11:32:18.205102",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eeb4324a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-02T11:32:18.311847Z",
     "iopub.status.busy": "2022-07-02T11:32:18.293884Z",
     "iopub.status.idle": "2022-07-02T11:45:19.164486Z",
     "shell.execute_reply": "2022-07-02T11:45:19.165093Z"
    },
    "papermill": {
     "duration": 780.911185,
     "end_time": "2022-07-02T11:45:19.165497",
     "exception": false,
     "start_time": "2022-07-02T11:32:18.254312",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This version of the API is not optimized and should not be used to estimate the runtime of your code on the hidden test set.\n",
      "[2022-07-02 19:34:08] Pushed to latest date: 2021-10-28 00:00:00\n",
      "[2022-07-02 19:34:32] Pushed to latest date: 2021-10-29 00:00:00\n",
      "[2022-07-02 19:34:56] Pushed to latest date: 2021-11-01 00:00:00\n",
      "[2022-07-02 19:35:21] Pushed to latest date: 2021-11-02 00:00:00\n",
      "[2022-07-02 19:35:45] Pushed to latest date: 2021-11-04 00:00:00\n",
      "[2022-07-02 19:36:10] Pushed to latest date: 2021-11-05 00:00:00\n",
      "[2022-07-02 19:36:35] Pushed to latest date: 2021-11-08 00:00:00\n",
      "[2022-07-02 19:36:59] Pushed to latest date: 2021-11-09 00:00:00\n",
      "[2022-07-02 19:37:23] Pushed to latest date: 2021-11-10 00:00:00\n",
      "[2022-07-02 19:37:48] Pushed to latest date: 2021-11-11 00:00:00\n",
      "[2022-07-02 19:38:13] Pushed to latest date: 2021-11-12 00:00:00\n",
      "[2022-07-02 19:38:37] Pushed to latest date: 2021-11-15 00:00:00\n",
      "[2022-07-02 19:39:01] Pushed to latest date: 2021-11-16 00:00:00\n",
      "[2022-07-02 19:39:26] Pushed to latest date: 2021-11-17 00:00:00\n",
      "[2022-07-02 19:39:50] Pushed to latest date: 2021-11-18 00:00:00\n",
      "[2022-07-02 19:40:14] Pushed to latest date: 2021-11-19 00:00:00\n",
      "[2022-07-02 19:40:38] Pushed to latest date: 2021-11-22 00:00:00\n",
      "[2022-07-02 19:41:02] Pushed to latest date: 2021-11-24 00:00:00\n",
      "[2022-07-02 19:41:27] Pushed to latest date: 2021-11-25 00:00:00\n",
      "[2022-07-02 19:41:51] Pushed to latest date: 2021-11-26 00:00:00\n",
      "[2022-07-02 19:42:15] Pushed to latest date: 2021-11-29 00:00:00\n",
      "[2022-07-02 19:42:39] Pushed to latest date: 2021-11-30 00:00:00\n",
      "[2022-07-02 19:43:03] Pushed to latest date: 2021-12-01 00:00:00\n",
      "[2022-07-02 19:43:27] Pushed to latest date: 2021-12-02 00:00:00\n",
      "[2022-07-02 19:43:52] Pushed to latest date: 2021-12-03 00:00:00\n",
      "[2022-07-02 19:44:19] Pushed to latest date: 2021-12-06 00:00:00\n",
      "[2022-07-02 19:45:04] Pushed to latest date: 2021-12-07 00:00:00\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>SecuritiesCode</th>\n",
       "      <th>Rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-12-06</td>\n",
       "      <td>1301</td>\n",
       "      <td>1321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-12-06</td>\n",
       "      <td>1332</td>\n",
       "      <td>1357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-12-06</td>\n",
       "      <td>1333</td>\n",
       "      <td>886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-12-06</td>\n",
       "      <td>1375</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-12-06</td>\n",
       "      <td>1376</td>\n",
       "      <td>408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>2021-12-07</td>\n",
       "      <td>9990</td>\n",
       "      <td>846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>2021-12-07</td>\n",
       "      <td>9991</td>\n",
       "      <td>1220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>2021-12-07</td>\n",
       "      <td>9993</td>\n",
       "      <td>1967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>2021-12-07</td>\n",
       "      <td>9994</td>\n",
       "      <td>1816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>2021-12-07</td>\n",
       "      <td>9997</td>\n",
       "      <td>1211</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date  SecuritiesCode  Rank\n",
       "0     2021-12-06            1301  1321\n",
       "1     2021-12-06            1332  1357\n",
       "2     2021-12-06            1333   886\n",
       "3     2021-12-06            1375     7\n",
       "4     2021-12-06            1376   408\n",
       "...          ...             ...   ...\n",
       "3995  2021-12-07            9990   846\n",
       "3996  2021-12-07            9991  1220\n",
       "3997  2021-12-07            9993  1967\n",
       "3998  2021-12-07            9994  1816\n",
       "3999  2021-12-07            9997  1211\n",
       "\n",
       "[4000 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowId</th>\n",
       "      <th>Date</th>\n",
       "      <th>SecuritiesCode</th>\n",
       "      <th>pred_model</th>\n",
       "      <th>target_train</th>\n",
       "      <th>Target</th>\n",
       "      <th>Rank</th>\n",
       "      <th>clust</th>\n",
       "      <th>pred_model_mean</th>\n",
       "      <th>pred_model_demean</th>\n",
       "      <th>std</th>\n",
       "      <th>pred_model_vol_penalty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20211206_1301</td>\n",
       "      <td>2021-12-06</td>\n",
       "      <td>1301</td>\n",
       "      <td>-0.154188</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1321</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.143411</td>\n",
       "      <td>-0.010777</td>\n",
       "      <td>0.013762</td>\n",
       "      <td>-9.362997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20211206_1332</td>\n",
       "      <td>2021-12-06</td>\n",
       "      <td>1332</td>\n",
       "      <td>-0.162823</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1357</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.143411</td>\n",
       "      <td>-0.019413</td>\n",
       "      <td>0.018825</td>\n",
       "      <td>-10.284835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20211206_1375</td>\n",
       "      <td>2021-12-06</td>\n",
       "      <td>1375</td>\n",
       "      <td>-0.093793</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.143411</td>\n",
       "      <td>0.049618</td>\n",
       "      <td>0.012052</td>\n",
       "      <td>53.156479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20211206_1376</td>\n",
       "      <td>2021-12-06</td>\n",
       "      <td>1376</td>\n",
       "      <td>-0.132352</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>408</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.143411</td>\n",
       "      <td>0.011059</td>\n",
       "      <td>0.014396</td>\n",
       "      <td>8.948097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20211206_1379</td>\n",
       "      <td>2021-12-06</td>\n",
       "      <td>1379</td>\n",
       "      <td>-0.156482</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1705</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.143411</td>\n",
       "      <td>-0.013071</td>\n",
       "      <td>0.008731</td>\n",
       "      <td>-23.293078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>20211207_9977</td>\n",
       "      <td>2021-12-07</td>\n",
       "      <td>9977</td>\n",
       "      <td>-0.116368</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1848</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.094188</td>\n",
       "      <td>-0.022180</td>\n",
       "      <td>0.009389</td>\n",
       "      <td>-35.243687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>20211207_2768</td>\n",
       "      <td>2021-12-07</td>\n",
       "      <td>2768</td>\n",
       "      <td>-0.042737</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>782</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.041017</td>\n",
       "      <td>-0.001721</td>\n",
       "      <td>0.500256</td>\n",
       "      <td>-0.005136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>20211207_7500</td>\n",
       "      <td>2021-12-07</td>\n",
       "      <td>7500</td>\n",
       "      <td>-0.124725</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1986</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.041017</td>\n",
       "      <td>-0.083708</td>\n",
       "      <td>0.008634</td>\n",
       "      <td>-151.839277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>20211207_8713</td>\n",
       "      <td>2021-12-07</td>\n",
       "      <td>8713</td>\n",
       "      <td>-0.011084</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>780</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.041017</td>\n",
       "      <td>0.029932</td>\n",
       "      <td>1.082455</td>\n",
       "      <td>0.026413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>20211207_9919</td>\n",
       "      <td>2021-12-07</td>\n",
       "      <td>9919</td>\n",
       "      <td>0.014480</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>626</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.041017</td>\n",
       "      <td>0.055496</td>\n",
       "      <td>0.068116</td>\n",
       "      <td>3.859250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              RowId       Date  SecuritiesCode  pred_model  target_train  \\\n",
       "0     20211206_1301 2021-12-06            1301   -0.154188           0.0   \n",
       "1     20211206_1332 2021-12-06            1332   -0.162823           0.0   \n",
       "2     20211206_1375 2021-12-06            1375   -0.093793           0.0   \n",
       "3     20211206_1376 2021-12-06            1376   -0.132352           0.0   \n",
       "4     20211206_1379 2021-12-06            1379   -0.156482           0.0   \n",
       "...             ...        ...             ...         ...           ...   \n",
       "3995  20211207_9977 2021-12-07            9977   -0.116368           0.0   \n",
       "3996  20211207_2768 2021-12-07            2768   -0.042737           0.0   \n",
       "3997  20211207_7500 2021-12-07            7500   -0.124725           0.0   \n",
       "3998  20211207_8713 2021-12-07            8713   -0.011084           0.0   \n",
       "3999  20211207_9919 2021-12-07            9919    0.014480           0.0   \n",
       "\n",
       "      Target  Rank  clust  pred_model_mean  pred_model_demean       std  \\\n",
       "0        0.0  1321      5        -0.143411          -0.010777  0.013762   \n",
       "1        0.0  1357      5        -0.143411          -0.019413  0.018825   \n",
       "2        0.0     7      5        -0.143411           0.049618  0.012052   \n",
       "3        0.0   408      5        -0.143411           0.011059  0.014396   \n",
       "4        0.0  1705      5        -0.143411          -0.013071  0.008731   \n",
       "...      ...   ...    ...              ...                ...       ...   \n",
       "3995     0.0  1848      2        -0.094188          -0.022180  0.009389   \n",
       "3996     0.0   782      1        -0.041017          -0.001721  0.500256   \n",
       "3997     0.0  1986      1        -0.041017          -0.083708  0.008634   \n",
       "3998     0.0   780      1        -0.041017           0.029932  1.082455   \n",
       "3999     0.0   626      1        -0.041017           0.055496  0.068116   \n",
       "\n",
       "      pred_model_vol_penalty  \n",
       "0                  -9.362997  \n",
       "1                 -10.284835  \n",
       "2                  53.156479  \n",
       "3                   8.948097  \n",
       "4                 -23.293078  \n",
       "...                      ...  \n",
       "3995              -35.243687  \n",
       "3996               -0.005136  \n",
       "3997             -151.839277  \n",
       "3998                0.026413  \n",
       "3999                3.859250  \n",
       "\n",
       "[4000 rows x 12 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12min 7s, sys: 37.8 s, total: 12min 45s\n",
      "Wall time: 13min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# init env\n",
    "env = jpx_tokyo_market_prediction.make_env()\n",
    "iter_test = env.iter_test()\n",
    "\n",
    "# init variables\n",
    "if FE=='orig':\n",
    "    data = load_pkl(f'{FE_PATH}/data')\n",
    "elif FE=='ta':\n",
    "    data = load_pkl(f'{FE_PATH}/results (2)/data')\n",
    "sample_prediction_all = []\n",
    "df_pred_val_all = []\n",
    "updated_data = 0\n",
    "close = []\n",
    "df = pd.concat([pd.read_csv(f'{JPX_PATH}/train_files/stock_prices.csv'),\n",
    "                 pd.read_csv(f'{JPX_PATH}/supplemental_files/stock_prices.csv')]) \\\n",
    "    .loc[:, ['Date','SecuritiesCode','Close']]\n",
    "close.append(df)\n",
    "\n",
    "# prepare feature means\n",
    "if FE=='orig':\n",
    "    features = pd.concat([pd.concat(load_pkl(f'{FE_PATH}/features/{filename}')) for filename in os.listdir(f'{FE_PATH}/features')]).sort_values('RowId').reset_index(drop=True)\n",
    "elif FE=='ta':\n",
    "    features1 = pd.concat([pd.concat(load_pkl(f'{FE_PATH}/results (1)/features/{filename}')) for filename in os.listdir(f'{FE_PATH}/results (1)/features')]).sort_values('RowId').reset_index(drop=True)\n",
    "    features2 = pd.concat([pd.concat(load_pkl(f'{FE_PATH}/results (2)/features/{filename}')) for filename in os.listdir(f'{FE_PATH}/results (2)/features')]).sort_values('RowId').reset_index(drop=True)\n",
    "    features = pd.concat([features1, features2]).reset_index(drop=True)\n",
    "    del features1, features2\n",
    "    gc.collect()\n",
    "mean = features.groupby('Date').mean().ffill()\n",
    "mean = mean.fillna(mean.mean())\n",
    "mean = mean.tail(20).mean()\n",
    "del features\n",
    "gc.collect()\n",
    "\n",
    "# iterations\n",
    "for df_prices, df_opts, df_fins, df_trades, df_sec_prices, sample_prediction in iter_test:\n",
    "    \n",
    "    # add new supplemental data for first iteration\n",
    "    date = np.datetime64(sample_prediction.Date.iloc[0])\n",
    "    if updated_data==0:\n",
    "        update_data(date)\n",
    "        updated_data = 1\n",
    "        \n",
    "    # append ret\n",
    "    close.append(df_prices.loc[:, ['Date','SecuritiesCode','Close']])\n",
    "    ret = pd.concat(close) \\\n",
    "        .sort_values(['Date','SecuritiesCode']) \\\n",
    "        .assign(ret = lambda x: x.groupby('SecuritiesCode').Close.pct_change()) \\\n",
    "        .loc[:, ['Date','SecuritiesCode','ret']] \\\n",
    "        .dropna() \\\n",
    "        .drop_duplicates(subset=['Date','SecuritiesCode']) \\\n",
    "        .reset_index(drop=True)\n",
    "        \n",
    "    # set date columns\n",
    "    for df in [df_prices, df_opts, df_fins, df_trades, df_sec_prices]:\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        \n",
    "    # feature engineering of current date\n",
    "    data.push_forward([df_prices, df_sec_prices, df_fins, df_opts, df_trades], append=False, last=False)\n",
    "    features = data.curr_features\n",
    "    \n",
    "    # fillna\n",
    "    cols = [c for c in features.columns if c not in ['RowId','SecuritiesCode','Date']]\n",
    "    features[cols] = features[cols].fillna(mean[cols])\n",
    "    \n",
    "    # train-val split\n",
    "    full_data = features.assign(fold = -1,\n",
    "                                trn_val = 'val',\n",
    "                                Target = 0.0,\n",
    "                                target_train = 0.0)\n",
    "\n",
    "    # define column types\n",
    "    id_cols = ['RowId','Date','SecuritiesCode','fold','trn_val']\n",
    "    all_features = [c for c in list(full_data) if c not in id_cols and c not in ['target_train','Target']]\n",
    "    cat_features = ['AdjustmentFactor','first_div']\n",
    "    time_features = [c for c in all_features if '_mkt' in c] + \\\n",
    "                    [c for c in all_features if c[:4]=='sec_'] + \\\n",
    "                    [c for c in all_features if c[-4:] in ['_sin','_cos']]\n",
    "    stock_features = [c for c in all_features if c not in cat_features + time_features]\n",
    "\n",
    "    # scaling\n",
    "    scaler = load_pkl(f'{MODEL_PATH}/scaler4')\n",
    "    feats = time_features + stock_features\n",
    "    full_data[feats] = scaler.transform(full_data[feats]).astype(np.float32)\n",
    "\n",
    "    # drop market features\n",
    "    if DROP_MARKET_FEATS:\n",
    "        full_data = full_data.drop(time_features, axis=1)\n",
    "        all_features = [c for c in all_features if c not in time_features]\n",
    "\n",
    "    # PCA compression\n",
    "    if RUN_PCA:\n",
    "        if PCA_SPLIT==False:\n",
    "            pca = load_pkl(f'{MODEL_PATH}/pca')        \n",
    "            cols = [f'pc{x}' for x in range(pca.components_.shape[0])]\n",
    "            X = pd.DataFrame(pca.transform(full_data.loc[:, all_features]), columns=cols)\n",
    "            header = full_data.loc[:, [c for c in full_data.columns if c not in all_features]].reset_index(drop=True)\n",
    "            full_data = pd.concat([header, X], axis=1)  \n",
    "            all_features = cols.copy()\n",
    "            stock_features = None\n",
    "            time_features = None\n",
    "\n",
    "        elif PCA_SPLIT==True:\n",
    "            stock_feats = [c for c in all_features if c not in time_features]\n",
    "            market_feats = [c for c in all_features if c in time_features]\n",
    "            pca_stock = load_pkl(f'{MODEL_PATH}/pca_stock')\n",
    "            pca_market = load_pkl(f'{MODEL_PATH}/pca_market')\n",
    "            cols_stock = [f'pc_stock{x}' for x in range(pca_stock.components_.shape[0])]\n",
    "            cols_market = [f'pc_market{x}' for x in range(pca_market.components_.shape[0])]\n",
    "            X_stock = pd.DataFrame(pca_stock.transform(full_data.loc[:, stock_feats]), columns=cols_stock)\n",
    "            X_market = pd.DataFrame(pca_market.transform(full_data.loc[:, market_feats]), columns=cols_market)\n",
    "            header = full_data.loc[:, [c for c in full_data.columns if c not in all_features]].reset_index(drop=True)\n",
    "            full_data = pd.concat([header, X_stock, X_market], axis=1)\n",
    "            all_features = cols_stock + cols_market\n",
    "            stock_features = cols_stock\n",
    "            time_features = cols_market\n",
    "\n",
    "    # model prediction\n",
    "    df_pred_val = []\n",
    "    for seed in SEEDS:\n",
    "        for fold in range(N_FOLD):\n",
    "            X_val, y_val, grp_val, qid_val, header_val, target_val = get_dataset(full_data, SELECTED_FEATS, 'val')\n",
    "            model = load_pkl(f'{MODEL_PATH}/model{fold}_seed{seed}')\n",
    "            pred_val = pred_score(model, X_val)\n",
    "            df_pred_val.append(get_pred_df(header_val, pred_val, y_val, target_val, RANK_ASCENDING))\n",
    "    df_pred_val = pd.concat(df_pred_val).reset_index(drop=True)\n",
    "    df_pred_val = df_pred_val.groupby(['RowId','Date','SecuritiesCode']).mean().reset_index()\n",
    "    df_pred_val['Rank'] = df_pred_val.groupby('Date').pred_model.rank(method='first', ascending=False).astype(int) - 1\n",
    "\n",
    "    # cluster demean\n",
    "    if CLUSTER_DEMEAN:\n",
    "        df_clust = []\n",
    "        for date in df_pred_val.Date.unique():\n",
    "            stock_list = df_pred_val.loc[lambda x: x.Date==date].SecuritiesCode.tolist()\n",
    "            df_clust.append(get_stock_clust(ret, date, CLUST_N_DAY, stock_list))\n",
    "        df_clust = pd.concat(df_clust)\n",
    "        df_pred_val = df_pred_val.merge(df_clust, how='inner', on=['Date','SecuritiesCode'])\n",
    "        clust_mean = df_pred_val.groupby(['Date','clust']).pred_model.mean().reset_index().rename(columns={'pred_model':'pred_model_mean'})\n",
    "        df_pred_val = df_pred_val.merge(clust_mean, how='inner', on=['Date','clust'])\n",
    "        df_pred_val['pred_model_demean'] = df_pred_val.pred_model - df_pred_val.pred_model_mean\n",
    "        df_pred_val['Rank'] = df_pred_val.groupby('Date').pred_model_demean.rank(method='first', ascending=False).astype(int) - 1\n",
    "\n",
    "    # volatility penalty\n",
    "    if VOL_PENALTY:\n",
    "        std = ret.pivot(index='Date', columns='SecuritiesCode', values='ret') \\\n",
    "            .rolling(VOL_N_DAY).std() \\\n",
    "            .stack().reset_index() \\\n",
    "            .dropna() \\\n",
    "            .rename(columns={0:'std'}) \\\n",
    "            .assign(Date = lambda x: pd.to_datetime(x.Date))\n",
    "        df_pred_val = df_pred_val.merge(std, how='inner', on=['Date','SecuritiesCode'])\n",
    "        # identify best power and apply\n",
    "        best_p = VOL_POW\n",
    "        df_pred_val['pred_model_vol_penalty'] = df_pred_val.pred_model_demean / df_pred_val['std'].pow(best_p)\n",
    "        df_pred_val['Rank'] = df_pred_val.groupby('Date').pred_model_vol_penalty.rank(method='first', ascending=False).astype(int) - 1\n",
    "\n",
    "    # final submission\n",
    "    rnk = df_pred_val.set_index('SecuritiesCode').Rank\n",
    "    sample_prediction['Rank'] = sample_prediction['SecuritiesCode'].map(rnk)\n",
    "    sample_prediction['Rank'] = sample_prediction['Rank'].fillna(1000).rank(method='first', ascending=True).astype(int) - 1\n",
    "    env.predict(sample_prediction)\n",
    "    \n",
    "    # save results\n",
    "    sample_prediction_all.append(sample_prediction)\n",
    "    df_pred_val_all.append(df_pred_val)\n",
    "\n",
    "# output\n",
    "sample_prediction_all = pd.concat(sample_prediction_all).reset_index(drop=True)\n",
    "df_pred_val_all = pd.concat(df_pred_val_all).reset_index(drop=True)\n",
    "display(sample_prediction_all)\n",
    "display(df_pred_val_all)\n",
    "if EXPORT:\n",
    "    sample_prediction_all.to_csv('sample_prediction_all.csv', index=False)\n",
    "    df_pred_val_all.to_csv('df_pred_val_all.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095fe586",
   "metadata": {
    "papermill": {
     "duration": 0.039273,
     "end_time": "2022-07-02T11:45:19.242032",
     "exception": false,
     "start_time": "2022-07-02T11:45:19.202759",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a6a7ed",
   "metadata": {
    "papermill": {
     "duration": 0.036354,
     "end_time": "2022-07-02T11:45:19.315100",
     "exception": false,
     "start_time": "2022-07-02T11:45:19.278746",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7c653d",
   "metadata": {
    "papermill": {
     "duration": 0.03617,
     "end_time": "2022-07-02T11:45:19.387123",
     "exception": false,
     "start_time": "2022-07-02T11:45:19.350953",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce7f8fc",
   "metadata": {
    "papermill": {
     "duration": 0.035686,
     "end_time": "2022-07-02T11:45:19.458582",
     "exception": false,
     "start_time": "2022-07-02T11:45:19.422896",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a64b52",
   "metadata": {
    "papermill": {
     "duration": 0.035809,
     "end_time": "2022-07-02T11:45:19.530111",
     "exception": false,
     "start_time": "2022-07-02T11:45:19.494302",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 828.002461,
   "end_time": "2022-07-02T11:45:20.882493",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-07-02T11:31:32.880032",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
