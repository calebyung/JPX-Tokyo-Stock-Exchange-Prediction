{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# import libraries\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nimport os\nfrom os.path import isfile, isdir, join\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime, date\nfrom dateutil.relativedelta import relativedelta\nfrom bs4 import BeautifulSoup\nimport re\nfrom IPython.display import display\nfrom zipfile import ZipFile\nimport pickle\nimport unicodedata\nimport pytz\nfrom joblib import Parallel, delayed\nimport shutil\nimport difflib\nimport random\nimport math\nfrom shutil import copyfile\nimport itertools\nimport time\nfrom tqdm import tqdm\nimport collections\nfrom collections import deque\nimport gc\nimport seaborn as sns\nimport scipy.cluster.hierarchy as spc\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import ndcg_score, accuracy_score\n\nimport lightgbm as lgbm\nimport optuna\nfrom optuna import Trial, visualization\n\nimport matplotlib as mpl\nfrom matplotlib import pyplot as plt\n\nimport jpx_tokyo_market_prediction\n\nfrom utility_script import *\n\npd.set_option('display.max_columns', None)\npd.set_option('display.max_colwidth', None)","metadata":{"_uuid":"2c45ee48-0594-488c-8a45-26a2e5510d83","_cell_guid":"5ae9011f-40e6-497b-a6be-61b748ce1d2d","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-06-27T16:18:58.187542Z","iopub.execute_input":"2022-06-27T16:18:58.187895Z","iopub.status.idle":"2022-06-27T16:19:01.892979Z","shell.execute_reply.started":"2022-06-27T16:18:58.187806Z","shell.execute_reply":"2022-06-27T16:19:01.891815Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style type='text/css'>\n.datatable table.frame { margin-bottom: 0; }\n.datatable table.frame thead { border-bottom: none; }\n.datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n.datatable .bool    { background: #DDDD99; }\n.datatable .object  { background: #565656; }\n.datatable .int     { background: #5D9E5D; }\n.datatable .float   { background: #4040CC; }\n.datatable .str     { background: #CC4040; }\n.datatable .time    { background: #40CC40; }\n.datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n.datatable .frame tbody td { text-align: left; }\n.datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n.datatable th:nth-child(2) { padding-left: 12px; }\n.datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n.datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n.datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n.datatable .sp {  opacity: 0.25;}\n.datatable .footer { font-size: 9px; }\n.datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n</style>\n"},"metadata":{}}]},{"cell_type":"code","source":"'''\nFE Parameters\n'''\nSEED = 0\nLAGS = {'1d':1, '3d':3, '1w':5, '1m':20, '3m':20*3, '6m':20*6, '12m':20*12}\nMAX_DAYS_LAG = max(list(LAGS.values()))\nWIN_SIZE = 500\nJPX_PATH = '../input/jpx-tokyo-stock-exchange-prediction'","metadata":{"_uuid":"c3cfb2dc-8437-441f-b2c0-88b3d4a77281","_cell_guid":"c0f0bfa1-3dea-43a3-b2f1-8a2c268d6339","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-06-27T16:19:01.895045Z","iopub.execute_input":"2022-06-27T16:19:01.895586Z","iopub.status.idle":"2022-06-27T16:19:01.903503Z","shell.execute_reply.started":"2022-06-27T16:19:01.895538Z","shell.execute_reply":"2022-06-27T16:19:01.902881Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"'''\nModel Params\n'''\n# basic\nSEED = 0\nSEEDS = [1,2,3,4,5]\nJPX_PATH = '../input/jpx-tokyo-stock-exchange-prediction'\nMODEL_PATH = '../input/jpx-model-lgbm-reg'\n\n# feature composition\nDROP_MARKET_FEATS = False\n\n# PCA\nRUN_PCA = True\nPCA_SPLIT = True\nN_COMP = 0.9\n\n# target definition\nRANK_ASCENDING = False # set this to False if model prediction is same direction of Target\nTARGET_POW = 0\n\n# data split\nN_FOLD = 5\n\n# final features\nSELECTED_FEATS = ''.split(', ')\n\n# optimization\nCLUSTER_DEMEAN = True\nCLUST_N_DAY = 60\nVOL_PENALTY = True\nVOL_N_DAY = 60\nVOL_POW = 1","metadata":{"execution":{"iopub.status.busy":"2022-06-27T16:19:01.904699Z","iopub.execute_input":"2022-06-27T16:19:01.905074Z","iopub.status.idle":"2022-06-27T16:19:01.916334Z","shell.execute_reply.started":"2022-06-27T16:19:01.905043Z","shell.execute_reply":"2022-06-27T16:19:01.915648Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{}},{"cell_type":"code","source":"class JPXData:\n    def __init__(self, window_size, df_names):\n        self.size = 0\n        self.window_size = window_size\n        self.df_names = df_names\n        self.num_df = len(df_names)\n        self.data = {df_name : pd.DataFrame() for df_name in df_names}\n        self.row_counts = {df_name : [] for df_name in df_names}\n        self.dates = []\n        self.first_date, self.last_date = None, None\n        self.features = []\n        self.curr_features = None\n        self.n_day_hist = 0\n        self.init_folders()\n        \n    def init_folders(self):\n        shutil.rmtree(path='./features', ignore_errors=True)\n        os.mkdir('./features')\n        \n    def append_data(self):\n        self.features.append(self.curr_features)\n        self.n_day_hist += 1\n        \n    def archive_data(self):\n        save_pkl(self.features, f'./features/features_{self.n_day_hist}')\n        self.clear_hist()\n        \n    def clear_hist(self):\n        self.features = []\n        \n    def push_forward(self, new_data, append, last):\n        # assign names to new data assuming the same as df_names\n        new_data = dict(zip(self.df_names, new_data))\n        # case when no enough data\n        if self.size < self.window_size:\n            for df_name in self.df_names:\n                self.data[df_name] = pd.concat([self.data[df_name], new_data[df_name]]).reset_index(drop=True)\n                self.row_counts[df_name] = self.row_counts[df_name] + [new_data[df_name].shape[0]]\n            self.dates = self.dates + [new_data[self.df_names[0]].Date.iloc[0]] \n            self.size += 1\n        # general case (shift by 1 day)\n        else:\n            for df_name in self.df_names:\n                self.data[df_name] = pd.concat([self.data[df_name].iloc[self.row_counts[df_name][0]:], new_data[df_name]]).reset_index(drop=True)\n                self.row_counts[df_name] = self.row_counts[df_name][1:] + [new_data[df_name].shape[0]]\n            self.dates = self.dates[1:] + [new_data[self.df_names[0]].Date.iloc[0]]  \n        # update date range\n        self.first_date, self.last_date = self.dates[0], self.dates[-1]\n        # generate features\n        if self.size == self.window_size:\n            self.curr_features = get_features(self.data)\n            if append==True:\n                self.append_data()\n                if (self.n_day_hist%20 == 0 and self.n_day_hist > 0) or last==True:\n                    self.archive_data()\n        log(f'Pushed to latest date: {self.last_date}')","metadata":{"_uuid":"54a20792-c5bc-4ee2-8162-07364e311d9c","_cell_guid":"6a3c94c5-9152-4477-9b79-dec0b881fd0e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-06-27T16:19:01.920018Z","iopub.execute_input":"2022-06-27T16:19:01.920557Z","iopub.status.idle":"2022-06-27T16:19:01.941856Z","shell.execute_reply.started":"2022-06-27T16:19:01.920505Z","shell.execute_reply":"2022-06-27T16:19:01.940569Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def standard_dist(s, lag):\n    tail_data = s.tail(LAGS[lag])\n    return (s.iloc[-1] - tail_data.mean()) / tail_data.std()\n\ndef ma_pctg_ch(s, lag):\n    return s.iloc[-1] / s.tail(LAGS[lag]).mean() - 1\n\ndef sharpe(s, lag):\n    tail_data = s.tail(LAGS[lag])\n    std = tail_data.std()\n    if std > 0:\n        sharpe_ratio = tail_data.mean() / tail_data.std()\n    else:\n        sharpe_ratio = 0\n    return sharpe_ratio","metadata":{"_uuid":"c4fccf97-02d7-4f36-a5fa-53efcfd2edc6","_cell_guid":"ae8fcff3-07c3-4ef5-ade3-61d6dc34243b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-06-27T16:19:01.943285Z","iopub.execute_input":"2022-06-27T16:19:01.945111Z","iopub.status.idle":"2022-06-27T16:19:01.960684Z","shell.execute_reply.started":"2022-06-27T16:19:01.945060Z","shell.execute_reply":"2022-06-27T16:19:01.959808Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"%%time\n\ndef get_features(data):\n    df_prices, df_sec_prices, df_fins, df_opts, df_trades = tuple(data.values())\n\n    # base table\n    features = df_prices \\\n        .loc[lambda x: x.Date==x.Date.iloc[-1]] \\\n        .loc[:, ['RowId','Date','SecuritiesCode']] \\\n        .drop_duplicates(subset='RowId') \\\n        .reset_index(drop=True)\n\n    '''\n    Major stock prices features\n    '''\n    # precalculate new columns\n    cols = [c for c in df_prices.columns.tolist()[3:] if c!='ExpectedDividend']\n    df_prices[cols] = df_prices.groupby('SecuritiesCode')[cols].ffill()\n    df_prices['ret'] = df_prices.groupby('SecuritiesCode').Close.pct_change()\n    ret_mkt = df_prices.groupby('Date').ret.mean()\n    var_mkt = (ret_mkt**2).tail(LAGS['12m']).sum()\n    df_prices['ret_mkt'] = df_prices.Date.map(ret_mkt)\n    df_prices['spread'] = df_prices['High'] - df_prices['Low']\n    df_prices['div_ratio'] = df_prices['ExpectedDividend'].fillna(0) / df_prices['Close']\n    df_prices['dollar_traded'] = np.log(df_prices.Volume * (df_prices.Open + df_prices.Close)/2 + 1)\n    df_prices['RS_sqrt_vol'] = np.sqrt(np.log(df_prices['High']/df_prices['Close'])*np.log(df_prices['High']/df_prices['Open']) + np.log(df_prices['Low']/df_prices['Close'])*np.log(df_prices['Low']/df_prices['Open']))\n    df_prices['num_div'] = df_prices.groupby('SecuritiesCode').ExpectedDividend.apply(lambda s: s.notnull().astype(int).cumsum())\n    df_prices['first_div'] = ((df_prices.num_div==1) & (df_prices.num_div.shift(1)==0)).astype(int)\n    # previous day return\n    features['ret'] = features.SecuritiesCode.map(df_prices.groupby('SecuritiesCode').ret.last())\n    # Change in Close price\n    for lag in ['3d','1w']:\n        features[f'price_ma_pctg_ch_{lag}'] = features.SecuritiesCode.map(df_prices.groupby('SecuritiesCode').Close.apply(lambda s: ma_pctg_ch(s, lag)))\n    for lag in ['1m','3m','6m','12m']:\n        features[f'price_standard_dist_{lag}'] = features.SecuritiesCode.map(df_prices.groupby('SecuritiesCode').Close.apply(lambda s: standard_dist(s, lag)))\n    # Change in volume\n    for lag in ['3d','1w']:\n        features[f'volume_ma_pctg_ch_{lag}'] = features.SecuritiesCode.map(df_prices.groupby('SecuritiesCode').Volume.apply(lambda s: ma_pctg_ch(s, lag)))\n    for lag in ['1m','3m','6m','12m']:\n        features[f'volume_standard_dist_{lag}'] = features.SecuritiesCode.map(df_prices.groupby('SecuritiesCode').Volume.apply(lambda s: standard_dist(s, lag)))\n    # daily spread\n    for lag in ['3d','1w']:\n        features[f'spread_ma_pctg_ch_{lag}'] = features.SecuritiesCode.map(df_prices.groupby('SecuritiesCode').spread.apply(lambda s: ma_pctg_ch(s, lag)))\n    for lag in ['1m']:\n        features[f'spread_standard_dist_{lag}'] = features.SecuritiesCode.map(df_prices.groupby('SecuritiesCode').spread.apply(lambda s: standard_dist(s, lag)))\n    # volatility\n    for lag in ['1w','1m','3m','12m']:\n        features[f'volatility_{lag}'] = features.SecuritiesCode.map(df_prices.groupby('SecuritiesCode').ret.apply(lambda s: s.tail(LAGS[lag]).std())) \n    # change in volatility\n    features['volatility_diff'] = features['volatility_1w'] - features['volatility_1m']\n    # market return and volatility\n    for lag in ['3d','1w','1m','3m']:\n        features[f'ret_mkt_{lag}'] = ret_mkt.tail(LAGS[lag]).sum()\n        features[f'vol_mkt_{lag}'] = ret_mkt.tail(LAGS[lag]).std()\n    # beta\n    df_prices['beta'] = df_prices.SecuritiesCode.map(df_prices.groupby('SecuritiesCode').apply(lambda df: (df.set_index('Date').ret * ret_mkt).tail(LAGS['12m']).sum() / var_mkt))\n    features['beta'] = features.SecuritiesCode.map(df_prices.groupby('SecuritiesCode')['beta'].last())\n    # excess return\n    df_prices['exret'] = df_prices['ret'] - df_prices['beta'] * df_prices['ret_mkt']\n    for lag in ['3d','1w','1m','3m']:\n        features[f'exret_{lag}'] = features.SecuritiesCode.map(df_prices.groupby('SecuritiesCode')['exret'].apply(lambda s: s.tail(LAGS[lag]).sum()))\n    # div ratio\n    features['div_ratio'] = features.SecuritiesCode.map(df_prices.groupby('SecuritiesCode').apply(lambda df: df.ExpectedDividend.fillna(0).iloc[-1] / df.Close.tail(LAGS['1m']).mean()))\n    # change in dollar value traded\n    for lag in ['1w','1m']:\n        features[f'dollar_standard_dist_{lag}'] = features.SecuritiesCode.map(df_prices.groupby('SecuritiesCode').dollar_traded.apply(lambda s: standard_dist(s, lag)))\n    # RS_sqrt_vol\n    features['RS_sqrt_vol'] = features.SecuritiesCode.map(df_prices.groupby('SecuritiesCode').RS_sqrt_vol.last())\n    # sharpe\n    for lag in ['1m','3m']:\n        features[f'sharpe_{lag}'] = features.SecuritiesCode.map(df_prices.groupby('SecuritiesCode').ret.apply(lambda s: sharpe(s, lag)))\n    # days since last dividend\n    features['days_since_last_div'] = features.SecuritiesCode.map((df_prices.Date.iloc[-1] - df_prices.loc[lambda x: x.ExpectedDividend.notnull()].groupby('SecuritiesCode').Date.last()) / np.timedelta64(1,'D'))\n    # initiate dividend\n    features['first_div'] = features.SecuritiesCode.map(df_prices.groupby('SecuritiesCode').first_div.last())\n    # AdjustmentFactor\n    features['AdjustmentFactor'] = features.SecuritiesCode.map(df_prices.groupby('SecuritiesCode').AdjustmentFactor.apply(lambda s: (s!=1).astype(int).iloc[-1]))\n\n\n    '''\n    Secondary stock prices features\n    '''\n    # precalculate new columns\n    df_sec_prices['ret'] = df_sec_prices.groupby('SecuritiesCode').Close.pct_change()\n    df_sec_prices['dollar_traded'] = np.log(df_sec_prices.Volume * (df_sec_prices.Open + df_sec_prices.Close)/2 + 1)\n    # cross-sectional return & volatility\n    for n in [1,3]:\n        features[f'sec_cross_sect_ret_{n}'] = df_sec_prices.groupby('SecuritiesCode').ret.apply(lambda s: s.tail(n).sum()).mean()\n        features[f'sec_cross_sect_vol_{n}'] = df_sec_prices.groupby('SecuritiesCode').ret.apply(lambda s: s.tail(n).sum()).std()\n    # Change in volume\n    for lag in ['3d','1w']:\n        features[f'sec_volume_ma_pctg_ch_{lag}'] = df_sec_prices.groupby('SecuritiesCode').Volume.apply(lambda s: ma_pctg_ch(s, lag)).mean()\n    # volatility\n    for lag in ['1w','1m','3m']:\n        features[f'sec_volatility_{lag}'] = df_sec_prices.groupby('SecuritiesCode').ret.apply(lambda s: s.tail(LAGS[lag]).std()).mean()\n    # change in volatility\n    features['sec_volatility_diff'] = features['sec_volatility_1w'] - features['sec_volatility_1m']\n    # change in dollar value traded\n    for lag in ['1w','1m']:\n        features[f'sec_dollar_standard_dist_{lag}'] = df_sec_prices.groupby('SecuritiesCode').dollar_traded.apply(lambda s: standard_dist(s, lag)).mean()\n\n    '''\n    Time phase features\n    '''\n    # day in week\n    day_in_week_angle = (features.Date.dt.weekday / 5 * 2 * np.pi).iloc[-1]\n    features['day_in_week_sin'] = np.sin(day_in_week_angle)\n    features['day_in_week_cos'] = np.cos(day_in_week_angle)\n    # day in month\n    day_in_month_angle = ((features.Date.dt.day - 1) / 31 * 2 * np.pi).iloc[-1]\n    features['day_in_month_sin'] = np.sin(day_in_month_angle)\n    features['day_in_month_cos'] = np.cos(day_in_month_angle)\n    # week in year\n    week_in_year_angle = ((features.Date.dt.week - 1) / 52 * 2 * np.pi).iloc[-1]\n    features['week_in_year_sin'] = np.sin(week_in_year_angle)\n    features['week_in_year_cos'] = np.cos(week_in_year_angle)\n\n\n    '''\n    Financials features\n    '''\n    # convert string to numbers\n    fin_cols = ['NetSales','OperatingProfit','OrdinaryProfit','Profit','EarningsPerShare','TotalAssets','Equity','EquityToAssetRatio','BookValuePerShare',\n                'ForecastNetSales','ForecastOperatingProfit','ForecastOrdinaryProfit','ForecastProfit','ForecastEarningsPerShare']\n    # clean numeric values\n    df_fins[fin_cols] = df_fins[fin_cols].replace('－',np.nan).astype(float)\n    # quarter forward fill\n    df_fins = df_fins.sort_values(['SecuritiesCode','TypeOfCurrentPeriod','Date']).reset_index(drop=True)\n    df_fins[fin_cols] = df_fins.groupby(['SecuritiesCode','TypeOfCurrentPeriod'])[fin_cols].ffill()\n    # overall forward fill\n    df_fins = df_fins.sort_values(['SecuritiesCode','Date']).reset_index(drop=True)\n    df_fins[fin_cols] = df_fins.groupby('SecuritiesCode')[fin_cols].ffill()\n    # drop invalid rows\n    df_fins = df_fins \\\n        .loc[lambda x: x.NetSales > 0] \\\n        .sort_values(['SecuritiesCode','Date']) \\\n        .reset_index(drop=True)\n    # define columns\n    df_fins['Close'] = df_fins.SecuritiesCode.map(df_prices.groupby('SecuritiesCode').Close.last())\n    df_fins['SalesToEquityRatio'] = df_fins['NetSales'] / df_fins['Equity']\n    df_fins['BookToMarketRatio'] = df_fins['Equity'] / df_fins['Close']\n    df_fins['ProfitoMarketRatio'] = df_fins['OperatingProfit'] / df_fins['Close']\n    df_fins['EarningToPriceRatio'] = df_fins['EarningsPerShare'] / df_fins['Close']\n    fin_cols_static = ['NetSales','OperatingProfit','OrdinaryProfit','Profit','EarningsPerShare','TotalAssets','Equity','BookValuePerShare',\n                        'ForecastNetSales','ForecastOperatingProfit','ForecastOrdinaryProfit','ForecastProfit','ForecastEarningsPerShare']\n    fin_cols_ratio = ['EquityToAssetRatio','SalesToEquityRatio','BookToMarketRatio','ProfitoMarketRatio','EarningToPriceRatio']\n    fin_cols = fin_cols_static + fin_cols_ratio\n    # fins feature calculation\n    df1 = df_fins.sort_values(['SecuritiesCode','TypeOfCurrentPeriod','Date']).reset_index(drop=True).groupby(['SecuritiesCode','TypeOfCurrentPeriod'])[fin_cols].nth(-1)\n    df2 = df_fins.sort_values(['SecuritiesCode','TypeOfCurrentPeriod','Date']).reset_index(drop=True).groupby(['SecuritiesCode','TypeOfCurrentPeriod'])[fin_cols].nth(-2)\n    df = df1.merge(df2, how='left', left_index=True, right_index=True)\n    for c in fin_cols:\n        if c in fin_cols_static:\n            df[f'{c}_pctg'] = (df[f'{c}_x'] - df[f'{c}_y']) / df[f'{c}_y'].abs()\n        elif c in fin_cols_ratio:\n            df[f'{c}_raw'] = df[f'{c}_x']\n            df[f'{c}_diff'] = df[f'{c}_x'] - df[f'{c}_y']\n    df = df.drop([c for c in df if c[-2:] in ['_x','_y']], axis=1).reset_index()\n    feats_fins = df_fins.sort_values(['SecuritiesCode','Date']).groupby('SecuritiesCode').last()['TypeOfCurrentPeriod'].reset_index()\n    feats_fins = feats_fins.merge(df, how='left', on=['SecuritiesCode','TypeOfCurrentPeriod']).drop('TypeOfCurrentPeriod', axis=1)\n    features = features.merge(feats_fins, how='left', on='SecuritiesCode')\n    # num days since last announcement\n    features['days_since_last_fin'] = (features.Date - features.SecuritiesCode.map(df_fins.groupby('SecuritiesCode').Date.last())) / np.timedelta64(1,'D')\n\n\n    '''\n    Post-processing\n    '''\n    cols = [c for c in features.columns if c not in ['RowId','Date','SecuritiesCode']]\n    features[cols] = features[cols].replace(np.inf, np.nan).replace(-np.inf, np.nan)\n    features[cols] = features[cols].fillna(features[cols].mean())\n    features[cols] = features[cols].astype(np.float32)\n    \n    return features","metadata":{"_uuid":"18f34c4c-eadc-4ab7-bbaf-ee1753588b7f","_cell_guid":"4a637337-7541-4b8f-bdff-7aca913a0aef","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-06-27T16:19:01.962244Z","iopub.execute_input":"2022-06-27T16:19:01.962779Z","iopub.status.idle":"2022-06-27T16:19:02.012553Z","shell.execute_reply.started":"2022-06-27T16:19:01.962733Z","shell.execute_reply":"2022-06-27T16:19:02.010674Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"CPU times: user 10 µs, sys: 2 µs, total: 12 µs\nWall time: 16.2 µs\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\n\ndef update_data(test_start_date):\n    # load train + sup data\n    df_prices = pd.concat([pd.read_csv(f'{JPX_PATH}/{folder}/stock_prices.csv', parse_dates=['Date']) for folder in ['train_files','supplemental_files']]).reset_index(drop=True)\n    df_sec_prices = pd.concat([pd.read_csv(f'{JPX_PATH}/{folder}/secondary_stock_prices.csv', parse_dates=['Date']) for folder in ['train_files','supplemental_files']]).reset_index(drop=True)\n    df_fins = pd.concat([pd.read_csv(f'{JPX_PATH}/{folder}/financials.csv', parse_dates=['Date']) for folder in ['train_files','supplemental_files']]).reset_index(drop=True)\n    df_opts = pd.concat([pd.read_csv(f'{JPX_PATH}/{folder}/options.csv', parse_dates=['Date']) for folder in ['train_files','supplemental_files']]).reset_index(drop=True)\n    df_trades = pd.concat([pd.read_csv(f'{JPX_PATH}/{folder}/trades.csv', parse_dates=['Date']) for folder in ['train_files','supplemental_files']]).reset_index(drop=True)\n\n    # identify missing dates\n    if test_start_date.astype('datetime64[Y]').astype(int) + 1970 == 2021:\n#         fe_end_date = np.datetime64('2021-10-27')\n        fe_end_date = np.datetime64('2021-12-01')\n    else:\n        fe_end_date = data.last_date\n    extra_dates = df_prices.Date.drop_duplicates().loc[lambda x: (x > fe_end_date) & (x < test_start_date)].tolist()\n\n    # FE for missing dates\n    for i in range(len(extra_dates)):\n        last = True if i==len(extra_dates)-1 else False\n        data.push_forward([df.loc[lambda x: x.Date==extra_dates[i]] for df in [df_prices, df_sec_prices, df_fins, df_opts, df_trades]], append=False, last=last)\n    \n    # release memory\n    del df_prices, df_sec_prices, df_fins, df_opts, df_trades\n    gc.collect()\n","metadata":{"execution":{"iopub.status.busy":"2022-06-27T16:19:02.014079Z","iopub.execute_input":"2022-06-27T16:19:02.014496Z","iopub.status.idle":"2022-06-27T16:19:02.037196Z","shell.execute_reply.started":"2022-06-27T16:19:02.014448Z","shell.execute_reply":"2022-06-27T16:19:02.035806Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"CPU times: user 8 µs, sys: 1 µs, total: 9 µs\nWall time: 14.3 µs\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"'''\nFunction to get sub-columns from table for model fitting\n'''\ndef get_dataset(df, selected_feats, trn_val):\n    if trn_val=='val':\n        df = df.groupby('Date').sample(frac=1.0, random_state=SEED)\n    df = df.reset_index(drop=True)\n    grp = df.groupby('Date').size().tolist()\n    qid = df['Date']\n    X = df[selected_feats]\n    y = df['target_train']\n    target = df['Target']\n    header = df[id_cols]\n    return X, y, grp, qid, header, target\n\n'''\nFunction to predict scores within groups\n'''\ndef pred_score(model, X):\n    return pd.Series(model.predict(X))\n\n'''\nFunction to predict rankings within groups\n'''\ndef pred_rank(model, X, qid):\n    X_ = X.assign(Date=qid)\n    rnk = []\n    for date in X_.Date.unique():\n        rnk += pd.Series(model.predict(X_.loc[lambda x: x.Date==date].drop('Date',axis=1))).rank(method='first').tolist()\n    return pd.Series(rnk)\n\n'''\nFunction to transform model output to rank prediction table\n'''\ndef get_pred_df(header, pred_model, y_true_train, y_true, rank_ascending):\n    df_pred = pd.concat([header[['RowId','Date','SecuritiesCode']].assign(Date=lambda x: x.Date.dt.strftime('%Y-%m-%d'), SecuritiesCode=lambda x: x.SecuritiesCode.astype(int)).reset_index(drop=True),\n                        pd.Series(pred_model).rename('pred_model').reset_index(drop=True),\n                        y_true_train.reset_index(drop=True),\n                        y_true.reset_index(drop=True)\n                        ], axis=1)\n    df_pred['Rank'] = df_pred.groupby('Date').pred_model.rank(method='first', ascending=rank_ascending).astype(int) - 1\n    return df_pred\n\n'''\nOfficial function to calculate Sharpe Ratio given a prediction\n'''\ndef _calc_spread_return_per_day(df, portfolio_size, toprank_weight_ratio):\n    assert df['Rank'].min() == 0\n    assert df['Rank'].max() == len(df['Rank']) - 1\n    weights = np.linspace(start=toprank_weight_ratio, stop=1, num=portfolio_size)\n    purchase = (df.sort_values(by='Rank')['Target'][:portfolio_size] * weights).sum() / weights.mean()\n    short = (df.sort_values(by='Rank', ascending=False)['Target'][:portfolio_size] * weights).sum() / weights.mean()\n    return purchase - short\n\ndef my_calc_spread_return_per_day(df, portfolio_size, toprank_weight_ratio):\n    assert df['Rank'].min() == 0\n    assert df['Rank'].max() == len(df['Rank']) - 1\n    weights = np.linspace(start=toprank_weight_ratio, stop=1, num=portfolio_size)\n    purchase = (df.sort_values(by='Rank')['Target'][:portfolio_size] * weights).sum() / weights.mean()\n    short = (df.sort_values(by='Rank', ascending=False)['Target'][:portfolio_size] * weights).sum() / weights.mean()\n    return pd.DataFrame({'long':[purchase],'short':[short],'net':[purchase - short]})\n\ndef calc_spread_return_sharpe(df: pd.DataFrame, portfolio_size: int = 200, toprank_weight_ratio: float = 2) -> float:\n    buf = df.groupby('Date').apply(_calc_spread_return_per_day, portfolio_size, toprank_weight_ratio)\n    sharpe_ratio = buf.mean() / buf.std()\n    return sharpe_ratio","metadata":{"execution":{"iopub.status.busy":"2022-06-27T16:19:02.038794Z","iopub.execute_input":"2022-06-27T16:19:02.039627Z","iopub.status.idle":"2022-06-27T16:19:02.065126Z","shell.execute_reply.started":"2022-06-27T16:19:02.039580Z","shell.execute_reply":"2022-06-27T16:19:02.063856Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def get_stock_clust(ret, date, n_day, stock_list):\n    # raw correlation table\n    corr = ret.loc[lambda x: pd.to_datetime(x.Date)<=date].pivot(index='Date', columns='SecuritiesCode', values='ret').tail(n_day).corr()\n    corr = corr.reindex(index=stock_list, columns=stock_list)\n    cols = [c for c in corr if corr[c].notnull().sum()==0]\n    corr = corr.drop(cols, axis=0).drop(cols, axis=1)\n    stocks = corr.columns.tolist()\n\n    # clustering\n    pdist = spc.distance.pdist(corr.values)\n    linkage = spc.linkage(pdist, method='complete')\n    idx = spc.fcluster(linkage, 0.5 * pdist.max(), 'distance')\n    stock_corr_clust = pd.DataFrame({'SecuritiesCode':stocks, 'clust':idx}).assign(Date=date).sort_values('clust').reset_index(drop=True)\n#     clust_map = stock_corr_clust.assign(val=1).pivot('stocks','clust','val').fillna(0).astype(int)\n    return stock_corr_clust","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Execution","metadata":{"execution":{"iopub.status.busy":"2022-06-27T14:24:06.821286Z","iopub.execute_input":"2022-06-27T14:24:06.821508Z","iopub.status.idle":"2022-06-27T14:24:14.792169Z","shell.execute_reply.started":"2022-06-27T14:24:06.821481Z","shell.execute_reply":"2022-06-27T14:24:14.791105Z"}}},{"cell_type":"code","source":"%%time\n\n# init env\nenv = jpx_tokyo_market_prediction.make_env()\niter_test = env.iter_test()\n\n# init variables\ndata = load_pkl('../input/jpx-feature-engineering/data')\nupdated_data = 0\nclose = []\ndf = pd.concat([pd.read_csv(f'{JPX_PATH}/train_files/stock_prices.csv'),\n                 pd.read_csv(f'{JPX_PATH}/supplemental_files/stock_prices.csv')]) \\\n    .loc[:, ['Date','SecuritiesCode','Close']]\nclose.append(df)\n\n# iterations\nfor df_prices, df_opts, df_fins, df_trades, df_sec_prices, sample_prediction in iter_test:\n    \n    # add new supplemental data for first iteration\n    date = np.datetime64(sample_prediction.Date.iloc[0])\n    if updated_data==0:\n        update_data(date)\n        updated_data = 1\n        \n    # append ret\n    close.append(df_prices.loc[:, ['Date','SecuritiesCode','Close']])\n    ret = pd.concat(close) \\\n        .sort_values(['Date','SecuritiesCode']) \\\n        .assign(ret = lambda x: x.groupby('SecuritiesCode').Close.pct_change()) \\\n        .loc[:, ['Date','SecuritiesCode','ret']] \\\n        .dropna() \\\n        .drop_duplicates(subset=['Date','SecuritiesCode']) \\\n        .reset_index(drop=True)\n        \n    # set date columns\n    for df in [df_prices, df_opts, df_fins, df_trades, df_sec_prices]:\n        df['Date'] = pd.to_datetime(df['Date'])\n        \n    # feature engineering of current date\n    data.push_forward([df_prices, df_sec_prices, df_fins, df_opts, df_trades], append=False, last=False)\n    features = data.curr_features\n    \n    # train-val split\n    full_data = features.assign(fold = -1,\n                                trn_val = 'val',\n                                Target = 0.0,\n                                target_train = 0.0)\n\n    # define column types\n    id_cols = ['RowId','Date','SecuritiesCode','fold','trn_val']\n    all_features = [c for c in list(full_data) if c not in id_cols and c not in ['target_train','Target']]\n    cat_features = ['AdjustmentFactor','first_div']\n    time_features = [c for c in all_features if '_mkt' in c] + \\\n                    [c for c in all_features if c[:4]=='sec_'] + \\\n                    [c for c in all_features if c[-4:] in ['_sin','_cos']]\n    stock_features = [c for c in all_features if c not in cat_features + time_features]\n\n    # scaling\n    scaler = load_pkl(f'{MODEL_PATH}/scaler4')\n    feats = time_features + stock_features\n    full_data[feats] = scaler.transform(full_data[feats]).astype(np.float32)\n\n    # drop market features\n    if DROP_MARKET_FEATS:\n        full_data = full_data.drop(time_features, axis=1)\n        all_features = [c for c in all_features if c not in time_features]\n\n    # PCA compression\n    if RUN_PCA:\n        if PCA_SPLIT==False:\n            pca = load_pkl(f'{MODEL_PATH}/pca')        \n            cols = [f'pc{x}' for x in range(pca.components_.shape[0])]\n            X = pd.DataFrame(pca.transform(full_data.loc[:, all_features]), columns=cols)\n            header = full_data.loc[:, [c for c in full_data.columns if c not in all_features]].reset_index(drop=True)\n            full_data = pd.concat([header, X], axis=1)  \n            all_features = cols.copy()\n            stock_features = None\n            time_features = None\n\n        elif PCA_SPLIT==True:\n            stock_feats = [c for c in all_features if c not in time_features]\n            market_feats = [c for c in all_features if c in time_features]\n            pca_stock = load_pkl(f'{MODEL_PATH}/pca_stock')\n            pca_market = load_pkl(f'{MODEL_PATH}/pca_market')\n            cols_stock = [f'pc_stock{x}' for x in range(pca_stock.components_.shape[0])]\n            cols_market = [f'pc_market{x}' for x in range(pca_market.components_.shape[0])]\n            X_stock = pd.DataFrame(pca_stock.transform(full_data.loc[:, stock_feats]), columns=cols_stock)\n            X_market = pd.DataFrame(pca_market.transform(full_data.loc[:, market_feats]), columns=cols_market)\n            header = full_data.loc[:, [c for c in full_data.columns if c not in all_features]].reset_index(drop=True)\n            full_data = pd.concat([header, X_stock, X_market], axis=1)\n            all_features = cols_stock + cols_market\n            stock_features = cols_stock\n            time_features = cols_market\n\n    # model prediction\n    df_pred_val = []\n    for seed in SEEDS:\n        for fold in range(N_FOLD):\n            X_val, y_val, grp_val, qid_val, header_val, target_val = get_dataset(full_data, SELECTED_FEATS, 'val')\n            model = load_pkl(f'{MODEL_PATH}/model{fold}_seed{seed}')\n            pred_val = pred_score(model, X_val)\n            df_pred_val.append(get_pred_df(header_val, pred_val, y_val, target_val, RANK_ASCENDING))\n    df_pred_val = pd.concat(df_pred_val).reset_index(drop=True)\n    df_pred_val = df_pred_val.groupby(['RowId','Date','SecuritiesCode']).mean().reset_index()\n    df_pred_val['Rank'] = df_pred_val.groupby('Date').pred_model.rank(method='first', ascending=False).astype(int) - 1\n\n    # cluster demean\n    if CLUSTER_DEMEAN:\n        df_clust = []\n        for date in df_pred_val.Date.unique():\n            stock_list = df_pred_val.loc[lambda x: x.Date==date].SecuritiesCode.tolist()\n            df_clust.append(get_stock_clust(ret, date, CLUST_N_DAY, stock_list))\n        df_clust = pd.concat(df_clust)\n        df_pred_val = df_pred_val.merge(df_clust, how='inner', on=['Date','SecuritiesCode'])\n        clust_mean = df_pred_val.groupby(['Date','clust']).pred_model.mean().reset_index().rename(columns={'pred_model':'pred_model_mean'})\n        df_pred_val = df_pred_val.merge(clust_mean, how='inner', on=['Date','clust'])\n        df_pred_val['pred_model_demean'] = df_pred_val.pred_model - df_pred_val.pred_model_mean\n        df_pred_val['Rank'] = df_pred_val.groupby('Date').pred_model_demean.rank(method='first', ascending=False).astype(int) - 1\n\n    # volatility penalty\n    if VOL_PENALTY:\n        std = ret.pivot(index='Date', columns='SecuritiesCode', values='ret') \\\n            .rolling(VOL_N_DAY).std() \\\n            .stack().reset_index() \\\n            .dropna() \\\n            .rename(columns={0:'std'}) \\\n            .assign(Date = lambda x: pd.to_datetime(x.Date))\n        df_pred_val = df_pred_val.merge(std, how='inner', on=['Date','SecuritiesCode'])\n        # identify best power and apply\n        best_p = VOL_POW\n        df_pred_val.pred_model_vol_penalty = df_pred_val.pred_model_demean / df_pred_val['std'].pow(best_p)\n        df_pred_val['Rank'] = df_pred_val.groupby('Date').pred_model_vol_penalty.rank(method='first', ascending=False).astype(int) - 1\n\n    # final submission\n    rnk = df_pred_val.set_index('SecuritiesCode').Rank\n    sample_prediction['Rank'] = sample_prediction['SecuritiesCode'].map(rnk)\n    sample_prediction['Rank'] = sample_prediction['Rank'].fillna(1000).rank(method='first').astype(int) - 1\n    env.predict(sample_prediction)","metadata":{"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"               Date  SecuritiesCode       ret\n0        2017-01-05            1301 -0.001459\n1        2017-01-05            1332 -0.005254\n2        2017-01-05            1333  0.012461\n3        2017-01-05            1376 -0.007742\n4        2017-01-05            1377 -0.007508\n...             ...             ...       ...\n2564220  2022-05-27            9990  0.051002\n2564221  2022-05-27            9991  0.008621\n2564222  2022-05-27            9993  0.008667\n2564223  2022-05-27            9994  0.025609\n2564224  2022-05-27            9997  0.004630\n\n[2564225 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date</th>\n      <th>SecuritiesCode</th>\n      <th>ret</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2017-01-05</td>\n      <td>1301</td>\n      <td>-0.001459</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2017-01-05</td>\n      <td>1332</td>\n      <td>-0.005254</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2017-01-05</td>\n      <td>1333</td>\n      <td>0.012461</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2017-01-05</td>\n      <td>1376</td>\n      <td>-0.007742</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2017-01-05</td>\n      <td>1377</td>\n      <td>-0.007508</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2564220</th>\n      <td>2022-05-27</td>\n      <td>9990</td>\n      <td>0.051002</td>\n    </tr>\n    <tr>\n      <th>2564221</th>\n      <td>2022-05-27</td>\n      <td>9991</td>\n      <td>0.008621</td>\n    </tr>\n    <tr>\n      <th>2564222</th>\n      <td>2022-05-27</td>\n      <td>9993</td>\n      <td>0.008667</td>\n    </tr>\n    <tr>\n      <th>2564223</th>\n      <td>2022-05-27</td>\n      <td>9994</td>\n      <td>0.025609</td>\n    </tr>\n    <tr>\n      <th>2564224</th>\n      <td>2022-05-27</td>\n      <td>9997</td>\n      <td>0.004630</td>\n    </tr>\n  </tbody>\n</table>\n<p>2564225 rows × 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-27T14:52:40.587461Z","iopub.execute_input":"2022-06-27T14:52:40.587837Z","iopub.status.idle":"2022-06-27T14:52:40.593639Z","shell.execute_reply.started":"2022-06-27T14:52:40.587800Z","shell.execute_reply":"2022-06-27T14:52:40.593094Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"[1, 2, 3]"},"metadata":{}}]},{"cell_type":"code","source":"pd.read_csv('../input/jpx-tokyo-stock-exchange-prediction/example_test_files/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-06-27T14:02:42.481021Z","iopub.execute_input":"2022-06-27T14:02:42.482336Z","iopub.status.idle":"2022-06-27T14:02:42.551671Z","shell.execute_reply.started":"2022-06-27T14:02:42.482267Z","shell.execute_reply":"2022-06-27T14:02:42.550773Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"              Date  SecuritiesCode  Rank\n0       2021-12-06            1301     0\n1       2021-12-06            1332     1\n2       2021-12-06            1333     2\n3       2021-12-06            1375     3\n4       2021-12-06            1376     4\n...            ...             ...   ...\n111995  2022-02-28            9990  1995\n111996  2022-02-28            9991  1996\n111997  2022-02-28            9993  1997\n111998  2022-02-28            9994  1998\n111999  2022-02-28            9997  1999\n\n[112000 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date</th>\n      <th>SecuritiesCode</th>\n      <th>Rank</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2021-12-06</td>\n      <td>1301</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2021-12-06</td>\n      <td>1332</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2021-12-06</td>\n      <td>1333</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2021-12-06</td>\n      <td>1375</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2021-12-06</td>\n      <td>1376</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>111995</th>\n      <td>2022-02-28</td>\n      <td>9990</td>\n      <td>1995</td>\n    </tr>\n    <tr>\n      <th>111996</th>\n      <td>2022-02-28</td>\n      <td>9991</td>\n      <td>1996</td>\n    </tr>\n    <tr>\n      <th>111997</th>\n      <td>2022-02-28</td>\n      <td>9993</td>\n      <td>1997</td>\n    </tr>\n    <tr>\n      <th>111998</th>\n      <td>2022-02-28</td>\n      <td>9994</td>\n      <td>1998</td>\n    </tr>\n    <tr>\n      <th>111999</th>\n      <td>2022-02-28</td>\n      <td>9997</td>\n      <td>1999</td>\n    </tr>\n  </tbody>\n</table>\n<p>112000 rows × 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"features.days_since_last_fin.hist()","metadata":{"execution":{"iopub.status.busy":"2022-06-27T16:14:12.601213Z","iopub.execute_input":"2022-06-27T16:14:12.601598Z","iopub.status.idle":"2022-06-27T16:14:12.905857Z","shell.execute_reply.started":"2022-06-27T16:14:12.601537Z","shell.execute_reply":"2022-06-27T16:14:12.904895Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"<AxesSubplot:>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]}]}