{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89bb23ae",
   "metadata": {
    "papermill": {
     "duration": 0.021961,
     "end_time": "2022-07-03T08:20:37.115524",
     "exception": false,
     "start_time": "2022-07-03T08:20:37.093563",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Notes\n",
    "- JPX_Model_LGBM_v20\n",
    "\n",
    "# To do list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70038523",
   "metadata": {
    "_cell_guid": "5ae9011f-40e6-497b-a6be-61b748ce1d2d",
    "_kg_hide-input": true,
    "_uuid": "2c45ee48-0594-488c-8a45-26a2e5510d83",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-07-03T08:20:37.161690Z",
     "iopub.status.busy": "2022-07-03T08:20:37.160525Z",
     "iopub.status.idle": "2022-07-03T08:21:11.301737Z",
     "shell.execute_reply": "2022-07-03T08:21:11.301002Z",
     "shell.execute_reply.started": "2022-07-02T11:31:22.793709Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 34.165272,
     "end_time": "2022-07-03T08:21:11.301930",
     "exception": false,
     "start_time": "2022-07-03T08:20:37.136658",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/ta-0101/ta-0.10.1-py3-none-any.whl\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from ta==0.10.1) (1.3.5)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from ta==0.10.1) (1.20.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->ta==0.10.1) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->ta==0.10.1) (2021.3)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->ta==0.10.1) (1.16.0)\r\n",
      "Installing collected packages: ta\r\n",
      "Successfully installed ta-0.10.1\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import os\n",
    "from os.path import isfile, isdir, join\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from IPython.display import display\n",
    "from zipfile import ZipFile\n",
    "import pickle\n",
    "import unicodedata\n",
    "import pytz\n",
    "from joblib import Parallel, delayed\n",
    "import shutil\n",
    "import difflib\n",
    "import random\n",
    "import math\n",
    "from shutil import copyfile\n",
    "import itertools\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import collections\n",
    "from collections import deque\n",
    "import gc\n",
    "import seaborn as sns\n",
    "import scipy.cluster.hierarchy as spc\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import ndcg_score, accuracy_score\n",
    "\n",
    "import lightgbm as lgbm\n",
    "import optuna\n",
    "from optuna import Trial, visualization\n",
    "\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import jpx_tokyo_market_prediction\n",
    "\n",
    "from utility_script import *\n",
    "\n",
    "!pip install ../input/ta-0101/ta-0.10.1-py3-none-any.whl\n",
    "import ta\n",
    "from ta import add_all_ta_features\n",
    "from ta.utils import dropna\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9a3ead9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-03T08:21:11.352601Z",
     "iopub.status.busy": "2022-07-03T08:21:11.351953Z",
     "iopub.status.idle": "2022-07-03T08:21:11.357122Z",
     "shell.execute_reply": "2022-07-03T08:21:11.357607Z",
     "shell.execute_reply.started": "2022-07-02T11:31:52.313941Z"
    },
    "papermill": {
     "duration": 0.031716,
     "end_time": "2022-07-03T08:21:11.357789",
     "exception": false,
     "start_time": "2022-07-03T08:21:11.326073",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# general params\n",
    "FE = 'orig'\n",
    "MODEL = 'clf3'\n",
    "FE_PATH = '../input/jpx-feature-engineering-v15'\n",
    "MODEL_PATH = '../input/jpx-model-lgbm-v20'\n",
    "JPX_PATH = '../input/jpx-tokyo-stock-exchange-prediction'\n",
    "EXPORT = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5217029b",
   "metadata": {
    "_cell_guid": "c0f0bfa1-3dea-43a3-b2f1-8a2c268d6339",
    "_uuid": "c3cfb2dc-8437-441f-b2c0-88b3d4a77281",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-07-03T08:21:11.406369Z",
     "iopub.status.busy": "2022-07-03T08:21:11.405702Z",
     "iopub.status.idle": "2022-07-03T08:21:11.410911Z",
     "shell.execute_reply": "2022-07-03T08:21:11.411430Z",
     "shell.execute_reply.started": "2022-07-02T11:31:52.321211Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.03107,
     "end_time": "2022-07-03T08:21:11.411599",
     "exception": false,
     "start_time": "2022-07-03T08:21:11.380529",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "FE Parameters\n",
    "'''\n",
    "SEED = 0\n",
    "LAGS = {'1d':1, '3d':3, '1w':5, '1m':20, '3m':20*3, '6m':20*6, '12m':20*12}\n",
    "MAX_DAYS_LAG = max(list(LAGS.values()))\n",
    "WIN_SIZE = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba853618",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-03T08:21:11.461280Z",
     "iopub.status.busy": "2022-07-03T08:21:11.460586Z",
     "iopub.status.idle": "2022-07-03T08:21:11.466568Z",
     "shell.execute_reply": "2022-07-03T08:21:11.467095Z",
     "shell.execute_reply.started": "2022-07-02T11:31:52.33734Z"
    },
    "papermill": {
     "duration": 0.032894,
     "end_time": "2022-07-03T08:21:11.467269",
     "exception": false,
     "start_time": "2022-07-03T08:21:11.434375",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Model Params\n",
    "'''\n",
    "# basic\n",
    "SEED = 0\n",
    "SEEDS = [1,2,3,4,5]\n",
    "\n",
    "# feature composition\n",
    "DROP_MARKET_FEATS = False\n",
    "\n",
    "# PCA\n",
    "RUN_PCA = True\n",
    "PCA_SPLIT = False\n",
    "\n",
    "# target definition\n",
    "RANK_ASCENDING = False # set this to False if model prediction is same direction of Target\n",
    "TARGET_POW = 0\n",
    "\n",
    "# data split\n",
    "N_FOLD = 5\n",
    "\n",
    "# final features\n",
    "SELECTED_FEATS = 'pc35, pc27, pc12, pc8, pc16, pc6, pc4'.split(', ')\n",
    "\n",
    "# optimization\n",
    "CLUSTER_DEMEAN = True\n",
    "CLUST_N_DAY = 60\n",
    "VOL_PENALTY = True\n",
    "VOL_N_DAY = 60\n",
    "VOL_POW = 1.5789473684210527"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5df99e",
   "metadata": {
    "papermill": {
     "duration": 0.022104,
     "end_time": "2022-07-03T08:21:11.511760",
     "exception": false,
     "start_time": "2022-07-03T08:21:11.489656",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "910cd09f",
   "metadata": {
    "_cell_guid": "6a3c94c5-9152-4477-9b79-dec0b881fd0e",
    "_uuid": "54a20792-c5bc-4ee2-8162-07364e311d9c",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-07-03T08:21:11.559926Z",
     "iopub.status.busy": "2022-07-03T08:21:11.559264Z",
     "iopub.status.idle": "2022-07-03T08:21:11.578859Z",
     "shell.execute_reply": "2022-07-03T08:21:11.579419Z",
     "shell.execute_reply.started": "2022-07-02T11:31:52.348716Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.045459,
     "end_time": "2022-07-03T08:21:11.579598",
     "exception": false,
     "start_time": "2022-07-03T08:21:11.534139",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class JPXData:\n",
    "    def __init__(self, window_size, df_names):\n",
    "        self.size = 0\n",
    "        self.window_size = window_size\n",
    "        self.df_names = df_names\n",
    "        self.num_df = len(df_names)\n",
    "        self.data = {df_name : pd.DataFrame() for df_name in df_names}\n",
    "        self.row_counts = {df_name : [] for df_name in df_names}\n",
    "        self.dates = []\n",
    "        self.first_date, self.last_date = None, None\n",
    "        self.features = []\n",
    "        self.curr_features = None\n",
    "        self.n_day_hist = 0\n",
    "        self.init_folders()\n",
    "        \n",
    "    def init_folders(self):\n",
    "        shutil.rmtree(path='./features', ignore_errors=True)\n",
    "        os.mkdir('./features')\n",
    "        \n",
    "    def append_data(self):\n",
    "        self.features.append(self.curr_features)\n",
    "        self.n_day_hist += 1\n",
    "        \n",
    "    def archive_data(self):\n",
    "        save_pkl(self.features, f'./features/features_{self.n_day_hist}')\n",
    "        self.clear_hist()\n",
    "        \n",
    "    def clear_hist(self):\n",
    "        self.features = []\n",
    "        \n",
    "    def push_forward(self, new_data, append, last):\n",
    "        # assign names to new data assuming the same as df_names\n",
    "        new_data = dict(zip(self.df_names, new_data))\n",
    "        # case when no enough data\n",
    "        if self.size < self.window_size:\n",
    "            for df_name in self.df_names:\n",
    "                self.data[df_name] = pd.concat([self.data[df_name], new_data[df_name]]).reset_index(drop=True)\n",
    "                self.row_counts[df_name] = self.row_counts[df_name] + [new_data[df_name].shape[0]]\n",
    "            self.dates = self.dates + [new_data[self.df_names[0]].Date.iloc[0]] \n",
    "            self.size += 1\n",
    "        # general case (shift by 1 day)\n",
    "        else:\n",
    "            for df_name in self.df_names:\n",
    "                self.data[df_name] = pd.concat([self.data[df_name].iloc[self.row_counts[df_name][0]:], new_data[df_name]]).reset_index(drop=True)\n",
    "                self.row_counts[df_name] = self.row_counts[df_name][1:] + [new_data[df_name].shape[0]]\n",
    "            self.dates = self.dates[1:] + [new_data[self.df_names[0]].Date.iloc[0]]  \n",
    "        # update date range\n",
    "        self.first_date, self.last_date = self.dates[0], self.dates[-1]\n",
    "        # generate features\n",
    "        if self.size == self.window_size:\n",
    "            self.curr_features = get_features(self.data)\n",
    "            if append==True:\n",
    "                self.append_data()\n",
    "                if (self.n_day_hist%20 == 0 and self.n_day_hist > 0) or last==True:\n",
    "                    self.archive_data()\n",
    "        log(f'Pushed to latest date: {self.last_date}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cf17cc9",
   "metadata": {
    "_cell_guid": "ae8fcff3-07c3-4ef5-ade3-61d6dc34243b",
    "_uuid": "c4fccf97-02d7-4f36-a5fa-53efcfd2edc6",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-07-03T08:21:11.633884Z",
     "iopub.status.busy": "2022-07-03T08:21:11.633148Z",
     "iopub.status.idle": "2022-07-03T08:21:11.643573Z",
     "shell.execute_reply": "2022-07-03T08:21:11.644099Z",
     "shell.execute_reply.started": "2022-07-02T11:31:52.370014Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.041946,
     "end_time": "2022-07-03T08:21:11.644274",
     "exception": false,
     "start_time": "2022-07-03T08:21:11.602328",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def standard_dist(s, lag):\n",
    "    tail_data = s.tail(LAGS[lag])\n",
    "    return (s.iloc[-1] - tail_data.mean()) / tail_data.std()\n",
    "\n",
    "def ma_pctg_ch(s, lag):\n",
    "    return s.iloc[-1] / s.tail(LAGS[lag]).mean() - 1\n",
    "\n",
    "def sharpe(s, lag):\n",
    "    tail_data = s.tail(LAGS[lag])\n",
    "    std = tail_data.std()\n",
    "    if std > 0:\n",
    "        sharpe_ratio = tail_data.mean() / tail_data.std()\n",
    "    else:\n",
    "        sharpe_ratio = 0\n",
    "    return sharpe_ratio\n",
    "\n",
    "def gen_ta_feats(df, n_day_ma, n_day_scale):\n",
    "    # preprocess\n",
    "    df = df \\\n",
    "        .sort_values(['SecuritiesCode','Date']) \\\n",
    "        .groupby('SecuritiesCode') \\\n",
    "        .tail(n_day_ma + n_day_scale) \\\n",
    "        .loc[:, ['SecuritiesCode','Open','High','Low','Close','Volume']] \\\n",
    "        .reset_index(drop=True)\n",
    "    # gen TA feats\n",
    "    df = ta.add_all_ta_features(df, \"Open\", \"High\", \"Low\", \"Close\", \"Volume\", fillna=False) \\\n",
    "        .drop(['Open','High','Low','Close','Volume'], axis=1)\n",
    "    ta_cols = df.columns.tolist()[1:]\n",
    "    ta_cols = [c for c in ta_cols if c not in ['volatility_atr',\n",
    "                                                 'trend_adx',\n",
    "                                                 'trend_adx_pos',\n",
    "                                                 'trend_adx_neg',\n",
    "                                                 'trend_psar_up',\n",
    "                                                 'trend_psar_down',\n",
    "                                                 'momentum_kama']]\n",
    "    ta_cols = ['volume_obv',\n",
    "                 'volume_cmf',\n",
    "                 'volume_fi',\n",
    "                 'volume_em',\n",
    "                 'volume_sma_em',\n",
    "                 'volume_vpt',\n",
    "                 'volume_mfi',\n",
    "                 'volume_nvi',\n",
    "                 'volatility_bbw',\n",
    "                 'volatility_bbhi',\n",
    "                 'volatility_bbli',\n",
    "                 'volatility_kcw',\n",
    "                 'volatility_kcp',\n",
    "                 'volatility_kchi',\n",
    "                 'volatility_kcli',\n",
    "                 'volatility_ui',\n",
    "                 'trend_macd_signal',\n",
    "                 'trend_macd_diff',\n",
    "                 'trend_vortex_ind_pos',\n",
    "                 'trend_vortex_ind_neg',\n",
    "                 'trend_mass_index',\n",
    "                 'trend_dpo',\n",
    "                 'trend_kst',\n",
    "                 'trend_kst_diff',\n",
    "                 'trend_ichimoku_conv',\n",
    "                 'trend_stc',\n",
    "                 'trend_cci',\n",
    "                 'trend_visual_ichimoku_b',\n",
    "                 'trend_aroon_up',\n",
    "                 'trend_aroon_down',\n",
    "                 'trend_psar_up_indicator',\n",
    "                 'trend_psar_down_indicator',\n",
    "                 'momentum_rsi',\n",
    "                 'momentum_stoch_rsi_d',\n",
    "                 'momentum_uo',\n",
    "                 'momentum_wr',\n",
    "                 'momentum_ao',\n",
    "                 'momentum_roc',\n",
    "                 'momentum_ppo',\n",
    "                 'momentum_ppo_hist',\n",
    "                 'momentum_pvo',\n",
    "                 'momentum_pvo_hist',\n",
    "                 'others_dlr']\n",
    "    df = df[['SecuritiesCode'] + ta_cols]\n",
    "    # scale by mean\n",
    "    mean = df \\\n",
    "        .groupby('SecuritiesCode') \\\n",
    "        .tail(n_day_scale)[ta_cols] \\\n",
    "        .abs() \\\n",
    "        .mean()\n",
    "    for c in ta_cols:\n",
    "        df[c] = df[c] / mean[c]\n",
    "    # take last row as features\n",
    "    df = df.groupby('SecuritiesCode').tail(1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d0af71c",
   "metadata": {
    "_cell_guid": "4a637337-7541-4b8f-bdff-7aca913a0aef",
    "_uuid": "18f34c4c-eadc-4ab7-bbaf-ee1753588b7f",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-07-03T08:21:11.694351Z",
     "iopub.status.busy": "2022-07-03T08:21:11.693492Z",
     "iopub.status.idle": "2022-07-03T08:21:11.739383Z",
     "shell.execute_reply": "2022-07-03T08:21:11.740423Z",
     "shell.execute_reply.started": "2022-07-02T11:31:52.391338Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.073608,
     "end_time": "2022-07-03T08:21:11.740779",
     "exception": false,
     "start_time": "2022-07-03T08:21:11.667171",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7 µs, sys: 0 ns, total: 7 µs\n",
      "Wall time: 11.4 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if FE=='orig':\n",
    "    def get_features(data):\n",
    "        df_prices, df_sec_prices, df_fins, df_opts, df_trades = tuple(data.values())\n",
    "\n",
    "        # base table\n",
    "        features = df_prices \\\n",
    "            .loc[lambda x: x.Date==x.Date.iloc[-1]] \\\n",
    "            .loc[:, ['RowId','Date','SecuritiesCode']] \\\n",
    "            .drop_duplicates(subset='RowId') \\\n",
    "            .reset_index(drop=True)\n",
    "\n",
    "        '''\n",
    "        Major stock prices features\n",
    "        '''\n",
    "        # precalculate new columns\n",
    "        cols = [c for c in df_prices.columns.tolist()[3:] if c!='ExpectedDividend']\n",
    "        df_prices[cols] = df_prices.groupby('SecuritiesCode')[cols].ffill()\n",
    "        df_prices['ret'] = df_prices.groupby('SecuritiesCode').Close.pct_change()\n",
    "        ret_mkt = df_prices.groupby('Date').ret.mean()\n",
    "        var_mkt = (ret_mkt**2).tail(LAGS['12m']).sum()\n",
    "        df_prices['ret_mkt'] = df_prices.Date.map(ret_mkt)\n",
    "        df_prices['spread'] = df_prices['High'] - df_prices['Low']\n",
    "        df_prices['div_ratio'] = df_prices['ExpectedDividend'].fillna(0) / df_prices['Close']\n",
    "        df_prices['dollar_traded'] = np.log(df_prices.Volume * (df_prices.Open + df_prices.Close)/2 + 1)\n",
    "        df_prices['RS_sqrt_vol'] = np.sqrt(np.log(df_prices['High']/df_prices['Close'])*np.log(df_prices['High']/df_prices['Open']) + np.log(df_prices['Low']/df_prices['Close'])*np.log(df_prices['Low']/df_prices['Open']))\n",
    "        df_prices['num_div'] = df_prices.groupby('SecuritiesCode').ExpectedDividend.apply(lambda s: s.notnull().astype(int).cumsum())\n",
    "        df_prices['first_div'] = ((df_prices.num_div==1) & (df_prices.num_div.shift(1)==0)).astype(int)\n",
    "        # previous day return\n",
    "        features['ret'] = features.SecuritiesCode.map(df_prices.groupby('SecuritiesCode').ret.last())\n",
    "        # Change in Close price\n",
    "        for lag in ['3d','1w']:\n",
    "            features[f'price_ma_pctg_ch_{lag}'] = features.SecuritiesCode.map(df_prices.groupby('SecuritiesCode').Close.apply(lambda s: ma_pctg_ch(s, lag)))\n",
    "        for lag in ['1m','3m','6m','12m']:\n",
    "            features[f'price_standard_dist_{lag}'] = features.SecuritiesCode.map(df_prices.groupby('SecuritiesCode').Close.apply(lambda s: standard_dist(s, lag)))\n",
    "        # Change in volume\n",
    "        for lag in ['3d','1w']:\n",
    "            features[f'volume_ma_pctg_ch_{lag}'] = features.SecuritiesCode.map(df_prices.groupby('SecuritiesCode').Volume.apply(lambda s: ma_pctg_ch(s, lag)))\n",
    "        for lag in ['1m','3m','6m','12m']:\n",
    "            features[f'volume_standard_dist_{lag}'] = features.SecuritiesCode.map(df_prices.groupby('SecuritiesCode').Volume.apply(lambda s: standard_dist(s, lag)))\n",
    "        # daily spread\n",
    "        for lag in ['3d','1w']:\n",
    "            features[f'spread_ma_pctg_ch_{lag}'] = features.SecuritiesCode.map(df_prices.groupby('SecuritiesCode').spread.apply(lambda s: ma_pctg_ch(s, lag)))\n",
    "        for lag in ['1m']:\n",
    "            features[f'spread_standard_dist_{lag}'] = features.SecuritiesCode.map(df_prices.groupby('SecuritiesCode').spread.apply(lambda s: standard_dist(s, lag)))\n",
    "        # volatility\n",
    "        for lag in ['1w','1m','3m','12m']:\n",
    "            features[f'volatility_{lag}'] = features.SecuritiesCode.map(df_prices.groupby('SecuritiesCode').ret.apply(lambda s: s.tail(LAGS[lag]).std())) \n",
    "        # change in volatility\n",
    "        features['volatility_diff'] = features['volatility_1w'] - features['volatility_1m']\n",
    "        # market return and volatility\n",
    "        for lag in ['3d','1w','1m','3m']:\n",
    "            features[f'ret_mkt_{lag}'] = ret_mkt.tail(LAGS[lag]).sum()\n",
    "            features[f'vol_mkt_{lag}'] = ret_mkt.tail(LAGS[lag]).std()\n",
    "        # beta\n",
    "        df_prices['beta'] = df_prices.SecuritiesCode.map(df_prices.groupby('SecuritiesCode').apply(lambda df: (df.set_index('Date').ret * ret_mkt).tail(LAGS['12m']).sum() / var_mkt))\n",
    "        features['beta'] = features.SecuritiesCode.map(df_prices.groupby('SecuritiesCode')['beta'].last())\n",
    "        # excess return\n",
    "        df_prices['exret'] = df_prices['ret'] - df_prices['beta'] * df_prices['ret_mkt']\n",
    "        for lag in ['3d','1w','1m','3m']:\n",
    "            features[f'exret_{lag}'] = features.SecuritiesCode.map(df_prices.groupby('SecuritiesCode')['exret'].apply(lambda s: s.tail(LAGS[lag]).sum()))\n",
    "        # div ratio\n",
    "        features['div_ratio'] = features.SecuritiesCode.map(df_prices.groupby('SecuritiesCode').apply(lambda df: df.ExpectedDividend.fillna(0).iloc[-1] / df.Close.tail(LAGS['1m']).mean()))\n",
    "        # change in dollar value traded\n",
    "        for lag in ['1w','1m']:\n",
    "            features[f'dollar_standard_dist_{lag}'] = features.SecuritiesCode.map(df_prices.groupby('SecuritiesCode').dollar_traded.apply(lambda s: standard_dist(s, lag)))\n",
    "        # RS_sqrt_vol\n",
    "        features['RS_sqrt_vol'] = features.SecuritiesCode.map(df_prices.groupby('SecuritiesCode').RS_sqrt_vol.last())\n",
    "        # sharpe\n",
    "        for lag in ['1m','3m']:\n",
    "            features[f'sharpe_{lag}'] = features.SecuritiesCode.map(df_prices.groupby('SecuritiesCode').ret.apply(lambda s: sharpe(s, lag)))\n",
    "        # days since last dividend\n",
    "        features['days_since_last_div'] = features.SecuritiesCode.map((df_prices.Date.iloc[-1] - df_prices.loc[lambda x: x.ExpectedDividend.notnull()].groupby('SecuritiesCode').Date.last()) / np.timedelta64(1,'D'))\n",
    "        # initiate dividend\n",
    "        features['first_div'] = features.SecuritiesCode.map(df_prices.groupby('SecuritiesCode').first_div.last())\n",
    "        # AdjustmentFactor\n",
    "        features['AdjustmentFactor'] = features.SecuritiesCode.map(df_prices.groupby('SecuritiesCode').AdjustmentFactor.apply(lambda s: (s!=1).astype(int).iloc[-1]))\n",
    "\n",
    "\n",
    "        '''\n",
    "        Secondary stock prices features\n",
    "        '''\n",
    "        # precalculate new columns\n",
    "        df_sec_prices['ret'] = df_sec_prices.groupby('SecuritiesCode').Close.pct_change()\n",
    "        df_sec_prices['dollar_traded'] = np.log(df_sec_prices.Volume * (df_sec_prices.Open + df_sec_prices.Close)/2 + 1)\n",
    "        # cross-sectional return & volatility\n",
    "        for n in [1,3]:\n",
    "            features[f'sec_cross_sect_ret_{n}'] = df_sec_prices.groupby('SecuritiesCode').ret.apply(lambda s: s.tail(n).sum()).mean()\n",
    "            features[f'sec_cross_sect_vol_{n}'] = df_sec_prices.groupby('SecuritiesCode').ret.apply(lambda s: s.tail(n).sum()).std()\n",
    "        # Change in volume\n",
    "        for lag in ['3d','1w']:\n",
    "            features[f'sec_volume_ma_pctg_ch_{lag}'] = df_sec_prices.groupby('SecuritiesCode').Volume.apply(lambda s: ma_pctg_ch(s, lag)).mean()\n",
    "        # volatility\n",
    "        for lag in ['1w','1m','3m']:\n",
    "            features[f'sec_volatility_{lag}'] = df_sec_prices.groupby('SecuritiesCode').ret.apply(lambda s: s.tail(LAGS[lag]).std()).mean()\n",
    "        # change in volatility\n",
    "        features['sec_volatility_diff'] = features['sec_volatility_1w'] - features['sec_volatility_1m']\n",
    "        # change in dollar value traded\n",
    "        for lag in ['1w','1m']:\n",
    "            features[f'sec_dollar_standard_dist_{lag}'] = df_sec_prices.groupby('SecuritiesCode').dollar_traded.apply(lambda s: standard_dist(s, lag)).mean()\n",
    "\n",
    "        '''\n",
    "        Time phase features\n",
    "        '''\n",
    "        # day in week\n",
    "        day_in_week_angle = (features.Date.dt.weekday / 5 * 2 * np.pi).iloc[-1]\n",
    "        features['day_in_week_sin'] = np.sin(day_in_week_angle)\n",
    "        features['day_in_week_cos'] = np.cos(day_in_week_angle)\n",
    "        # day in month\n",
    "        day_in_month_angle = ((features.Date.dt.day - 1) / 31 * 2 * np.pi).iloc[-1]\n",
    "        features['day_in_month_sin'] = np.sin(day_in_month_angle)\n",
    "        features['day_in_month_cos'] = np.cos(day_in_month_angle)\n",
    "        # week in year\n",
    "        week_in_year_angle = ((features.Date.dt.week - 1) / 52 * 2 * np.pi).iloc[-1]\n",
    "        features['week_in_year_sin'] = np.sin(week_in_year_angle)\n",
    "        features['week_in_year_cos'] = np.cos(week_in_year_angle)\n",
    "\n",
    "\n",
    "        '''\n",
    "        Financials features\n",
    "        '''\n",
    "        # convert string to numbers\n",
    "        fin_cols = ['NetSales','OperatingProfit','OrdinaryProfit','Profit','EarningsPerShare','TotalAssets','Equity','EquityToAssetRatio','BookValuePerShare',\n",
    "                    'ForecastNetSales','ForecastOperatingProfit','ForecastOrdinaryProfit','ForecastProfit','ForecastEarningsPerShare']\n",
    "        # clean numeric values\n",
    "        df_fins[fin_cols] = df_fins[fin_cols].replace('－',np.nan).astype(float)\n",
    "        # quarter forward fill\n",
    "        df_fins = df_fins.sort_values(['SecuritiesCode','TypeOfCurrentPeriod','Date']).reset_index(drop=True)\n",
    "        df_fins[fin_cols] = df_fins.groupby(['SecuritiesCode','TypeOfCurrentPeriod'])[fin_cols].ffill()\n",
    "        # overall forward fill\n",
    "        df_fins = df_fins.sort_values(['SecuritiesCode','Date']).reset_index(drop=True)\n",
    "        df_fins[fin_cols] = df_fins.groupby('SecuritiesCode')[fin_cols].ffill()\n",
    "        # drop invalid rows\n",
    "        df_fins = df_fins \\\n",
    "            .loc[lambda x: x.NetSales > 0] \\\n",
    "            .sort_values(['SecuritiesCode','Date']) \\\n",
    "            .reset_index(drop=True)\n",
    "        # define columns\n",
    "        df_fins['Close'] = df_fins.SecuritiesCode.map(df_prices.groupby('SecuritiesCode').Close.last())\n",
    "        df_fins['SalesToEquityRatio'] = df_fins['NetSales'] / df_fins['Equity']\n",
    "        df_fins['BookToMarketRatio'] = df_fins['Equity'] / df_fins['Close']\n",
    "        df_fins['ProfitoMarketRatio'] = df_fins['OperatingProfit'] / df_fins['Close']\n",
    "        df_fins['EarningToPriceRatio'] = df_fins['EarningsPerShare'] / df_fins['Close']\n",
    "        fin_cols_static = ['NetSales','OperatingProfit','OrdinaryProfit','Profit','EarningsPerShare','TotalAssets','Equity','BookValuePerShare',\n",
    "                            'ForecastNetSales','ForecastOperatingProfit','ForecastOrdinaryProfit','ForecastProfit','ForecastEarningsPerShare']\n",
    "        fin_cols_ratio = ['EquityToAssetRatio','SalesToEquityRatio','BookToMarketRatio','ProfitoMarketRatio','EarningToPriceRatio']\n",
    "        fin_cols = fin_cols_static + fin_cols_ratio\n",
    "        # fins feature calculation\n",
    "        df1 = df_fins.sort_values(['SecuritiesCode','TypeOfCurrentPeriod','Date']).reset_index(drop=True).groupby(['SecuritiesCode','TypeOfCurrentPeriod'])[fin_cols].nth(-1)\n",
    "        df2 = df_fins.sort_values(['SecuritiesCode','TypeOfCurrentPeriod','Date']).reset_index(drop=True).groupby(['SecuritiesCode','TypeOfCurrentPeriod'])[fin_cols].nth(-2)\n",
    "        df = df1.merge(df2, how='left', left_index=True, right_index=True)\n",
    "        for c in fin_cols:\n",
    "            if c in fin_cols_static:\n",
    "                df[f'{c}_pctg'] = (df[f'{c}_x'] - df[f'{c}_y']) / df[f'{c}_y'].abs()\n",
    "            elif c in fin_cols_ratio:\n",
    "                df[f'{c}_raw'] = df[f'{c}_x']\n",
    "                df[f'{c}_diff'] = df[f'{c}_x'] - df[f'{c}_y']\n",
    "        df = df.drop([c for c in df if c[-2:] in ['_x','_y']], axis=1).reset_index()\n",
    "        feats_fins = df_fins.sort_values(['SecuritiesCode','Date']).groupby('SecuritiesCode').last()['TypeOfCurrentPeriod'].reset_index()\n",
    "        feats_fins = feats_fins.merge(df, how='left', on=['SecuritiesCode','TypeOfCurrentPeriod']).drop('TypeOfCurrentPeriod', axis=1)\n",
    "        features = features.merge(feats_fins, how='left', on='SecuritiesCode')\n",
    "        # num days since last announcement\n",
    "        features['days_since_last_fin'] = (features.Date - features.SecuritiesCode.map(df_fins.groupby('SecuritiesCode').Date.last())) / np.timedelta64(1,'D')\n",
    "\n",
    "\n",
    "        '''\n",
    "        Post-processing\n",
    "        '''\n",
    "        cols = [c for c in features.columns if c not in ['RowId','Date','SecuritiesCode']]\n",
    "        features[cols] = features[cols].replace(np.inf, np.nan).replace(-np.inf, np.nan)\n",
    "        features[cols] = features[cols].fillna(features[cols].mean())\n",
    "        features[cols] = features[cols].astype(np.float32)\n",
    "\n",
    "        return features\n",
    "\n",
    "    \n",
    "elif FE=='ta':\n",
    "\n",
    "    def get_features(data):\n",
    "        df_prices, df_sec_prices, df_fins, df_opts, df_trades = tuple(data.values())\n",
    "    #     df_prices, df_sec_prices, df_fins, df_opts, df_trades = tuple(data.data.values())\n",
    "\n",
    "        # base table\n",
    "        features = df_prices.loc[lambda x: x.Date==x.Date.iloc[-1]][['RowId','Date','SecuritiesCode']]\n",
    "\n",
    "        # precalculate new columns\n",
    "        cols = [c for c in df_prices.columns.tolist()[3:] if c!='ExpectedDividend']\n",
    "        df_prices[cols] = df_prices.groupby('SecuritiesCode')[cols].ffill()\n",
    "        df_prices['ret'] = df_prices.groupby('SecuritiesCode').Close.pct_change()\n",
    "        ret_mkt = df_prices.groupby('Date').ret.mean()\n",
    "\n",
    "        # market return and volatility\n",
    "        for lag in ['3d','1w','1m','3m']:\n",
    "            features[f'ret_mkt_{lag}'] = ret_mkt.tail(LAGS[lag]).sum()\n",
    "            features[f'vol_mkt_{lag}'] = ret_mkt.tail(LAGS[lag]).std()\n",
    "\n",
    "        # TA feats\n",
    "        ta_feats = gen_ta_feats(df=df_prices, n_day_ma=52, n_day_scale=20)\n",
    "        features = features.merge(ta_feats, how='inner', on='SecuritiesCode')\n",
    "\n",
    "        '''\n",
    "        Post-processing\n",
    "        '''\n",
    "        cols = [c for c in features.columns if c not in ['RowId','Date','SecuritiesCode']]\n",
    "        features[cols] = features[cols].replace(np.inf, np.nan).replace(-np.inf, np.nan)\n",
    "        features[cols] = features[cols].fillna(features[cols].mean())\n",
    "        features[cols] = features[cols].astype(np.float32)\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf4e2c4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-03T08:21:11.790757Z",
     "iopub.status.busy": "2022-07-03T08:21:11.790101Z",
     "iopub.status.idle": "2022-07-03T08:21:11.800495Z",
     "shell.execute_reply": "2022-07-03T08:21:11.801541Z",
     "shell.execute_reply.started": "2022-07-02T11:31:52.450004Z"
    },
    "papermill": {
     "duration": 0.037311,
     "end_time": "2022-07-03T08:21:11.801904",
     "exception": false,
     "start_time": "2022-07-03T08:21:11.764593",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 0 ns, total: 5 µs\n",
      "Wall time: 8.58 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def update_data(test_start_date):\n",
    "    # load train + sup data\n",
    "    df_prices = pd.concat([pd.read_csv(f'{JPX_PATH}/{folder}/stock_prices.csv', parse_dates=['Date']) for folder in ['train_files','supplemental_files']]).reset_index(drop=True)\n",
    "    df_sec_prices = pd.concat([pd.read_csv(f'{JPX_PATH}/{folder}/secondary_stock_prices.csv', parse_dates=['Date']) for folder in ['train_files','supplemental_files']]).reset_index(drop=True)\n",
    "    df_fins = pd.concat([pd.read_csv(f'{JPX_PATH}/{folder}/financials.csv', parse_dates=['Date']) for folder in ['train_files','supplemental_files']]).reset_index(drop=True)\n",
    "    df_opts = pd.concat([pd.read_csv(f'{JPX_PATH}/{folder}/options.csv', parse_dates=['Date']) for folder in ['train_files','supplemental_files']]).reset_index(drop=True)\n",
    "    df_trades = pd.concat([pd.read_csv(f'{JPX_PATH}/{folder}/trades.csv', parse_dates=['Date']) for folder in ['train_files','supplemental_files']]).reset_index(drop=True)\n",
    "\n",
    "    # identify missing dates\n",
    "    if test_start_date.astype('datetime64[Y]').astype(int) + 1970 == 2021:\n",
    "        fe_end_date = np.datetime64('2021-10-27')\n",
    "#         fe_end_date = np.datetime64('2021-12-01')\n",
    "    else:\n",
    "        fe_end_date = data.last_date\n",
    "    extra_dates = df_prices.Date.drop_duplicates().loc[lambda x: (x > fe_end_date) & (x < test_start_date)].tolist()\n",
    "\n",
    "    # FE for missing dates\n",
    "    for i in range(len(extra_dates)):\n",
    "        data.push_forward([df.loc[lambda x: x.Date==extra_dates[i]] for df in [df_prices, df_sec_prices, df_fins, df_opts, df_trades]], append=False, last=False)\n",
    "    \n",
    "    # release memory\n",
    "    del df_prices, df_sec_prices, df_fins, df_opts, df_trades\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dfbfe5",
   "metadata": {
    "papermill": {
     "duration": 0.022891,
     "end_time": "2022-07-03T08:21:11.849354",
     "exception": false,
     "start_time": "2022-07-03T08:21:11.826463",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b59561f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-03T08:21:11.901549Z",
     "iopub.status.busy": "2022-07-03T08:21:11.900770Z",
     "iopub.status.idle": "2022-07-03T08:21:11.912620Z",
     "shell.execute_reply": "2022-07-03T08:21:11.913177Z",
     "shell.execute_reply.started": "2022-07-02T11:31:52.462507Z"
    },
    "papermill": {
     "duration": 0.039,
     "end_time": "2022-07-03T08:21:11.913356",
     "exception": false,
     "start_time": "2022-07-03T08:21:11.874356",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Function to get sub-columns from table for model fitting\n",
    "'''\n",
    "def get_dataset(df, selected_feats, trn_val):\n",
    "    if trn_val=='val':\n",
    "        df = df.groupby('Date').sample(frac=1.0, random_state=SEED)\n",
    "    df = df.reset_index(drop=True)\n",
    "    grp = df.groupby('Date').size().tolist()\n",
    "    qid = df['Date']\n",
    "    X = df[selected_feats]\n",
    "    y = df['target_train']\n",
    "    target = df['Target']\n",
    "    header = df[id_cols]\n",
    "    return X, y, grp, qid, header, target\n",
    "\n",
    "'''\n",
    "Function to predict scores within groups\n",
    "'''\n",
    "def pred_score(model, X):\n",
    "    if MODEL=='reg':\n",
    "        return pd.Series(model.predict(X))\n",
    "    elif MODEL=='clf':\n",
    "        return pd.Series(model.predict_proba(X)[:,1])\n",
    "    elif MODEL=='clf3':\n",
    "        return pd.Series((model.predict_proba(X) * [-1,0,1]).sum(axis=1))\n",
    "\n",
    "'''\n",
    "Function to transform model output to rank prediction table\n",
    "'''\n",
    "def get_pred_df(header, pred_model, y_true_train, y_true, rank_ascending):\n",
    "    df_pred = pd.concat([header[['RowId','Date','SecuritiesCode']].assign(SecuritiesCode=lambda x: x.SecuritiesCode.astype(int)).reset_index(drop=True),\n",
    "                        pd.Series(pred_model).rename('pred_model').reset_index(drop=True),\n",
    "                        y_true_train.reset_index(drop=True),\n",
    "                        y_true.reset_index(drop=True)\n",
    "                        ], axis=1)\n",
    "    df_pred['Rank'] = df_pred.groupby('Date').pred_model.rank(method='first', ascending=rank_ascending).astype(int) - 1\n",
    "    return df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e6a3730",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-03T08:21:11.963377Z",
     "iopub.status.busy": "2022-07-03T08:21:11.962692Z",
     "iopub.status.idle": "2022-07-03T08:21:11.970824Z",
     "shell.execute_reply": "2022-07-03T08:21:11.971308Z",
     "shell.execute_reply.started": "2022-07-02T11:31:52.481565Z"
    },
    "papermill": {
     "duration": 0.034817,
     "end_time": "2022-07-03T08:21:11.971488",
     "exception": false,
     "start_time": "2022-07-03T08:21:11.936671",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_stock_clust(ret, date, n_day, stock_list):\n",
    "    # raw correlation table\n",
    "    corr = ret.loc[lambda x: pd.to_datetime(x.Date)<=date].pivot(index='Date', columns='SecuritiesCode', values='ret').tail(n_day).corr()\n",
    "    corr = corr.reindex(index=stock_list, columns=stock_list)\n",
    "    cols = [c for c in corr if corr[c].notnull().sum()==0]\n",
    "    corr = corr.drop(cols, axis=0).drop(cols, axis=1)\n",
    "    stocks = corr.columns.tolist()\n",
    "\n",
    "    # clustering\n",
    "    pdist = spc.distance.pdist(corr.values)\n",
    "    linkage = spc.linkage(pdist, method='complete')\n",
    "    idx = spc.fcluster(linkage, 0.5 * pdist.max(), 'distance')\n",
    "    stock_corr_clust = pd.DataFrame({'SecuritiesCode':stocks, 'clust':idx}).assign(Date=date).sort_values('clust').reset_index(drop=True)\n",
    "    return stock_corr_clust"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecb8f6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-27T14:24:06.821508Z",
     "iopub.status.busy": "2022-06-27T14:24:06.821286Z",
     "iopub.status.idle": "2022-06-27T14:24:14.792169Z",
     "shell.execute_reply": "2022-06-27T14:24:14.791105Z",
     "shell.execute_reply.started": "2022-06-27T14:24:06.821481Z"
    },
    "papermill": {
     "duration": 0.023278,
     "end_time": "2022-07-03T08:21:12.018133",
     "exception": false,
     "start_time": "2022-07-03T08:21:11.994855",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ef0c0b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-03T08:21:12.068770Z",
     "iopub.status.busy": "2022-07-03T08:21:12.067983Z",
     "iopub.status.idle": "2022-07-03T08:34:01.120305Z",
     "shell.execute_reply": "2022-07-03T08:34:01.121050Z",
     "shell.execute_reply.started": "2022-07-02T11:31:52.497187Z"
    },
    "papermill": {
     "duration": 769.080089,
     "end_time": "2022-07-03T08:34:01.121517",
     "exception": false,
     "start_time": "2022-07-03T08:21:12.041428",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This version of the API is not optimized and should not be used to estimate the runtime of your code on the hidden test set.\n",
      "[2022-07-03 16:23:03] Pushed to latest date: 2021-10-28 00:00:00\n",
      "[2022-07-03 16:23:28] Pushed to latest date: 2021-10-29 00:00:00\n",
      "[2022-07-03 16:23:52] Pushed to latest date: 2021-11-01 00:00:00\n",
      "[2022-07-03 16:24:16] Pushed to latest date: 2021-11-02 00:00:00\n",
      "[2022-07-03 16:24:40] Pushed to latest date: 2021-11-04 00:00:00\n",
      "[2022-07-03 16:25:05] Pushed to latest date: 2021-11-05 00:00:00\n",
      "[2022-07-03 16:25:29] Pushed to latest date: 2021-11-08 00:00:00\n",
      "[2022-07-03 16:25:53] Pushed to latest date: 2021-11-09 00:00:00\n",
      "[2022-07-03 16:26:17] Pushed to latest date: 2021-11-10 00:00:00\n",
      "[2022-07-03 16:26:41] Pushed to latest date: 2021-11-11 00:00:00\n",
      "[2022-07-03 16:27:05] Pushed to latest date: 2021-11-12 00:00:00\n",
      "[2022-07-03 16:27:29] Pushed to latest date: 2021-11-15 00:00:00\n",
      "[2022-07-03 16:27:53] Pushed to latest date: 2021-11-16 00:00:00\n",
      "[2022-07-03 16:28:18] Pushed to latest date: 2021-11-17 00:00:00\n",
      "[2022-07-03 16:28:42] Pushed to latest date: 2021-11-18 00:00:00\n",
      "[2022-07-03 16:29:06] Pushed to latest date: 2021-11-19 00:00:00\n",
      "[2022-07-03 16:29:30] Pushed to latest date: 2021-11-22 00:00:00\n",
      "[2022-07-03 16:29:55] Pushed to latest date: 2021-11-24 00:00:00\n",
      "[2022-07-03 16:30:18] Pushed to latest date: 2021-11-25 00:00:00\n",
      "[2022-07-03 16:30:42] Pushed to latest date: 2021-11-26 00:00:00\n",
      "[2022-07-03 16:31:06] Pushed to latest date: 2021-11-29 00:00:00\n",
      "[2022-07-03 16:31:29] Pushed to latest date: 2021-11-30 00:00:00\n",
      "[2022-07-03 16:31:55] Pushed to latest date: 2021-12-01 00:00:00\n",
      "[2022-07-03 16:32:19] Pushed to latest date: 2021-12-02 00:00:00\n",
      "[2022-07-03 16:32:43] Pushed to latest date: 2021-12-03 00:00:00\n",
      "[2022-07-03 16:33:08] Pushed to latest date: 2021-12-06 00:00:00\n",
      "[2022-07-03 16:33:47] Pushed to latest date: 2021-12-07 00:00:00\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>SecuritiesCode</th>\n",
       "      <th>Rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-12-06</td>\n",
       "      <td>1301</td>\n",
       "      <td>1321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-12-06</td>\n",
       "      <td>1332</td>\n",
       "      <td>1357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-12-06</td>\n",
       "      <td>1333</td>\n",
       "      <td>886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-12-06</td>\n",
       "      <td>1375</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-12-06</td>\n",
       "      <td>1376</td>\n",
       "      <td>408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>2021-12-07</td>\n",
       "      <td>9990</td>\n",
       "      <td>846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>2021-12-07</td>\n",
       "      <td>9991</td>\n",
       "      <td>1220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>2021-12-07</td>\n",
       "      <td>9993</td>\n",
       "      <td>1967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>2021-12-07</td>\n",
       "      <td>9994</td>\n",
       "      <td>1816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>2021-12-07</td>\n",
       "      <td>9997</td>\n",
       "      <td>1211</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date  SecuritiesCode  Rank\n",
       "0     2021-12-06            1301  1321\n",
       "1     2021-12-06            1332  1357\n",
       "2     2021-12-06            1333   886\n",
       "3     2021-12-06            1375     7\n",
       "4     2021-12-06            1376   408\n",
       "...          ...             ...   ...\n",
       "3995  2021-12-07            9990   846\n",
       "3996  2021-12-07            9991  1220\n",
       "3997  2021-12-07            9993  1967\n",
       "3998  2021-12-07            9994  1816\n",
       "3999  2021-12-07            9997  1211\n",
       "\n",
       "[4000 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowId</th>\n",
       "      <th>Date</th>\n",
       "      <th>SecuritiesCode</th>\n",
       "      <th>pred_model</th>\n",
       "      <th>target_train</th>\n",
       "      <th>Target</th>\n",
       "      <th>Rank</th>\n",
       "      <th>clust</th>\n",
       "      <th>pred_model_mean</th>\n",
       "      <th>pred_model_demean</th>\n",
       "      <th>std</th>\n",
       "      <th>pred_model_vol_penalty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20211206_1301</td>\n",
       "      <td>2021-12-06</td>\n",
       "      <td>1301</td>\n",
       "      <td>-0.154188</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1321</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.143411</td>\n",
       "      <td>-0.010777</td>\n",
       "      <td>0.013762</td>\n",
       "      <td>-9.362997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20211206_1332</td>\n",
       "      <td>2021-12-06</td>\n",
       "      <td>1332</td>\n",
       "      <td>-0.162823</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1357</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.143411</td>\n",
       "      <td>-0.019413</td>\n",
       "      <td>0.018825</td>\n",
       "      <td>-10.284835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20211206_1375</td>\n",
       "      <td>2021-12-06</td>\n",
       "      <td>1375</td>\n",
       "      <td>-0.093793</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.143411</td>\n",
       "      <td>0.049618</td>\n",
       "      <td>0.012052</td>\n",
       "      <td>53.156479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20211206_1376</td>\n",
       "      <td>2021-12-06</td>\n",
       "      <td>1376</td>\n",
       "      <td>-0.132352</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>408</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.143411</td>\n",
       "      <td>0.011059</td>\n",
       "      <td>0.014396</td>\n",
       "      <td>8.948097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20211206_1379</td>\n",
       "      <td>2021-12-06</td>\n",
       "      <td>1379</td>\n",
       "      <td>-0.156482</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1705</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.143411</td>\n",
       "      <td>-0.013071</td>\n",
       "      <td>0.008731</td>\n",
       "      <td>-23.293078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>20211207_9977</td>\n",
       "      <td>2021-12-07</td>\n",
       "      <td>9977</td>\n",
       "      <td>-0.116368</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1848</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.094188</td>\n",
       "      <td>-0.022180</td>\n",
       "      <td>0.009389</td>\n",
       "      <td>-35.243687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>20211207_2768</td>\n",
       "      <td>2021-12-07</td>\n",
       "      <td>2768</td>\n",
       "      <td>-0.042737</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>782</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.041017</td>\n",
       "      <td>-0.001721</td>\n",
       "      <td>0.500256</td>\n",
       "      <td>-0.005136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>20211207_7500</td>\n",
       "      <td>2021-12-07</td>\n",
       "      <td>7500</td>\n",
       "      <td>-0.124725</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1986</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.041017</td>\n",
       "      <td>-0.083708</td>\n",
       "      <td>0.008634</td>\n",
       "      <td>-151.839277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>20211207_8713</td>\n",
       "      <td>2021-12-07</td>\n",
       "      <td>8713</td>\n",
       "      <td>-0.011084</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>780</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.041017</td>\n",
       "      <td>0.029932</td>\n",
       "      <td>1.082455</td>\n",
       "      <td>0.026413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>20211207_9919</td>\n",
       "      <td>2021-12-07</td>\n",
       "      <td>9919</td>\n",
       "      <td>0.014480</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>626</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.041017</td>\n",
       "      <td>0.055496</td>\n",
       "      <td>0.068116</td>\n",
       "      <td>3.859250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              RowId       Date  SecuritiesCode  pred_model  target_train  \\\n",
       "0     20211206_1301 2021-12-06            1301   -0.154188           0.0   \n",
       "1     20211206_1332 2021-12-06            1332   -0.162823           0.0   \n",
       "2     20211206_1375 2021-12-06            1375   -0.093793           0.0   \n",
       "3     20211206_1376 2021-12-06            1376   -0.132352           0.0   \n",
       "4     20211206_1379 2021-12-06            1379   -0.156482           0.0   \n",
       "...             ...        ...             ...         ...           ...   \n",
       "3995  20211207_9977 2021-12-07            9977   -0.116368           0.0   \n",
       "3996  20211207_2768 2021-12-07            2768   -0.042737           0.0   \n",
       "3997  20211207_7500 2021-12-07            7500   -0.124725           0.0   \n",
       "3998  20211207_8713 2021-12-07            8713   -0.011084           0.0   \n",
       "3999  20211207_9919 2021-12-07            9919    0.014480           0.0   \n",
       "\n",
       "      Target  Rank  clust  pred_model_mean  pred_model_demean       std  \\\n",
       "0        0.0  1321      5        -0.143411          -0.010777  0.013762   \n",
       "1        0.0  1357      5        -0.143411          -0.019413  0.018825   \n",
       "2        0.0     7      5        -0.143411           0.049618  0.012052   \n",
       "3        0.0   408      5        -0.143411           0.011059  0.014396   \n",
       "4        0.0  1705      5        -0.143411          -0.013071  0.008731   \n",
       "...      ...   ...    ...              ...                ...       ...   \n",
       "3995     0.0  1848      2        -0.094188          -0.022180  0.009389   \n",
       "3996     0.0   782      1        -0.041017          -0.001721  0.500256   \n",
       "3997     0.0  1986      1        -0.041017          -0.083708  0.008634   \n",
       "3998     0.0   780      1        -0.041017           0.029932  1.082455   \n",
       "3999     0.0   626      1        -0.041017           0.055496  0.068116   \n",
       "\n",
       "      pred_model_vol_penalty  \n",
       "0                  -9.362997  \n",
       "1                 -10.284835  \n",
       "2                  53.156479  \n",
       "3                   8.948097  \n",
       "4                 -23.293078  \n",
       "...                      ...  \n",
       "3995              -35.243687  \n",
       "3996               -0.005136  \n",
       "3997             -151.839277  \n",
       "3998                0.026413  \n",
       "3999                3.859250  \n",
       "\n",
       "[4000 rows x 12 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11min 52s, sys: 43.5 s, total: 12min 35s\n",
      "Wall time: 12min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# init env\n",
    "env = jpx_tokyo_market_prediction.make_env()\n",
    "iter_test = env.iter_test()\n",
    "\n",
    "# init variables\n",
    "if FE=='orig':\n",
    "    data = load_pkl(f'{FE_PATH}/data')\n",
    "elif FE=='ta':\n",
    "    data = load_pkl(f'{FE_PATH}/results (2)/data')\n",
    "sample_prediction_all = []\n",
    "df_pred_val_all = []\n",
    "updated_data = 0\n",
    "close = []\n",
    "df = pd.concat([pd.read_csv(f'{JPX_PATH}/train_files/stock_prices.csv'),\n",
    "                 pd.read_csv(f'{JPX_PATH}/supplemental_files/stock_prices.csv')]) \\\n",
    "    .loc[:, ['Date','SecuritiesCode','Close']]\n",
    "close.append(df)\n",
    "\n",
    "# prepare feature means\n",
    "if FE=='orig':\n",
    "    features = pd.concat([pd.concat(load_pkl(f'{FE_PATH}/features/{filename}')) for filename in os.listdir(f'{FE_PATH}/features')]).sort_values('RowId').reset_index(drop=True)\n",
    "elif FE=='ta':\n",
    "    features1 = pd.concat([pd.concat(load_pkl(f'{FE_PATH}/results (1)/features/{filename}')) for filename in os.listdir(f'{FE_PATH}/results (1)/features')]).sort_values('RowId').reset_index(drop=True)\n",
    "    features2 = pd.concat([pd.concat(load_pkl(f'{FE_PATH}/results (2)/features/{filename}')) for filename in os.listdir(f'{FE_PATH}/results (2)/features')]).sort_values('RowId').reset_index(drop=True)\n",
    "    features = pd.concat([features1, features2]).reset_index(drop=True)\n",
    "    del features1, features2\n",
    "    gc.collect()\n",
    "mean = features.groupby('Date').mean().ffill()\n",
    "mean = mean.fillna(mean.mean())\n",
    "mean = mean.tail(20).mean()\n",
    "del features\n",
    "gc.collect()\n",
    "\n",
    "# iterations\n",
    "for df_prices, df_opts, df_fins, df_trades, df_sec_prices, sample_prediction in iter_test:\n",
    "    \n",
    "    # add new supplemental data for first iteration\n",
    "    date = np.datetime64(sample_prediction.Date.iloc[0])\n",
    "    if updated_data==0:\n",
    "        update_data(date)\n",
    "        updated_data = 1\n",
    "        \n",
    "    # append ret\n",
    "    close.append(df_prices.loc[:, ['Date','SecuritiesCode','Close']])\n",
    "    ret = pd.concat(close) \\\n",
    "        .sort_values(['Date','SecuritiesCode']) \\\n",
    "        .assign(ret = lambda x: x.groupby('SecuritiesCode').Close.pct_change()) \\\n",
    "        .loc[:, ['Date','SecuritiesCode','ret']] \\\n",
    "        .dropna() \\\n",
    "        .drop_duplicates(subset=['Date','SecuritiesCode']) \\\n",
    "        .reset_index(drop=True)\n",
    "        \n",
    "    # set date columns\n",
    "    for df in [df_prices, df_opts, df_fins, df_trades, df_sec_prices]:\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        \n",
    "    # feature engineering of current date\n",
    "    data.push_forward([df_prices, df_sec_prices, df_fins, df_opts, df_trades], append=False, last=False)\n",
    "    features = data.curr_features\n",
    "    \n",
    "    # fillna\n",
    "    cols = [c for c in features.columns if c not in ['RowId','SecuritiesCode','Date']]\n",
    "    features[cols] = features[cols].fillna(mean[cols])\n",
    "    \n",
    "    # train-val split\n",
    "    full_data = features.assign(fold = -1,\n",
    "                                trn_val = 'val',\n",
    "                                Target = 0.0,\n",
    "                                target_train = 0.0)\n",
    "\n",
    "    # define column types\n",
    "    id_cols = ['RowId','Date','SecuritiesCode','fold','trn_val']\n",
    "    all_features = [c for c in list(full_data) if c not in id_cols and c not in ['target_train','Target']]\n",
    "    cat_features = ['AdjustmentFactor','first_div']\n",
    "    time_features = [c for c in all_features if '_mkt' in c] + \\\n",
    "                    [c for c in all_features if c[:4]=='sec_'] + \\\n",
    "                    [c for c in all_features if c[-4:] in ['_sin','_cos']]\n",
    "    stock_features = [c for c in all_features if c not in cat_features + time_features]\n",
    "\n",
    "    # scaling\n",
    "    scaler = load_pkl(f'{MODEL_PATH}/scaler4')\n",
    "    feats = time_features + stock_features\n",
    "    full_data[feats] = scaler.transform(full_data[feats]).astype(np.float32)\n",
    "\n",
    "    # drop market features\n",
    "    if DROP_MARKET_FEATS:\n",
    "        full_data = full_data.drop(time_features, axis=1)\n",
    "        all_features = [c for c in all_features if c not in time_features]\n",
    "\n",
    "    # PCA compression\n",
    "    if RUN_PCA:\n",
    "        if PCA_SPLIT==False:\n",
    "            pca = load_pkl(f'{MODEL_PATH}/pca')        \n",
    "            cols = [f'pc{x}' for x in range(pca.components_.shape[0])]\n",
    "            X = pd.DataFrame(pca.transform(full_data.loc[:, all_features]), columns=cols)\n",
    "            header = full_data.loc[:, [c for c in full_data.columns if c not in all_features]].reset_index(drop=True)\n",
    "            full_data = pd.concat([header, X], axis=1)  \n",
    "            all_features = cols.copy()\n",
    "            stock_features = None\n",
    "            time_features = None\n",
    "\n",
    "        elif PCA_SPLIT==True:\n",
    "            stock_feats = [c for c in all_features if c not in time_features]\n",
    "            market_feats = [c for c in all_features if c in time_features]\n",
    "            pca_stock = load_pkl(f'{MODEL_PATH}/pca_stock')\n",
    "            pca_market = load_pkl(f'{MODEL_PATH}/pca_market')\n",
    "            cols_stock = [f'pc_stock{x}' for x in range(pca_stock.components_.shape[0])]\n",
    "            cols_market = [f'pc_market{x}' for x in range(pca_market.components_.shape[0])]\n",
    "            X_stock = pd.DataFrame(pca_stock.transform(full_data.loc[:, stock_feats]), columns=cols_stock)\n",
    "            X_market = pd.DataFrame(pca_market.transform(full_data.loc[:, market_feats]), columns=cols_market)\n",
    "            header = full_data.loc[:, [c for c in full_data.columns if c not in all_features]].reset_index(drop=True)\n",
    "            full_data = pd.concat([header, X_stock, X_market], axis=1)\n",
    "            all_features = cols_stock + cols_market\n",
    "            stock_features = cols_stock\n",
    "            time_features = cols_market\n",
    "\n",
    "    # model prediction\n",
    "    df_pred_val = []\n",
    "    for seed in SEEDS:\n",
    "        for fold in range(N_FOLD):\n",
    "            X_val, y_val, grp_val, qid_val, header_val, target_val = get_dataset(full_data, SELECTED_FEATS, 'val')\n",
    "            model = load_pkl(f'{MODEL_PATH}/model{fold}_seed{seed}')\n",
    "            pred_val = pred_score(model, X_val)\n",
    "            df_pred_val.append(get_pred_df(header_val, pred_val, y_val, target_val, RANK_ASCENDING))\n",
    "    df_pred_val = pd.concat(df_pred_val).reset_index(drop=True)\n",
    "    df_pred_val = df_pred_val.groupby(['RowId','Date','SecuritiesCode']).mean().reset_index()\n",
    "    df_pred_val['Rank'] = df_pred_val.groupby('Date').pred_model.rank(method='first', ascending=False).astype(int) - 1\n",
    "\n",
    "    # cluster demean\n",
    "    if CLUSTER_DEMEAN:\n",
    "        df_clust = []\n",
    "        for date in df_pred_val.Date.unique():\n",
    "            stock_list = df_pred_val.loc[lambda x: x.Date==date].SecuritiesCode.tolist()\n",
    "            df_clust.append(get_stock_clust(ret, date, CLUST_N_DAY, stock_list))\n",
    "        df_clust = pd.concat(df_clust)\n",
    "        df_pred_val = df_pred_val.merge(df_clust, how='inner', on=['Date','SecuritiesCode'])\n",
    "        clust_mean = df_pred_val.groupby(['Date','clust']).pred_model.mean().reset_index().rename(columns={'pred_model':'pred_model_mean'})\n",
    "        df_pred_val = df_pred_val.merge(clust_mean, how='inner', on=['Date','clust'])\n",
    "        df_pred_val['pred_model_demean'] = df_pred_val.pred_model - df_pred_val.pred_model_mean\n",
    "        df_pred_val['Rank'] = df_pred_val.groupby('Date').pred_model_demean.rank(method='first', ascending=False).astype(int) - 1\n",
    "\n",
    "    # volatility penalty\n",
    "    if VOL_PENALTY:\n",
    "        std = ret.pivot(index='Date', columns='SecuritiesCode', values='ret') \\\n",
    "            .rolling(VOL_N_DAY).std() \\\n",
    "            .stack().reset_index() \\\n",
    "            .dropna() \\\n",
    "            .rename(columns={0:'std'}) \\\n",
    "            .assign(Date = lambda x: pd.to_datetime(x.Date))\n",
    "        df_pred_val = df_pred_val.merge(std, how='inner', on=['Date','SecuritiesCode'])\n",
    "        # identify best power and apply\n",
    "        best_p = VOL_POW\n",
    "        df_pred_val['pred_model_vol_penalty'] = df_pred_val.pred_model_demean / df_pred_val['std'].pow(best_p)\n",
    "        df_pred_val['Rank'] = df_pred_val.groupby('Date').pred_model_vol_penalty.rank(method='first', ascending=False).astype(int) - 1\n",
    "\n",
    "    # final submission\n",
    "    rnk = df_pred_val.set_index('SecuritiesCode').Rank\n",
    "    sample_prediction['Rank'] = sample_prediction['SecuritiesCode'].map(rnk)\n",
    "    sample_prediction['Rank'] = sample_prediction['Rank'].fillna(1000).rank(method='first', ascending=True).astype(int) - 1\n",
    "    env.predict(sample_prediction)\n",
    "    \n",
    "    # save results\n",
    "    sample_prediction_all.append(sample_prediction)\n",
    "    df_pred_val_all.append(df_pred_val)\n",
    "\n",
    "# output\n",
    "sample_prediction_all = pd.concat(sample_prediction_all).reset_index(drop=True)\n",
    "df_pred_val_all = pd.concat(df_pred_val_all).reset_index(drop=True)\n",
    "display(sample_prediction_all)\n",
    "display(df_pred_val_all)\n",
    "if EXPORT:\n",
    "    sample_prediction_all.to_csv('sample_prediction_all.csv', index=False)\n",
    "    df_pred_val_all.to_csv('df_pred_val_all.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9747951",
   "metadata": {
    "papermill": {
     "duration": 0.035579,
     "end_time": "2022-07-03T08:34:01.193221",
     "exception": false,
     "start_time": "2022-07-03T08:34:01.157642",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69f9819",
   "metadata": {
    "papermill": {
     "duration": 0.035191,
     "end_time": "2022-07-03T08:34:01.263808",
     "exception": false,
     "start_time": "2022-07-03T08:34:01.228617",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa55ee10",
   "metadata": {
    "papermill": {
     "duration": 0.035367,
     "end_time": "2022-07-03T08:34:01.334751",
     "exception": false,
     "start_time": "2022-07-03T08:34:01.299384",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e3e681",
   "metadata": {
    "papermill": {
     "duration": 0.035161,
     "end_time": "2022-07-03T08:34:01.405415",
     "exception": false,
     "start_time": "2022-07-03T08:34:01.370254",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b973ba1",
   "metadata": {
    "papermill": {
     "duration": 0.035137,
     "end_time": "2022-07-03T08:34:01.476121",
     "exception": false,
     "start_time": "2022-07-03T08:34:01.440984",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 816.236234,
   "end_time": "2022-07-03T08:34:02.830207",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-07-03T08:20:26.593973",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
