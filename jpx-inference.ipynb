{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f3c2c7c",
   "metadata": {
    "papermill": {
     "duration": 0.026055,
     "end_time": "2022-07-03T08:24:53.270311",
     "exception": false,
     "start_time": "2022-07-03T08:24:53.244256",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Notes\n",
    "- JPX_Model_LGBM_v26\n",
    "\n",
    "# To do list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6a67673",
   "metadata": {
    "_cell_guid": "5ae9011f-40e6-497b-a6be-61b748ce1d2d",
    "_kg_hide-input": true,
    "_uuid": "2c45ee48-0594-488c-8a45-26a2e5510d83",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-07-03T08:24:53.319154Z",
     "iopub.status.busy": "2022-07-03T08:24:53.317917Z",
     "iopub.status.idle": "2022-07-03T08:25:26.447476Z",
     "shell.execute_reply": "2022-07-03T08:25:26.446776Z",
     "shell.execute_reply.started": "2022-07-02T11:31:22.793709Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 33.154519,
     "end_time": "2022-07-03T08:25:26.447656",
     "exception": false,
     "start_time": "2022-07-03T08:24:53.293137",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/ta-0101/ta-0.10.1-py3-none-any.whl\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from ta==0.10.1) (1.3.5)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from ta==0.10.1) (1.20.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->ta==0.10.1) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->ta==0.10.1) (2021.3)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->ta==0.10.1) (1.16.0)\r\n",
      "Installing collected packages: ta\r\n",
      "Successfully installed ta-0.10.1\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import os\n",
    "from os.path import isfile, isdir, join\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from IPython.display import display\n",
    "from zipfile import ZipFile\n",
    "import pickle\n",
    "import unicodedata\n",
    "import pytz\n",
    "from joblib import Parallel, delayed\n",
    "import shutil\n",
    "import difflib\n",
    "import random\n",
    "import math\n",
    "from shutil import copyfile\n",
    "import itertools\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import collections\n",
    "from collections import deque\n",
    "import gc\n",
    "import seaborn as sns\n",
    "import scipy.cluster.hierarchy as spc\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import ndcg_score, accuracy_score\n",
    "\n",
    "import lightgbm as lgbm\n",
    "import optuna\n",
    "from optuna import Trial, visualization\n",
    "\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import jpx_tokyo_market_prediction\n",
    "\n",
    "from utility_script import *\n",
    "\n",
    "!pip install ../input/ta-0101/ta-0.10.1-py3-none-any.whl\n",
    "import ta\n",
    "from ta import add_all_ta_features\n",
    "from ta.utils import dropna\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e78f4df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-03T08:25:26.500243Z",
     "iopub.status.busy": "2022-07-03T08:25:26.499239Z",
     "iopub.status.idle": "2022-07-03T08:25:26.502369Z",
     "shell.execute_reply": "2022-07-03T08:25:26.501819Z",
     "shell.execute_reply.started": "2022-07-02T11:31:52.313941Z"
    },
    "papermill": {
     "duration": 0.03143,
     "end_time": "2022-07-03T08:25:26.502504",
     "exception": false,
     "start_time": "2022-07-03T08:25:26.471074",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# general params\n",
    "FE = 'ta'\n",
    "MODEL = 'clf'\n",
    "FE_PATH = '../input/jpx-feature-engineering-ta-v3'\n",
    "MODEL_PATH = '../input/jpx-model-lgbm-v26'\n",
    "JPX_PATH = '../input/jpx-tokyo-stock-exchange-prediction'\n",
    "EXPORT = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfc88a18",
   "metadata": {
    "_cell_guid": "c0f0bfa1-3dea-43a3-b2f1-8a2c268d6339",
    "_uuid": "c3cfb2dc-8437-441f-b2c0-88b3d4a77281",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-07-03T08:25:26.554738Z",
     "iopub.status.busy": "2022-07-03T08:25:26.553917Z",
     "iopub.status.idle": "2022-07-03T08:25:26.556541Z",
     "shell.execute_reply": "2022-07-03T08:25:26.556024Z",
     "shell.execute_reply.started": "2022-07-02T11:31:52.321211Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.031164,
     "end_time": "2022-07-03T08:25:26.556674",
     "exception": false,
     "start_time": "2022-07-03T08:25:26.525510",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "FE Parameters\n",
    "'''\n",
    "SEED = 0\n",
    "LAGS = {'1d':1, '3d':3, '1w':5, '1m':20, '3m':20*3, '6m':20*6, '12m':20*12}\n",
    "MAX_DAYS_LAG = max(list(LAGS.values()))\n",
    "WIN_SIZE = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61154859",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-03T08:25:26.608600Z",
     "iopub.status.busy": "2022-07-03T08:25:26.604648Z",
     "iopub.status.idle": "2022-07-03T08:25:26.610977Z",
     "shell.execute_reply": "2022-07-03T08:25:26.610333Z",
     "shell.execute_reply.started": "2022-07-02T11:31:52.33734Z"
    },
    "papermill": {
     "duration": 0.031619,
     "end_time": "2022-07-03T08:25:26.611116",
     "exception": false,
     "start_time": "2022-07-03T08:25:26.579497",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Model Params\n",
    "'''\n",
    "# basic\n",
    "SEED = 0\n",
    "SEEDS = [1,2,3,4,5]\n",
    "\n",
    "# feature composition\n",
    "DROP_MARKET_FEATS = False\n",
    "\n",
    "# PCA\n",
    "RUN_PCA = True\n",
    "PCA_SPLIT = True\n",
    "\n",
    "# target definition\n",
    "RANK_ASCENDING = False # set this to False if model prediction is same direction of Target\n",
    "TARGET_POW = 0\n",
    "\n",
    "# data split\n",
    "N_FOLD = 5\n",
    "\n",
    "# final features\n",
    "SELECTED_FEATS = 'pc_stock17, pc_stock5, pc_stock3, pc_market2, pc_market4'.split(', ')\n",
    "\n",
    "# optimization\n",
    "CLUSTER_DEMEAN = True\n",
    "CLUST_N_DAY = 60\n",
    "VOL_PENALTY = False\n",
    "VOL_N_DAY = 60\n",
    "VOL_POW = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e89f47e",
   "metadata": {
    "papermill": {
     "duration": 0.022072,
     "end_time": "2022-07-03T08:25:26.655935",
     "exception": false,
     "start_time": "2022-07-03T08:25:26.633863",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0184629e",
   "metadata": {
    "_cell_guid": "6a3c94c5-9152-4477-9b79-dec0b881fd0e",
    "_uuid": "54a20792-c5bc-4ee2-8162-07364e311d9c",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-07-03T08:25:26.722114Z",
     "iopub.status.busy": "2022-07-03T08:25:26.721153Z",
     "iopub.status.idle": "2022-07-03T08:25:26.723311Z",
     "shell.execute_reply": "2022-07-03T08:25:26.723771Z",
     "shell.execute_reply.started": "2022-07-02T11:31:52.348716Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.045619,
     "end_time": "2022-07-03T08:25:26.723946",
     "exception": false,
     "start_time": "2022-07-03T08:25:26.678327",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class JPXData:\n",
    "    def __init__(self, window_size, df_names):\n",
    "        self.size = 0\n",
    "        self.window_size = window_size\n",
    "        self.df_names = df_names\n",
    "        self.num_df = len(df_names)\n",
    "        self.data = {df_name : pd.DataFrame() for df_name in df_names}\n",
    "        self.row_counts = {df_name : [] for df_name in df_names}\n",
    "        self.dates = []\n",
    "        self.first_date, self.last_date = None, None\n",
    "        self.features = []\n",
    "        self.curr_features = None\n",
    "        self.n_day_hist = 0\n",
    "        self.init_folders()\n",
    "        \n",
    "    def init_folders(self):\n",
    "        shutil.rmtree(path='./features', ignore_errors=True)\n",
    "        os.mkdir('./features')\n",
    "        \n",
    "    def append_data(self):\n",
    "        self.features.append(self.curr_features)\n",
    "        self.n_day_hist += 1\n",
    "        \n",
    "    def archive_data(self):\n",
    "        save_pkl(self.features, f'./features/features_{self.n_day_hist}')\n",
    "        self.clear_hist()\n",
    "        \n",
    "    def clear_hist(self):\n",
    "        self.features = []\n",
    "        \n",
    "    def push_forward(self, new_data, append, last):\n",
    "        # assign names to new data assuming the same as df_names\n",
    "        new_data = dict(zip(self.df_names, new_data))\n",
    "        # case when no enough data\n",
    "        if self.size < self.window_size:\n",
    "            for df_name in self.df_names:\n",
    "                self.data[df_name] = pd.concat([self.data[df_name], new_data[df_name]]).reset_index(drop=True)\n",
    "                self.row_counts[df_name] = self.row_counts[df_name] + [new_data[df_name].shape[0]]\n",
    "            self.dates = self.dates + [new_data[self.df_names[0]].Date.iloc[0]] \n",
    "            self.size += 1\n",
    "        # general case (shift by 1 day)\n",
    "        else:\n",
    "            for df_name in self.df_names:\n",
    "                self.data[df_name] = pd.concat([self.data[df_name].iloc[self.row_counts[df_name][0]:], new_data[df_name]]).reset_index(drop=True)\n",
    "                self.row_counts[df_name] = self.row_counts[df_name][1:] + [new_data[df_name].shape[0]]\n",
    "            self.dates = self.dates[1:] + [new_data[self.df_names[0]].Date.iloc[0]]  \n",
    "        # update date range\n",
    "        self.first_date, self.last_date = self.dates[0], self.dates[-1]\n",
    "        # generate features\n",
    "        if self.size == self.window_size:\n",
    "            self.curr_features = get_features(self.data)\n",
    "            if append==True:\n",
    "                self.append_data()\n",
    "                if (self.n_day_hist%20 == 0 and self.n_day_hist > 0) or last==True:\n",
    "                    self.archive_data()\n",
    "        log(f'Pushed to latest date: {self.last_date}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35a09195",
   "metadata": {
    "_cell_guid": "ae8fcff3-07c3-4ef5-ade3-61d6dc34243b",
    "_uuid": "c4fccf97-02d7-4f36-a5fa-53efcfd2edc6",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-07-03T08:25:26.786070Z",
     "iopub.status.busy": "2022-07-03T08:25:26.785357Z",
     "iopub.status.idle": "2022-07-03T08:25:26.788060Z",
     "shell.execute_reply": "2022-07-03T08:25:26.787534Z",
     "shell.execute_reply.started": "2022-07-02T11:31:52.370014Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.041688,
     "end_time": "2022-07-03T08:25:26.788196",
     "exception": false,
     "start_time": "2022-07-03T08:25:26.746508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def standard_dist(s, lag):\n",
    "    tail_data = s.tail(LAGS[lag])\n",
    "    return (s.iloc[-1] - tail_data.mean()) / tail_data.std()\n",
    "\n",
    "def ma_pctg_ch(s, lag):\n",
    "    return s.iloc[-1] / s.tail(LAGS[lag]).mean() - 1\n",
    "\n",
    "def sharpe(s, lag):\n",
    "    tail_data = s.tail(LAGS[lag])\n",
    "    std = tail_data.std()\n",
    "    if std > 0:\n",
    "        sharpe_ratio = tail_data.mean() / tail_data.std()\n",
    "    else:\n",
    "        sharpe_ratio = 0\n",
    "    return sharpe_ratio\n",
    "\n",
    "def gen_ta_feats(df, n_day_ma, n_day_scale):\n",
    "    # preprocess\n",
    "    df = df \\\n",
    "        .sort_values(['SecuritiesCode','Date']) \\\n",
    "        .groupby('SecuritiesCode') \\\n",
    "        .tail(n_day_ma + n_day_scale) \\\n",
    "        .loc[:, ['SecuritiesCode','Open','High','Low','Close','Volume']] \\\n",
    "        .reset_index(drop=True)\n",
    "    # gen TA feats\n",
    "    df = ta.add_all_ta_features(df, \"Open\", \"High\", \"Low\", \"Close\", \"Volume\", fillna=False) \\\n",
    "        .drop(['Open','High','Low','Close','Volume'], axis=1)\n",
    "    ta_cols = df.columns.tolist()[1:]\n",
    "    ta_cols = [c for c in ta_cols if c not in ['volatility_atr',\n",
    "                                                 'trend_adx',\n",
    "                                                 'trend_adx_pos',\n",
    "                                                 'trend_adx_neg',\n",
    "                                                 'trend_psar_up',\n",
    "                                                 'trend_psar_down',\n",
    "                                                 'momentum_kama']]\n",
    "    ta_cols = ['volume_obv',\n",
    "                 'volume_cmf',\n",
    "                 'volume_fi',\n",
    "                 'volume_em',\n",
    "                 'volume_sma_em',\n",
    "                 'volume_vpt',\n",
    "                 'volume_mfi',\n",
    "                 'volume_nvi',\n",
    "                 'volatility_bbw',\n",
    "                 'volatility_bbhi',\n",
    "                 'volatility_bbli',\n",
    "                 'volatility_kcw',\n",
    "                 'volatility_kcp',\n",
    "                 'volatility_kchi',\n",
    "                 'volatility_kcli',\n",
    "                 'volatility_ui',\n",
    "                 'trend_macd_signal',\n",
    "                 'trend_macd_diff',\n",
    "                 'trend_vortex_ind_pos',\n",
    "                 'trend_vortex_ind_neg',\n",
    "                 'trend_mass_index',\n",
    "                 'trend_dpo',\n",
    "                 'trend_kst',\n",
    "                 'trend_kst_diff',\n",
    "                 'trend_ichimoku_conv',\n",
    "                 'trend_stc',\n",
    "                 'trend_cci',\n",
    "                 'trend_visual_ichimoku_b',\n",
    "                 'trend_aroon_up',\n",
    "                 'trend_aroon_down',\n",
    "                 'trend_psar_up_indicator',\n",
    "                 'trend_psar_down_indicator',\n",
    "                 'momentum_rsi',\n",
    "                 'momentum_stoch_rsi_d',\n",
    "                 'momentum_uo',\n",
    "                 'momentum_wr',\n",
    "                 'momentum_ao',\n",
    "                 'momentum_roc',\n",
    "                 'momentum_ppo',\n",
    "                 'momentum_ppo_hist',\n",
    "                 'momentum_pvo',\n",
    "                 'momentum_pvo_hist',\n",
    "                 'others_dlr']\n",
    "    df = df[['SecuritiesCode'] + ta_cols]\n",
    "    # scale by mean\n",
    "    mean = df \\\n",
    "        .groupby('SecuritiesCode') \\\n",
    "        .tail(n_day_scale)[ta_cols] \\\n",
    "        .abs() \\\n",
    "        .mean()\n",
    "    for c in ta_cols:\n",
    "        df[c] = df[c] / mean[c]\n",
    "    # take last row as features\n",
    "    df = df.groupby('SecuritiesCode').tail(1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a64a923d",
   "metadata": {
    "_cell_guid": "4a637337-7541-4b8f-bdff-7aca913a0aef",
    "_uuid": "18f34c4c-eadc-4ab7-bbaf-ee1753588b7f",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-07-03T08:25:26.880077Z",
     "iopub.status.busy": "2022-07-03T08:25:26.868864Z",
     "iopub.status.idle": "2022-07-03T08:25:26.882751Z",
     "shell.execute_reply": "2022-07-03T08:25:26.883410Z",
     "shell.execute_reply.started": "2022-07-02T11:31:52.391338Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.072474,
     "end_time": "2022-07-03T08:25:26.883576",
     "exception": false,
     "start_time": "2022-07-03T08:25:26.811102",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7 µs, sys: 0 ns, total: 7 µs\n",
      "Wall time: 10.5 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if FE=='orig':\n",
    "    def get_features(data):\n",
    "        df_prices, df_sec_prices, df_fins, df_opts, df_trades = tuple(data.values())\n",
    "\n",
    "        # base table\n",
    "        features = df_prices \\\n",
    "            .loc[lambda x: x.Date==x.Date.iloc[-1]] \\\n",
    "            .loc[:, ['RowId','Date','SecuritiesCode']] \\\n",
    "            .drop_duplicates(subset='RowId') \\\n",
    "            .reset_index(drop=True)\n",
    "\n",
    "        '''\n",
    "        Major stock prices features\n",
    "        '''\n",
    "        # precalculate new columns\n",
    "        cols = [c for c in df_prices.columns.tolist()[3:] if c!='ExpectedDividend']\n",
    "        df_prices[cols] = df_prices.groupby('SecuritiesCode')[cols].ffill()\n",
    "        df_prices['ret'] = df_prices.groupby('SecuritiesCode').Close.pct_change()\n",
    "        ret_mkt = df_prices.groupby('Date').ret.mean()\n",
    "        var_mkt = (ret_mkt**2).tail(LAGS['12m']).sum()\n",
    "        df_prices['ret_mkt'] = df_prices.Date.map(ret_mkt)\n",
    "        df_prices['spread'] = df_prices['High'] - df_prices['Low']\n",
    "        df_prices['div_ratio'] = df_prices['ExpectedDividend'].fillna(0) / df_prices['Close']\n",
    "        df_prices['dollar_traded'] = np.log(df_prices.Volume * (df_prices.Open + df_prices.Close)/2 + 1)\n",
    "        df_prices['RS_sqrt_vol'] = np.sqrt(np.log(df_prices['High']/df_prices['Close'])*np.log(df_prices['High']/df_prices['Open']) + np.log(df_prices['Low']/df_prices['Close'])*np.log(df_prices['Low']/df_prices['Open']))\n",
    "        df_prices['num_div'] = df_prices.groupby('SecuritiesCode').ExpectedDividend.apply(lambda s: s.notnull().astype(int).cumsum())\n",
    "        df_prices['first_div'] = ((df_prices.num_div==1) & (df_prices.num_div.shift(1)==0)).astype(int)\n",
    "        # previous day return\n",
    "        features['ret'] = features.SecuritiesCode.map(df_prices.groupby('SecuritiesCode').ret.last())\n",
    "        # Change in Close price\n",
    "        for lag in ['3d','1w']:\n",
    "            features[f'price_ma_pctg_ch_{lag}'] = features.SecuritiesCode.map(df_prices.groupby('SecuritiesCode').Close.apply(lambda s: ma_pctg_ch(s, lag)))\n",
    "        for lag in ['1m','3m','6m','12m']:\n",
    "            features[f'price_standard_dist_{lag}'] = features.SecuritiesCode.map(df_prices.groupby('SecuritiesCode').Close.apply(lambda s: standard_dist(s, lag)))\n",
    "        # Change in volume\n",
    "        for lag in ['3d','1w']:\n",
    "            features[f'volume_ma_pctg_ch_{lag}'] = features.SecuritiesCode.map(df_prices.groupby('SecuritiesCode').Volume.apply(lambda s: ma_pctg_ch(s, lag)))\n",
    "        for lag in ['1m','3m','6m','12m']:\n",
    "            features[f'volume_standard_dist_{lag}'] = features.SecuritiesCode.map(df_prices.groupby('SecuritiesCode').Volume.apply(lambda s: standard_dist(s, lag)))\n",
    "        # daily spread\n",
    "        for lag in ['3d','1w']:\n",
    "            features[f'spread_ma_pctg_ch_{lag}'] = features.SecuritiesCode.map(df_prices.groupby('SecuritiesCode').spread.apply(lambda s: ma_pctg_ch(s, lag)))\n",
    "        for lag in ['1m']:\n",
    "            features[f'spread_standard_dist_{lag}'] = features.SecuritiesCode.map(df_prices.groupby('SecuritiesCode').spread.apply(lambda s: standard_dist(s, lag)))\n",
    "        # volatility\n",
    "        for lag in ['1w','1m','3m','12m']:\n",
    "            features[f'volatility_{lag}'] = features.SecuritiesCode.map(df_prices.groupby('SecuritiesCode').ret.apply(lambda s: s.tail(LAGS[lag]).std())) \n",
    "        # change in volatility\n",
    "        features['volatility_diff'] = features['volatility_1w'] - features['volatility_1m']\n",
    "        # market return and volatility\n",
    "        for lag in ['3d','1w','1m','3m']:\n",
    "            features[f'ret_mkt_{lag}'] = ret_mkt.tail(LAGS[lag]).sum()\n",
    "            features[f'vol_mkt_{lag}'] = ret_mkt.tail(LAGS[lag]).std()\n",
    "        # beta\n",
    "        df_prices['beta'] = df_prices.SecuritiesCode.map(df_prices.groupby('SecuritiesCode').apply(lambda df: (df.set_index('Date').ret * ret_mkt).tail(LAGS['12m']).sum() / var_mkt))\n",
    "        features['beta'] = features.SecuritiesCode.map(df_prices.groupby('SecuritiesCode')['beta'].last())\n",
    "        # excess return\n",
    "        df_prices['exret'] = df_prices['ret'] - df_prices['beta'] * df_prices['ret_mkt']\n",
    "        for lag in ['3d','1w','1m','3m']:\n",
    "            features[f'exret_{lag}'] = features.SecuritiesCode.map(df_prices.groupby('SecuritiesCode')['exret'].apply(lambda s: s.tail(LAGS[lag]).sum()))\n",
    "        # div ratio\n",
    "        features['div_ratio'] = features.SecuritiesCode.map(df_prices.groupby('SecuritiesCode').apply(lambda df: df.ExpectedDividend.fillna(0).iloc[-1] / df.Close.tail(LAGS['1m']).mean()))\n",
    "        # change in dollar value traded\n",
    "        for lag in ['1w','1m']:\n",
    "            features[f'dollar_standard_dist_{lag}'] = features.SecuritiesCode.map(df_prices.groupby('SecuritiesCode').dollar_traded.apply(lambda s: standard_dist(s, lag)))\n",
    "        # RS_sqrt_vol\n",
    "        features['RS_sqrt_vol'] = features.SecuritiesCode.map(df_prices.groupby('SecuritiesCode').RS_sqrt_vol.last())\n",
    "        # sharpe\n",
    "        for lag in ['1m','3m']:\n",
    "            features[f'sharpe_{lag}'] = features.SecuritiesCode.map(df_prices.groupby('SecuritiesCode').ret.apply(lambda s: sharpe(s, lag)))\n",
    "        # days since last dividend\n",
    "        features['days_since_last_div'] = features.SecuritiesCode.map((df_prices.Date.iloc[-1] - df_prices.loc[lambda x: x.ExpectedDividend.notnull()].groupby('SecuritiesCode').Date.last()) / np.timedelta64(1,'D'))\n",
    "        # initiate dividend\n",
    "        features['first_div'] = features.SecuritiesCode.map(df_prices.groupby('SecuritiesCode').first_div.last())\n",
    "        # AdjustmentFactor\n",
    "        features['AdjustmentFactor'] = features.SecuritiesCode.map(df_prices.groupby('SecuritiesCode').AdjustmentFactor.apply(lambda s: (s!=1).astype(int).iloc[-1]))\n",
    "\n",
    "\n",
    "        '''\n",
    "        Secondary stock prices features\n",
    "        '''\n",
    "        # precalculate new columns\n",
    "        df_sec_prices['ret'] = df_sec_prices.groupby('SecuritiesCode').Close.pct_change()\n",
    "        df_sec_prices['dollar_traded'] = np.log(df_sec_prices.Volume * (df_sec_prices.Open + df_sec_prices.Close)/2 + 1)\n",
    "        # cross-sectional return & volatility\n",
    "        for n in [1,3]:\n",
    "            features[f'sec_cross_sect_ret_{n}'] = df_sec_prices.groupby('SecuritiesCode').ret.apply(lambda s: s.tail(n).sum()).mean()\n",
    "            features[f'sec_cross_sect_vol_{n}'] = df_sec_prices.groupby('SecuritiesCode').ret.apply(lambda s: s.tail(n).sum()).std()\n",
    "        # Change in volume\n",
    "        for lag in ['3d','1w']:\n",
    "            features[f'sec_volume_ma_pctg_ch_{lag}'] = df_sec_prices.groupby('SecuritiesCode').Volume.apply(lambda s: ma_pctg_ch(s, lag)).mean()\n",
    "        # volatility\n",
    "        for lag in ['1w','1m','3m']:\n",
    "            features[f'sec_volatility_{lag}'] = df_sec_prices.groupby('SecuritiesCode').ret.apply(lambda s: s.tail(LAGS[lag]).std()).mean()\n",
    "        # change in volatility\n",
    "        features['sec_volatility_diff'] = features['sec_volatility_1w'] - features['sec_volatility_1m']\n",
    "        # change in dollar value traded\n",
    "        for lag in ['1w','1m']:\n",
    "            features[f'sec_dollar_standard_dist_{lag}'] = df_sec_prices.groupby('SecuritiesCode').dollar_traded.apply(lambda s: standard_dist(s, lag)).mean()\n",
    "\n",
    "        '''\n",
    "        Time phase features\n",
    "        '''\n",
    "        # day in week\n",
    "        day_in_week_angle = (features.Date.dt.weekday / 5 * 2 * np.pi).iloc[-1]\n",
    "        features['day_in_week_sin'] = np.sin(day_in_week_angle)\n",
    "        features['day_in_week_cos'] = np.cos(day_in_week_angle)\n",
    "        # day in month\n",
    "        day_in_month_angle = ((features.Date.dt.day - 1) / 31 * 2 * np.pi).iloc[-1]\n",
    "        features['day_in_month_sin'] = np.sin(day_in_month_angle)\n",
    "        features['day_in_month_cos'] = np.cos(day_in_month_angle)\n",
    "        # week in year\n",
    "        week_in_year_angle = ((features.Date.dt.week - 1) / 52 * 2 * np.pi).iloc[-1]\n",
    "        features['week_in_year_sin'] = np.sin(week_in_year_angle)\n",
    "        features['week_in_year_cos'] = np.cos(week_in_year_angle)\n",
    "\n",
    "\n",
    "        '''\n",
    "        Financials features\n",
    "        '''\n",
    "        # convert string to numbers\n",
    "        fin_cols = ['NetSales','OperatingProfit','OrdinaryProfit','Profit','EarningsPerShare','TotalAssets','Equity','EquityToAssetRatio','BookValuePerShare',\n",
    "                    'ForecastNetSales','ForecastOperatingProfit','ForecastOrdinaryProfit','ForecastProfit','ForecastEarningsPerShare']\n",
    "        # clean numeric values\n",
    "        df_fins[fin_cols] = df_fins[fin_cols].replace('－',np.nan).astype(float)\n",
    "        # quarter forward fill\n",
    "        df_fins = df_fins.sort_values(['SecuritiesCode','TypeOfCurrentPeriod','Date']).reset_index(drop=True)\n",
    "        df_fins[fin_cols] = df_fins.groupby(['SecuritiesCode','TypeOfCurrentPeriod'])[fin_cols].ffill()\n",
    "        # overall forward fill\n",
    "        df_fins = df_fins.sort_values(['SecuritiesCode','Date']).reset_index(drop=True)\n",
    "        df_fins[fin_cols] = df_fins.groupby('SecuritiesCode')[fin_cols].ffill()\n",
    "        # drop invalid rows\n",
    "        df_fins = df_fins \\\n",
    "            .loc[lambda x: x.NetSales > 0] \\\n",
    "            .sort_values(['SecuritiesCode','Date']) \\\n",
    "            .reset_index(drop=True)\n",
    "        # define columns\n",
    "        df_fins['Close'] = df_fins.SecuritiesCode.map(df_prices.groupby('SecuritiesCode').Close.last())\n",
    "        df_fins['SalesToEquityRatio'] = df_fins['NetSales'] / df_fins['Equity']\n",
    "        df_fins['BookToMarketRatio'] = df_fins['Equity'] / df_fins['Close']\n",
    "        df_fins['ProfitoMarketRatio'] = df_fins['OperatingProfit'] / df_fins['Close']\n",
    "        df_fins['EarningToPriceRatio'] = df_fins['EarningsPerShare'] / df_fins['Close']\n",
    "        fin_cols_static = ['NetSales','OperatingProfit','OrdinaryProfit','Profit','EarningsPerShare','TotalAssets','Equity','BookValuePerShare',\n",
    "                            'ForecastNetSales','ForecastOperatingProfit','ForecastOrdinaryProfit','ForecastProfit','ForecastEarningsPerShare']\n",
    "        fin_cols_ratio = ['EquityToAssetRatio','SalesToEquityRatio','BookToMarketRatio','ProfitoMarketRatio','EarningToPriceRatio']\n",
    "        fin_cols = fin_cols_static + fin_cols_ratio\n",
    "        # fins feature calculation\n",
    "        df1 = df_fins.sort_values(['SecuritiesCode','TypeOfCurrentPeriod','Date']).reset_index(drop=True).groupby(['SecuritiesCode','TypeOfCurrentPeriod'])[fin_cols].nth(-1)\n",
    "        df2 = df_fins.sort_values(['SecuritiesCode','TypeOfCurrentPeriod','Date']).reset_index(drop=True).groupby(['SecuritiesCode','TypeOfCurrentPeriod'])[fin_cols].nth(-2)\n",
    "        df = df1.merge(df2, how='left', left_index=True, right_index=True)\n",
    "        for c in fin_cols:\n",
    "            if c in fin_cols_static:\n",
    "                df[f'{c}_pctg'] = (df[f'{c}_x'] - df[f'{c}_y']) / df[f'{c}_y'].abs()\n",
    "            elif c in fin_cols_ratio:\n",
    "                df[f'{c}_raw'] = df[f'{c}_x']\n",
    "                df[f'{c}_diff'] = df[f'{c}_x'] - df[f'{c}_y']\n",
    "        df = df.drop([c for c in df if c[-2:] in ['_x','_y']], axis=1).reset_index()\n",
    "        feats_fins = df_fins.sort_values(['SecuritiesCode','Date']).groupby('SecuritiesCode').last()['TypeOfCurrentPeriod'].reset_index()\n",
    "        feats_fins = feats_fins.merge(df, how='left', on=['SecuritiesCode','TypeOfCurrentPeriod']).drop('TypeOfCurrentPeriod', axis=1)\n",
    "        features = features.merge(feats_fins, how='left', on='SecuritiesCode')\n",
    "        # num days since last announcement\n",
    "        features['days_since_last_fin'] = (features.Date - features.SecuritiesCode.map(df_fins.groupby('SecuritiesCode').Date.last())) / np.timedelta64(1,'D')\n",
    "\n",
    "\n",
    "        '''\n",
    "        Post-processing\n",
    "        '''\n",
    "        cols = [c for c in features.columns if c not in ['RowId','Date','SecuritiesCode']]\n",
    "        features[cols] = features[cols].replace(np.inf, np.nan).replace(-np.inf, np.nan)\n",
    "        features[cols] = features[cols].fillna(features[cols].mean())\n",
    "        features[cols] = features[cols].astype(np.float32)\n",
    "\n",
    "        return features\n",
    "\n",
    "    \n",
    "elif FE=='ta':\n",
    "\n",
    "    def get_features(data):\n",
    "        df_prices, df_sec_prices, df_fins, df_opts, df_trades = tuple(data.values())\n",
    "    #     df_prices, df_sec_prices, df_fins, df_opts, df_trades = tuple(data.data.values())\n",
    "\n",
    "        # base table\n",
    "        features = df_prices.loc[lambda x: x.Date==x.Date.iloc[-1]][['RowId','Date','SecuritiesCode']]\n",
    "\n",
    "        # precalculate new columns\n",
    "        cols = [c for c in df_prices.columns.tolist()[3:] if c!='ExpectedDividend']\n",
    "        df_prices[cols] = df_prices.groupby('SecuritiesCode')[cols].ffill()\n",
    "        df_prices['ret'] = df_prices.groupby('SecuritiesCode').Close.pct_change()\n",
    "        ret_mkt = df_prices.groupby('Date').ret.mean()\n",
    "\n",
    "        # market return and volatility\n",
    "        for lag in ['3d','1w','1m','3m']:\n",
    "            features[f'ret_mkt_{lag}'] = ret_mkt.tail(LAGS[lag]).sum()\n",
    "            features[f'vol_mkt_{lag}'] = ret_mkt.tail(LAGS[lag]).std()\n",
    "\n",
    "        # TA feats\n",
    "        ta_feats = gen_ta_feats(df=df_prices, n_day_ma=52, n_day_scale=20)\n",
    "        features = features.merge(ta_feats, how='inner', on='SecuritiesCode')\n",
    "\n",
    "        '''\n",
    "        Post-processing\n",
    "        '''\n",
    "        cols = [c for c in features.columns if c not in ['RowId','Date','SecuritiesCode']]\n",
    "        features[cols] = features[cols].replace(np.inf, np.nan).replace(-np.inf, np.nan)\n",
    "        features[cols] = features[cols].fillna(features[cols].mean())\n",
    "        features[cols] = features[cols].astype(np.float32)\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65d7601d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-03T08:25:26.939417Z",
     "iopub.status.busy": "2022-07-03T08:25:26.938651Z",
     "iopub.status.idle": "2022-07-03T08:25:26.945238Z",
     "shell.execute_reply": "2022-07-03T08:25:26.943833Z",
     "shell.execute_reply.started": "2022-07-02T11:31:52.450004Z"
    },
    "papermill": {
     "duration": 0.038902,
     "end_time": "2022-07-03T08:25:26.945519",
     "exception": false,
     "start_time": "2022-07-03T08:25:26.906617",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6 µs, sys: 0 ns, total: 6 µs\n",
      "Wall time: 9.3 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def update_data(test_start_date):\n",
    "    # load train + sup data\n",
    "    df_prices = pd.concat([pd.read_csv(f'{JPX_PATH}/{folder}/stock_prices.csv', parse_dates=['Date']) for folder in ['train_files','supplemental_files']]).reset_index(drop=True)\n",
    "    df_sec_prices = pd.concat([pd.read_csv(f'{JPX_PATH}/{folder}/secondary_stock_prices.csv', parse_dates=['Date']) for folder in ['train_files','supplemental_files']]).reset_index(drop=True)\n",
    "    df_fins = pd.concat([pd.read_csv(f'{JPX_PATH}/{folder}/financials.csv', parse_dates=['Date']) for folder in ['train_files','supplemental_files']]).reset_index(drop=True)\n",
    "    df_opts = pd.concat([pd.read_csv(f'{JPX_PATH}/{folder}/options.csv', parse_dates=['Date']) for folder in ['train_files','supplemental_files']]).reset_index(drop=True)\n",
    "    df_trades = pd.concat([pd.read_csv(f'{JPX_PATH}/{folder}/trades.csv', parse_dates=['Date']) for folder in ['train_files','supplemental_files']]).reset_index(drop=True)\n",
    "\n",
    "    # identify missing dates\n",
    "    if test_start_date.astype('datetime64[Y]').astype(int) + 1970 == 2021:\n",
    "        fe_end_date = np.datetime64('2021-10-27')\n",
    "#         fe_end_date = np.datetime64('2021-12-01')\n",
    "    else:\n",
    "        fe_end_date = data.last_date\n",
    "    extra_dates = df_prices.Date.drop_duplicates().loc[lambda x: (x > fe_end_date) & (x < test_start_date)].tolist()\n",
    "\n",
    "    # FE for missing dates\n",
    "    for i in range(len(extra_dates)):\n",
    "        data.push_forward([df.loc[lambda x: x.Date==extra_dates[i]] for df in [df_prices, df_sec_prices, df_fins, df_opts, df_trades]], append=False, last=False)\n",
    "    \n",
    "    # release memory\n",
    "    del df_prices, df_sec_prices, df_fins, df_opts, df_trades\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1093c23d",
   "metadata": {
    "papermill": {
     "duration": 0.02492,
     "end_time": "2022-07-03T08:25:26.995664",
     "exception": false,
     "start_time": "2022-07-03T08:25:26.970744",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a5c38bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-03T08:25:27.056389Z",
     "iopub.status.busy": "2022-07-03T08:25:27.051608Z",
     "iopub.status.idle": "2022-07-03T08:25:27.059135Z",
     "shell.execute_reply": "2022-07-03T08:25:27.059644Z",
     "shell.execute_reply.started": "2022-07-02T11:31:52.462507Z"
    },
    "papermill": {
     "duration": 0.040833,
     "end_time": "2022-07-03T08:25:27.059844",
     "exception": false,
     "start_time": "2022-07-03T08:25:27.019011",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Function to get sub-columns from table for model fitting\n",
    "'''\n",
    "def get_dataset(df, selected_feats, trn_val):\n",
    "    if trn_val=='val':\n",
    "        df = df.groupby('Date').sample(frac=1.0, random_state=SEED)\n",
    "    df = df.reset_index(drop=True)\n",
    "    grp = df.groupby('Date').size().tolist()\n",
    "    qid = df['Date']\n",
    "    X = df[selected_feats]\n",
    "    y = df['target_train']\n",
    "    target = df['Target']\n",
    "    header = df[id_cols]\n",
    "    return X, y, grp, qid, header, target\n",
    "\n",
    "'''\n",
    "Function to predict scores within groups\n",
    "'''\n",
    "def pred_score(model, X):\n",
    "    if MODEL=='reg':\n",
    "        return pd.Series(model.predict(X))\n",
    "    elif MODEL=='clf':\n",
    "        return pd.Series(model.predict_proba(X)[:,1])\n",
    "    elif MODEL=='clf3':\n",
    "        return pd.Series((model.predict_proba(X) * [-1,0,1]).sum(axis=1))\n",
    "\n",
    "'''\n",
    "Function to transform model output to rank prediction table\n",
    "'''\n",
    "def get_pred_df(header, pred_model, y_true_train, y_true, rank_ascending):\n",
    "    df_pred = pd.concat([header[['RowId','Date','SecuritiesCode']].assign(SecuritiesCode=lambda x: x.SecuritiesCode.astype(int)).reset_index(drop=True),\n",
    "                        pd.Series(pred_model).rename('pred_model').reset_index(drop=True),\n",
    "                        y_true_train.reset_index(drop=True),\n",
    "                        y_true.reset_index(drop=True)\n",
    "                        ], axis=1)\n",
    "    df_pred['Rank'] = df_pred.groupby('Date').pred_model.rank(method='first', ascending=rank_ascending).astype(int) - 1\n",
    "    return df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f76307ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-03T08:25:27.118278Z",
     "iopub.status.busy": "2022-07-03T08:25:27.117522Z",
     "iopub.status.idle": "2022-07-03T08:25:27.120343Z",
     "shell.execute_reply": "2022-07-03T08:25:27.119648Z",
     "shell.execute_reply.started": "2022-07-02T11:31:52.481565Z"
    },
    "papermill": {
     "duration": 0.036583,
     "end_time": "2022-07-03T08:25:27.120497",
     "exception": false,
     "start_time": "2022-07-03T08:25:27.083914",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_stock_clust(ret, date, n_day, stock_list):\n",
    "    # raw correlation table\n",
    "    corr = ret.loc[lambda x: pd.to_datetime(x.Date)<=date].pivot(index='Date', columns='SecuritiesCode', values='ret').tail(n_day).corr()\n",
    "    corr = corr.reindex(index=stock_list, columns=stock_list)\n",
    "    cols = [c for c in corr if corr[c].notnull().sum()==0]\n",
    "    corr = corr.drop(cols, axis=0).drop(cols, axis=1)\n",
    "    stocks = corr.columns.tolist()\n",
    "\n",
    "    # clustering\n",
    "    pdist = spc.distance.pdist(corr.values)\n",
    "    linkage = spc.linkage(pdist, method='complete')\n",
    "    idx = spc.fcluster(linkage, 0.5 * pdist.max(), 'distance')\n",
    "    stock_corr_clust = pd.DataFrame({'SecuritiesCode':stocks, 'clust':idx}).assign(Date=date).sort_values('clust').reset_index(drop=True)\n",
    "    return stock_corr_clust"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c757d048",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-27T14:24:06.821508Z",
     "iopub.status.busy": "2022-06-27T14:24:06.821286Z",
     "iopub.status.idle": "2022-06-27T14:24:14.792169Z",
     "shell.execute_reply": "2022-06-27T14:24:14.791105Z",
     "shell.execute_reply.started": "2022-06-27T14:24:06.821481Z"
    },
    "papermill": {
     "duration": 0.023335,
     "end_time": "2022-07-03T08:25:27.167299",
     "exception": false,
     "start_time": "2022-07-03T08:25:27.143964",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc9a6d08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-03T08:25:27.243440Z",
     "iopub.status.busy": "2022-07-03T08:25:27.238110Z",
     "iopub.status.idle": "2022-07-03T08:52:51.583471Z",
     "shell.execute_reply": "2022-07-03T08:52:51.584432Z",
     "shell.execute_reply.started": "2022-07-02T11:31:52.497187Z"
    },
    "papermill": {
     "duration": 1644.393894,
     "end_time": "2022-07-03T08:52:51.584879",
     "exception": false,
     "start_time": "2022-07-03T08:25:27.190985",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This version of the API is not optimized and should not be used to estimate the runtime of your code on the hidden test set.\n",
      "[2022-07-03 16:27:38] Pushed to latest date: 2021-10-28 00:00:00\n",
      "[2022-07-03 16:28:34] Pushed to latest date: 2021-10-29 00:00:00\n",
      "[2022-07-03 16:29:31] Pushed to latest date: 2021-11-01 00:00:00\n",
      "[2022-07-03 16:30:28] Pushed to latest date: 2021-11-02 00:00:00\n",
      "[2022-07-03 16:31:25] Pushed to latest date: 2021-11-04 00:00:00\n",
      "[2022-07-03 16:32:22] Pushed to latest date: 2021-11-05 00:00:00\n",
      "[2022-07-03 16:33:20] Pushed to latest date: 2021-11-08 00:00:00\n",
      "[2022-07-03 16:34:17] Pushed to latest date: 2021-11-09 00:00:00\n",
      "[2022-07-03 16:35:14] Pushed to latest date: 2021-11-10 00:00:00\n",
      "[2022-07-03 16:36:11] Pushed to latest date: 2021-11-11 00:00:00\n",
      "[2022-07-03 16:37:07] Pushed to latest date: 2021-11-12 00:00:00\n",
      "[2022-07-03 16:38:04] Pushed to latest date: 2021-11-15 00:00:00\n",
      "[2022-07-03 16:39:02] Pushed to latest date: 2021-11-16 00:00:00\n",
      "[2022-07-03 16:39:59] Pushed to latest date: 2021-11-17 00:00:00\n",
      "[2022-07-03 16:40:56] Pushed to latest date: 2021-11-18 00:00:00\n",
      "[2022-07-03 16:41:53] Pushed to latest date: 2021-11-19 00:00:00\n",
      "[2022-07-03 16:42:52] Pushed to latest date: 2021-11-22 00:00:00\n",
      "[2022-07-03 16:43:50] Pushed to latest date: 2021-11-24 00:00:00\n",
      "[2022-07-03 16:44:47] Pushed to latest date: 2021-11-25 00:00:00\n",
      "[2022-07-03 16:45:44] Pushed to latest date: 2021-11-26 00:00:00\n",
      "[2022-07-03 16:46:42] Pushed to latest date: 2021-11-29 00:00:00\n",
      "[2022-07-03 16:47:39] Pushed to latest date: 2021-11-30 00:00:00\n",
      "[2022-07-03 16:48:36] Pushed to latest date: 2021-12-01 00:00:00\n",
      "[2022-07-03 16:49:33] Pushed to latest date: 2021-12-02 00:00:00\n",
      "[2022-07-03 16:50:30] Pushed to latest date: 2021-12-03 00:00:00\n",
      "[2022-07-03 16:51:29] Pushed to latest date: 2021-12-06 00:00:00\n",
      "[2022-07-03 16:52:41] Pushed to latest date: 2021-12-07 00:00:00\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>SecuritiesCode</th>\n",
       "      <th>Rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-12-06</td>\n",
       "      <td>1301</td>\n",
       "      <td>312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-12-06</td>\n",
       "      <td>1332</td>\n",
       "      <td>348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-12-06</td>\n",
       "      <td>1333</td>\n",
       "      <td>514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-12-06</td>\n",
       "      <td>1375</td>\n",
       "      <td>695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-12-06</td>\n",
       "      <td>1376</td>\n",
       "      <td>1816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>2021-12-07</td>\n",
       "      <td>9990</td>\n",
       "      <td>1988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>2021-12-07</td>\n",
       "      <td>9991</td>\n",
       "      <td>1846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>2021-12-07</td>\n",
       "      <td>9993</td>\n",
       "      <td>978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>2021-12-07</td>\n",
       "      <td>9994</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>2021-12-07</td>\n",
       "      <td>9997</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date  SecuritiesCode  Rank\n",
       "0     2021-12-06            1301   312\n",
       "1     2021-12-06            1332   348\n",
       "2     2021-12-06            1333   514\n",
       "3     2021-12-06            1375   695\n",
       "4     2021-12-06            1376  1816\n",
       "...          ...             ...   ...\n",
       "3995  2021-12-07            9990  1988\n",
       "3996  2021-12-07            9991  1846\n",
       "3997  2021-12-07            9993   978\n",
       "3998  2021-12-07            9994    58\n",
       "3999  2021-12-07            9997   109\n",
       "\n",
       "[4000 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowId</th>\n",
       "      <th>Date</th>\n",
       "      <th>SecuritiesCode</th>\n",
       "      <th>pred_model</th>\n",
       "      <th>target_train</th>\n",
       "      <th>Target</th>\n",
       "      <th>Rank</th>\n",
       "      <th>clust</th>\n",
       "      <th>pred_model_mean</th>\n",
       "      <th>pred_model_demean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20211206_1301</td>\n",
       "      <td>2021-12-06</td>\n",
       "      <td>1301</td>\n",
       "      <td>0.508325</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>312</td>\n",
       "      <td>5</td>\n",
       "      <td>0.505495</td>\n",
       "      <td>0.002830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20211206_1332</td>\n",
       "      <td>2021-12-06</td>\n",
       "      <td>1332</td>\n",
       "      <td>0.508159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>348</td>\n",
       "      <td>5</td>\n",
       "      <td>0.505495</td>\n",
       "      <td>0.002664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20211206_1375</td>\n",
       "      <td>2021-12-06</td>\n",
       "      <td>1375</td>\n",
       "      <td>0.506609</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>695</td>\n",
       "      <td>5</td>\n",
       "      <td>0.505495</td>\n",
       "      <td>0.001114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20211206_1376</td>\n",
       "      <td>2021-12-06</td>\n",
       "      <td>1376</td>\n",
       "      <td>0.501807</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1816</td>\n",
       "      <td>5</td>\n",
       "      <td>0.505495</td>\n",
       "      <td>-0.003688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20211206_1379</td>\n",
       "      <td>2021-12-06</td>\n",
       "      <td>1379</td>\n",
       "      <td>0.504093</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1406</td>\n",
       "      <td>5</td>\n",
       "      <td>0.505495</td>\n",
       "      <td>-0.001402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>20211207_9977</td>\n",
       "      <td>2021-12-07</td>\n",
       "      <td>9977</td>\n",
       "      <td>0.502072</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1767</td>\n",
       "      <td>2</td>\n",
       "      <td>0.505255</td>\n",
       "      <td>-0.003183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>20211207_2768</td>\n",
       "      <td>2021-12-07</td>\n",
       "      <td>2768</td>\n",
       "      <td>0.506568</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>875</td>\n",
       "      <td>1</td>\n",
       "      <td>0.506278</td>\n",
       "      <td>0.000290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>20211207_7500</td>\n",
       "      <td>2021-12-07</td>\n",
       "      <td>7500</td>\n",
       "      <td>0.509964</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>175</td>\n",
       "      <td>1</td>\n",
       "      <td>0.506278</td>\n",
       "      <td>0.003686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>20211207_8713</td>\n",
       "      <td>2021-12-07</td>\n",
       "      <td>8713</td>\n",
       "      <td>0.504063</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1612</td>\n",
       "      <td>1</td>\n",
       "      <td>0.506278</td>\n",
       "      <td>-0.002215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>20211207_9919</td>\n",
       "      <td>2021-12-07</td>\n",
       "      <td>9919</td>\n",
       "      <td>0.504517</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1500</td>\n",
       "      <td>1</td>\n",
       "      <td>0.506278</td>\n",
       "      <td>-0.001761</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              RowId       Date  SecuritiesCode  pred_model  target_train  \\\n",
       "0     20211206_1301 2021-12-06            1301    0.508325           0.0   \n",
       "1     20211206_1332 2021-12-06            1332    0.508159           0.0   \n",
       "2     20211206_1375 2021-12-06            1375    0.506609           0.0   \n",
       "3     20211206_1376 2021-12-06            1376    0.501807           0.0   \n",
       "4     20211206_1379 2021-12-06            1379    0.504093           0.0   \n",
       "...             ...        ...             ...         ...           ...   \n",
       "3995  20211207_9977 2021-12-07            9977    0.502072           0.0   \n",
       "3996  20211207_2768 2021-12-07            2768    0.506568           0.0   \n",
       "3997  20211207_7500 2021-12-07            7500    0.509964           0.0   \n",
       "3998  20211207_8713 2021-12-07            8713    0.504063           0.0   \n",
       "3999  20211207_9919 2021-12-07            9919    0.504517           0.0   \n",
       "\n",
       "      Target  Rank  clust  pred_model_mean  pred_model_demean  \n",
       "0        0.0   312      5         0.505495           0.002830  \n",
       "1        0.0   348      5         0.505495           0.002664  \n",
       "2        0.0   695      5         0.505495           0.001114  \n",
       "3        0.0  1816      5         0.505495          -0.003688  \n",
       "4        0.0  1406      5         0.505495          -0.001402  \n",
       "...      ...   ...    ...              ...                ...  \n",
       "3995     0.0  1767      2         0.505255          -0.003183  \n",
       "3996     0.0   875      1         0.506278           0.000290  \n",
       "3997     0.0   175      1         0.506278           0.003686  \n",
       "3998     0.0  1612      1         0.506278          -0.002215  \n",
       "3999     0.0  1500      1         0.506278          -0.001761  \n",
       "\n",
       "[4000 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26min 34s, sys: 36.3 s, total: 27min 10s\n",
      "Wall time: 27min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# init env\n",
    "env = jpx_tokyo_market_prediction.make_env()\n",
    "iter_test = env.iter_test()\n",
    "\n",
    "# init variables\n",
    "if FE=='orig':\n",
    "    data = load_pkl(f'{FE_PATH}/data')\n",
    "elif FE=='ta':\n",
    "    data = load_pkl(f'{FE_PATH}/results (2)/data')\n",
    "sample_prediction_all = []\n",
    "df_pred_val_all = []\n",
    "updated_data = 0\n",
    "close = []\n",
    "df = pd.concat([pd.read_csv(f'{JPX_PATH}/train_files/stock_prices.csv'),\n",
    "                 pd.read_csv(f'{JPX_PATH}/supplemental_files/stock_prices.csv')]) \\\n",
    "    .loc[:, ['Date','SecuritiesCode','Close']]\n",
    "close.append(df)\n",
    "\n",
    "# prepare feature means\n",
    "if FE=='orig':\n",
    "    features = pd.concat([pd.concat(load_pkl(f'{FE_PATH}/features/{filename}')) for filename in os.listdir(f'{FE_PATH}/features')]).sort_values('RowId').reset_index(drop=True)\n",
    "elif FE=='ta':\n",
    "    features1 = pd.concat([pd.concat(load_pkl(f'{FE_PATH}/results (1)/features/{filename}')) for filename in os.listdir(f'{FE_PATH}/results (1)/features')]).sort_values('RowId').reset_index(drop=True)\n",
    "    features2 = pd.concat([pd.concat(load_pkl(f'{FE_PATH}/results (2)/features/{filename}')) for filename in os.listdir(f'{FE_PATH}/results (2)/features')]).sort_values('RowId').reset_index(drop=True)\n",
    "    features = pd.concat([features1, features2]).reset_index(drop=True)\n",
    "    del features1, features2\n",
    "    gc.collect()\n",
    "mean = features.groupby('Date').mean().ffill()\n",
    "mean = mean.fillna(mean.mean())\n",
    "mean = mean.tail(20).mean()\n",
    "del features\n",
    "gc.collect()\n",
    "\n",
    "# iterations\n",
    "for df_prices, df_opts, df_fins, df_trades, df_sec_prices, sample_prediction in iter_test:\n",
    "    \n",
    "    # add new supplemental data for first iteration\n",
    "    date = np.datetime64(sample_prediction.Date.iloc[0])\n",
    "    if updated_data==0:\n",
    "        update_data(date)\n",
    "        updated_data = 1\n",
    "        \n",
    "    # append ret\n",
    "    close.append(df_prices.loc[:, ['Date','SecuritiesCode','Close']])\n",
    "    ret = pd.concat(close) \\\n",
    "        .sort_values(['Date','SecuritiesCode']) \\\n",
    "        .assign(ret = lambda x: x.groupby('SecuritiesCode').Close.pct_change()) \\\n",
    "        .loc[:, ['Date','SecuritiesCode','ret']] \\\n",
    "        .dropna() \\\n",
    "        .drop_duplicates(subset=['Date','SecuritiesCode']) \\\n",
    "        .reset_index(drop=True)\n",
    "        \n",
    "    # set date columns\n",
    "    for df in [df_prices, df_opts, df_fins, df_trades, df_sec_prices]:\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        \n",
    "    # feature engineering of current date\n",
    "    data.push_forward([df_prices, df_sec_prices, df_fins, df_opts, df_trades], append=False, last=False)\n",
    "    features = data.curr_features\n",
    "    \n",
    "    # fillna\n",
    "    cols = [c for c in features.columns if c not in ['RowId','SecuritiesCode','Date']]\n",
    "    features[cols] = features[cols].fillna(mean[cols])\n",
    "    \n",
    "    # train-val split\n",
    "    full_data = features.assign(fold = -1,\n",
    "                                trn_val = 'val',\n",
    "                                Target = 0.0,\n",
    "                                target_train = 0.0)\n",
    "\n",
    "    # define column types\n",
    "    id_cols = ['RowId','Date','SecuritiesCode','fold','trn_val']\n",
    "    all_features = [c for c in list(full_data) if c not in id_cols and c not in ['target_train','Target']]\n",
    "    cat_features = ['AdjustmentFactor','first_div']\n",
    "    time_features = [c for c in all_features if '_mkt' in c] + \\\n",
    "                    [c for c in all_features if c[:4]=='sec_'] + \\\n",
    "                    [c for c in all_features if c[-4:] in ['_sin','_cos']]\n",
    "    stock_features = [c for c in all_features if c not in cat_features + time_features]\n",
    "\n",
    "    # scaling\n",
    "    scaler = load_pkl(f'{MODEL_PATH}/scaler4')\n",
    "    feats = time_features + stock_features\n",
    "    full_data[feats] = scaler.transform(full_data[feats]).astype(np.float32)\n",
    "\n",
    "    # drop market features\n",
    "    if DROP_MARKET_FEATS:\n",
    "        full_data = full_data.drop(time_features, axis=1)\n",
    "        all_features = [c for c in all_features if c not in time_features]\n",
    "\n",
    "    # PCA compression\n",
    "    if RUN_PCA:\n",
    "        if PCA_SPLIT==False:\n",
    "            pca = load_pkl(f'{MODEL_PATH}/pca')        \n",
    "            cols = [f'pc{x}' for x in range(pca.components_.shape[0])]\n",
    "            X = pd.DataFrame(pca.transform(full_data.loc[:, all_features]), columns=cols)\n",
    "            header = full_data.loc[:, [c for c in full_data.columns if c not in all_features]].reset_index(drop=True)\n",
    "            full_data = pd.concat([header, X], axis=1)  \n",
    "            all_features = cols.copy()\n",
    "            stock_features = None\n",
    "            time_features = None\n",
    "\n",
    "        elif PCA_SPLIT==True:\n",
    "            stock_feats = [c for c in all_features if c not in time_features]\n",
    "            market_feats = [c for c in all_features if c in time_features]\n",
    "            pca_stock = load_pkl(f'{MODEL_PATH}/pca_stock')\n",
    "            pca_market = load_pkl(f'{MODEL_PATH}/pca_market')\n",
    "            cols_stock = [f'pc_stock{x}' for x in range(pca_stock.components_.shape[0])]\n",
    "            cols_market = [f'pc_market{x}' for x in range(pca_market.components_.shape[0])]\n",
    "            X_stock = pd.DataFrame(pca_stock.transform(full_data.loc[:, stock_feats]), columns=cols_stock)\n",
    "            X_market = pd.DataFrame(pca_market.transform(full_data.loc[:, market_feats]), columns=cols_market)\n",
    "            header = full_data.loc[:, [c for c in full_data.columns if c not in all_features]].reset_index(drop=True)\n",
    "            full_data = pd.concat([header, X_stock, X_market], axis=1)\n",
    "            all_features = cols_stock + cols_market\n",
    "            stock_features = cols_stock\n",
    "            time_features = cols_market\n",
    "\n",
    "    # model prediction\n",
    "    df_pred_val = []\n",
    "    for seed in SEEDS:\n",
    "        for fold in range(N_FOLD):\n",
    "            X_val, y_val, grp_val, qid_val, header_val, target_val = get_dataset(full_data, SELECTED_FEATS, 'val')\n",
    "            model = load_pkl(f'{MODEL_PATH}/model{fold}_seed{seed}')\n",
    "            pred_val = pred_score(model, X_val)\n",
    "            df_pred_val.append(get_pred_df(header_val, pred_val, y_val, target_val, RANK_ASCENDING))\n",
    "    df_pred_val = pd.concat(df_pred_val).reset_index(drop=True)\n",
    "    df_pred_val = df_pred_val.groupby(['RowId','Date','SecuritiesCode']).mean().reset_index()\n",
    "    df_pred_val['Rank'] = df_pred_val.groupby('Date').pred_model.rank(method='first', ascending=False).astype(int) - 1\n",
    "\n",
    "    # cluster demean\n",
    "    if CLUSTER_DEMEAN:\n",
    "        df_clust = []\n",
    "        for date in df_pred_val.Date.unique():\n",
    "            stock_list = df_pred_val.loc[lambda x: x.Date==date].SecuritiesCode.tolist()\n",
    "            df_clust.append(get_stock_clust(ret, date, CLUST_N_DAY, stock_list))\n",
    "        df_clust = pd.concat(df_clust)\n",
    "        df_pred_val = df_pred_val.merge(df_clust, how='inner', on=['Date','SecuritiesCode'])\n",
    "        clust_mean = df_pred_val.groupby(['Date','clust']).pred_model.mean().reset_index().rename(columns={'pred_model':'pred_model_mean'})\n",
    "        df_pred_val = df_pred_val.merge(clust_mean, how='inner', on=['Date','clust'])\n",
    "        df_pred_val['pred_model_demean'] = df_pred_val.pred_model - df_pred_val.pred_model_mean\n",
    "        df_pred_val['Rank'] = df_pred_val.groupby('Date').pred_model_demean.rank(method='first', ascending=False).astype(int) - 1\n",
    "\n",
    "    # volatility penalty\n",
    "    if VOL_PENALTY:\n",
    "        std = ret.pivot(index='Date', columns='SecuritiesCode', values='ret') \\\n",
    "            .rolling(VOL_N_DAY).std() \\\n",
    "            .stack().reset_index() \\\n",
    "            .dropna() \\\n",
    "            .rename(columns={0:'std'}) \\\n",
    "            .assign(Date = lambda x: pd.to_datetime(x.Date))\n",
    "        df_pred_val = df_pred_val.merge(std, how='inner', on=['Date','SecuritiesCode'])\n",
    "        # identify best power and apply\n",
    "        best_p = VOL_POW\n",
    "        df_pred_val['pred_model_vol_penalty'] = df_pred_val.pred_model_demean / df_pred_val['std'].pow(best_p)\n",
    "        df_pred_val['Rank'] = df_pred_val.groupby('Date').pred_model_vol_penalty.rank(method='first', ascending=False).astype(int) - 1\n",
    "\n",
    "    # final submission\n",
    "    rnk = df_pred_val.set_index('SecuritiesCode').Rank\n",
    "    sample_prediction['Rank'] = sample_prediction['SecuritiesCode'].map(rnk)\n",
    "    sample_prediction['Rank'] = sample_prediction['Rank'].fillna(1000).rank(method='first', ascending=True).astype(int) - 1\n",
    "    env.predict(sample_prediction)\n",
    "    \n",
    "    # save results\n",
    "    sample_prediction_all.append(sample_prediction)\n",
    "    df_pred_val_all.append(df_pred_val)\n",
    "\n",
    "# output\n",
    "sample_prediction_all = pd.concat(sample_prediction_all).reset_index(drop=True)\n",
    "df_pred_val_all = pd.concat(df_pred_val_all).reset_index(drop=True)\n",
    "display(sample_prediction_all)\n",
    "display(df_pred_val_all)\n",
    "if EXPORT:\n",
    "    sample_prediction_all.to_csv('sample_prediction_all.csv', index=False)\n",
    "    df_pred_val_all.to_csv('df_pred_val_all.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfba77a",
   "metadata": {
    "papermill": {
     "duration": 0.035377,
     "end_time": "2022-07-03T08:52:51.656601",
     "exception": false,
     "start_time": "2022-07-03T08:52:51.621224",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382f2325",
   "metadata": {
    "papermill": {
     "duration": 0.035915,
     "end_time": "2022-07-03T08:52:51.729002",
     "exception": false,
     "start_time": "2022-07-03T08:52:51.693087",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7bfa60",
   "metadata": {
    "papermill": {
     "duration": 0.035068,
     "end_time": "2022-07-03T08:52:51.799437",
     "exception": false,
     "start_time": "2022-07-03T08:52:51.764369",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333573be",
   "metadata": {
    "papermill": {
     "duration": 0.035688,
     "end_time": "2022-07-03T08:52:51.871057",
     "exception": false,
     "start_time": "2022-07-03T08:52:51.835369",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d839d7ce",
   "metadata": {
    "papermill": {
     "duration": 0.035803,
     "end_time": "2022-07-03T08:52:51.942312",
     "exception": false,
     "start_time": "2022-07-03T08:52:51.906509",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1689.549117,
   "end_time": "2022-07-03T08:52:53.295176",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-07-03T08:24:43.746059",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
